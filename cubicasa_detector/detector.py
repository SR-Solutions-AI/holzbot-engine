# file: engine/cubicasa_detector/detector.py
from __future__ import annotations

import sys
import os
import cv2
import numpy as np
import torch
import torch.nn.functional as F
import json
import math
import re
import requests
import base64
from pathlib import Path
from PIL import Image
from io import BytesIO
from skimage.morphology import skeletonize

# OCR pentru detectarea textului
try:
    import pytesseract
    TESSERACT_AVAILABLE = True
except ImportError:
    TESSERACT_AVAILABLE = False
    print("âš ï¸ pytesseract nu este disponibil. Detectarea textului 'terasa' va fi dezactivatÄƒ.")

# ============================================
# CONFIGURARE PATHS
# ============================================

def _get_cubicasa_path():
    """GÄƒseÈ™te automat CubiCasa5k relativ la acest fiÈ™ier."""
    current_dir = Path(__file__).parent
    candidates = [
        current_dir / "CubiCasa5k",
        current_dir.parent / "CubiCasa5k",
        current_dir.parent.parent / "CubiCasa5k",
    ]
    
    for path in candidates:
        if path.exists():
            return str(path)
    
    # Am comentat partea de raise pentru a nu bloca rularea
    # raise FileNotFoundError(
    #     "Nu gÄƒsesc folderul CubiCasa5k. "
    #     "PlaseazÄƒ-l Ã®n runner/cubicasa_detector/ sau runner/"
    # )
    return str(current_dir / "CubiCasa5k")

CUBICASA_PATH = _get_cubicasa_path()
sys.path.insert(0, CUBICASA_PATH)

try:
    from floortrans.models.hg_furukawa_original import hg_furukawa_original
except ImportError as e:
    # Am modificat excepÈ›ia pentru a nu bloca rularea
    print(f"AtenÈ›ie: Nu pot importa modelul CubiCasa. VerificÄƒ path-ul: {e}")
    class hg_furukawa_original:
        def __init__(self, n_classes): pass
        def to(self, device): pass
        def eval(self): pass


# ============================================
# HELPERS GENERALE
# ============================================

def ensure_dir(path):
    Path(path).mkdir(parents=True, exist_ok=True)

def save_step(name, img, steps_dir):
    path = Path(steps_dir) / f"{name}.png"
    cv2.imwrite(str(path), img)

def filter_thin_lines(walls_raw: np.ndarray, image_dims: tuple, steps_dir: str = None) -> np.ndarray:
    """
    Filtrarea liniilor subÈ›iri (nemodificatÄƒ).
    """
    h, w = image_dims
    min_dim = min(h, w)
    min_wall_thickness = max(3, int(min_dim * 0.004))
    
    print(f"      ğŸ§¹ Filtrez linii subÈ›iri: prag {min_wall_thickness}px...")
    
    filter_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (min_wall_thickness, min_wall_thickness))
    walls_eroded = cv2.erode(walls_raw, filter_kernel, iterations=1)
    
    if steps_dir:
        save_step("filter_01_eroded", walls_eroded, str(steps_dir))
    
    walls_filtered = cv2.dilate(walls_eroded, filter_kernel, iterations=1)
    
    if steps_dir:
        save_step("filter_02_restored", walls_filtered, str(steps_dir))
    
    pixels_before = np.count_nonzero(walls_raw)
    pixels_after = np.count_nonzero(walls_filtered)
    removed_pct = 100 * (pixels_before - pixels_after) / pixels_before if pixels_before > 0 else 0
    
    print(f"         Eliminat {removed_pct:.1f}% pixeli (linii subÈ›iri)")
    
    return walls_filtered

def aggressive_wall_repair(walls_raw: np.ndarray, image_dims: tuple, steps_dir: str = None) -> np.ndarray:
    """Reparare puternicÄƒ a pereÈ›ilor pentru imagini mari (nemodificatÄƒ)."""
    h, w = image_dims
    min_dim = min(h, w)
    
    kernel_size = max(7, int(min_dim * 0.009))
    if kernel_size % 2 == 0: kernel_size += 1
    
    print(f"      ğŸ”§ Strong repair: kernel {kernel_size}x{kernel_size}")
    
    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (kernel_size, kernel_size))
    kernel_small = cv2.getStructuringElement(cv2.MORPH_RECT, (max(5, kernel_size-2), max(5, kernel_size-2)))
    
    walls_closed = cv2.morphologyEx(walls_raw, cv2.MORPH_CLOSE, kernel, iterations=2)
    if steps_dir:
        save_step("repair_01_close", walls_closed, steps_dir)
    
    walls_dilated = cv2.dilate(walls_closed, kernel_small, iterations=1)
    if steps_dir:
        save_step("repair_02_dilate", walls_dilated, steps_dir)
    
    walls_final = cv2.morphologyEx(walls_dilated, cv2.MORPH_CLOSE, kernel, iterations=1)
    if steps_dir:
        save_step("repair_03_final_close", walls_final, steps_dir)
    
    return walls_final

def border_constrained_fill(walls_mask: np.ndarray, steps_dir: str = None) -> np.ndarray:
    """
    Ãnchide golurile Ã®n peretele exterior folosind Border-Constrained Fill.
    
    DetecteazÄƒ cel mai mare contur (perimetrul casei), genereazÄƒ convex hull,
    È™i foloseÈ™te hull-ul ca ghidaj pentru a vedea unde ar trebui sÄƒ existe un perete
    Ã®ntre douÄƒ puncte extreme.
    
    Args:
        walls_mask: Masca pereÈ›ilor (255 = perete, 0 = spaÈ›iu liber)
        steps_dir: Director pentru salvarea step-urilor de debug (opÈ›ional)
    
    Returns:
        Masca pereÈ›ilor cu golurile din peretele exterior Ã®nchise
    """
    h, w = walls_mask.shape[:2]
    
    print(f"      ğŸ›ï¸ Border-Constrained Fill: Ã®nchid goluri Ã®n peretele exterior...")
    
    result = walls_mask.copy()
    
    # Pas 1: DetectÄƒm contururile exterioare
    contours, _ = cv2.findContours(walls_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    
    if len(contours) == 0:
        print(f"         âš ï¸ Nu s-au detectat contururi")
        return result
    
    # Pas 2: GÄƒsim cel mai mare contur (perimetrul casei)
    largest_contour = max(contours, key=cv2.contourArea)
    
    if cv2.contourArea(largest_contour) < min(h, w) * 10:  # Contur prea mic
        print(f"         âš ï¸ Conturul cel mai mare este prea mic")
        return result
    
    # Pas 3: GenerÄƒm convex hull pentru conturul cel mai mare
    hull = cv2.convexHull(largest_contour)
    
    if steps_dir:
        # SalvÄƒm vizualizarea hull-ului
        debug_image = cv2.cvtColor(walls_mask, cv2.COLOR_GRAY2BGR)
        cv2.drawContours(debug_image, [largest_contour], -1, (0, 255, 0), 2)  # Verde pentru conturul original
        cv2.drawContours(debug_image, [hull], -1, (0, 0, 255), 2)  # RoÈ™u pentru hull
        cv2.imwrite(str(Path(steps_dir) / "02c_border_hull_debug.png"), debug_image)
    
    # Pas 4: ComparÄƒm hull-ul cu conturul original pentru a gÄƒsi golurile
    # CreÄƒm o mascÄƒ pentru hull
    hull_mask = np.zeros((h, w), dtype=np.uint8)
    cv2.fillPoly(hull_mask, [hull], 255)
    
    # CreÄƒm o mascÄƒ pentru conturul original
    contour_mask = np.zeros((h, w), dtype=np.uint8)
    cv2.fillPoly(contour_mask, [largest_contour], 255)
    
    # DiferenÈ›a dintre hull È™i contur indicÄƒ zonele unde ar trebui sÄƒ existe pereÈ›i
    # Dar nu vrem sÄƒ umplem interiorul, doar sÄƒ conectÄƒm punctele extreme
    hull_points = hull.reshape(-1, 2)
    contour_points = largest_contour.reshape(-1, 2)
    
    # Pas 5: GÄƒsim segmentele din hull care reprezintÄƒ goluri reale Ã®n peretele exterior
    # Strategie: VerificÄƒm doar segmentele care conecteazÄƒ puncte apropiate de conturul original
    connections_made = 0
    max_gap_distance = max(150, int(min(h, w) * 0.12))  # 12% din dimensiunea minimÄƒ
    
    # GÄƒsim punctele din conturul original care sunt aproape de hull
    contour_points = largest_contour.reshape(-1, 2)
    
    for i in range(len(hull_points)):
        p1 = tuple(hull_points[i])
        p2 = tuple(hull_points[(i + 1) % len(hull_points)])
        
        # CalculÄƒm distanÈ›a dintre puncte
        segment_length = np.sqrt((p2[0] - p1[0])**2 + (p2[1] - p1[1])**2)
        
        # IgnorÄƒm segmentele prea scurte sau prea lungi
        if segment_length < 20 or segment_length > max_gap_distance:
            continue
        
        # GÄƒsim cele mai apropiate puncte din conturul original pentru p1 È™i p2
        min_dist_p1 = float('inf')
        min_dist_p2 = float('inf')
        closest_contour_p1 = None
        closest_contour_p2 = None
        
        for cp in contour_points:
            dist_p1 = np.sqrt((cp[0] - p1[0])**2 + (cp[1] - p1[1])**2)
            dist_p2 = np.sqrt((cp[0] - p2[0])**2 + (cp[1] - p2[1])**2)
            
            if dist_p1 < min_dist_p1:
                min_dist_p1 = dist_p1
                closest_contour_p1 = tuple(cp)
            
            if dist_p2 < min_dist_p2:
                min_dist_p2 = dist_p2
                closest_contour_p2 = tuple(cp)
        
        # VerificÄƒm cÄƒ ambele capete sunt aproape de conturul original (max 50 pixeli)
        max_endpoint_distance = 50
        if min_dist_p1 > max_endpoint_distance or min_dist_p2 > max_endpoint_distance:
            continue
        
        # VerificÄƒm dacÄƒ existÄƒ deja un perete Ã®ntre cele douÄƒ puncte din conturul original
        # (nu Ã®ntre punctele din hull, ci Ã®ntre cele mai apropiate puncte din contur)
        if closest_contour_p1 is None or closest_contour_p2 is None:
            continue
        
        # VerificÄƒm dacÄƒ cele douÄƒ puncte din contur sunt consecutive sau apropiate
        # GÄƒsim poziÈ›iile lor Ã®n contur
        idx1 = None
        idx2 = None
        for idx, cp in enumerate(contour_points):
            if np.allclose(cp, closest_contour_p1, atol=2):
                idx1 = idx
            if np.allclose(cp, closest_contour_p2, atol=2):
                idx2 = idx
        
        if idx1 is None or idx2 is None:
            continue
        
        # VerificÄƒm dacÄƒ punctele sunt apropiate Ã®n contur (nu la capete opuse)
        contour_distance = min(abs(idx2 - idx1), len(contour_points) - abs(idx2 - idx1))
        if contour_distance > len(contour_points) * 0.3:  # Prea departe Ã®n contur
            continue
        
        # VerificÄƒm dacÄƒ existÄƒ un gol Ã®ntre cele douÄƒ puncte din contur
        # EÈ™antionÄƒm puncte de-a lungul liniei dintre punctele din contur
        num_samples = max(30, int(segment_length / 5))
        wall_pixels = 0
        space_pixels = 0
        
        for t in np.linspace(0, 1, num_samples):
            px = int(closest_contour_p1[0] + t * (closest_contour_p2[0] - closest_contour_p1[0]))
            py = int(closest_contour_p1[1] + t * (closest_contour_p2[1] - closest_contour_p1[1]))
            
            if 0 <= px < w and 0 <= py < h:
                if walls_mask[py, px] == 255:
                    wall_pixels += 1
                else:
                    space_pixels += 1
        
        # DacÄƒ mai puÈ›in de 40% din linie este perete, existÄƒ un gol
        if wall_pixels / num_samples < 0.4:
            # VerificÄƒm cÄƒ linia nu trece prin interiorul casei
            # EÈ™antionÄƒm cÃ¢teva puncte È™i verificÄƒm cÄƒ nu sunt Ã®n interior
            valid_connection = True
            interior_count = 0
            
            for t in np.linspace(0.2, 0.8, 5):  # VerificÄƒm doar punctele din mijloc
                px = int(closest_contour_p1[0] + t * (closest_contour_p2[0] - closest_contour_p1[0]))
                py = int(closest_contour_p1[1] + t * (closest_contour_p2[1] - closest_contour_p1[1]))
                
                if 0 <= px < w and 0 <= py < h:
                    # VerificÄƒm dacÄƒ punctul este Ã®n interiorul conturului
                    point_inside = cv2.pointPolygonTest(largest_contour, (px, py), False)
                    if point_inside > 0:  # Ãn interior
                        interior_count += 1
                    
                    # VerificÄƒm cÄƒ existÄƒ pereÈ›i Ã®n jur (nu doar spaÈ›iu liber)
                    y_min = max(0, py - 15)
                    y_max = min(h, py + 16)
                    x_min = max(0, px - 15)
                    x_max = min(w, px + 16)
                    neighborhood = walls_mask[y_min:y_max, x_min:x_max]
                    if np.count_nonzero(neighborhood) < 10:  # Prea puÈ›ini pereÈ›i Ã®n jur
                        valid_connection = False
                        break
            
            # DacÄƒ mai mult de 2 puncte sunt Ã®n interior, nu desenÄƒm (ar tÄƒia colÈ›urile)
            if interior_count > 2:
                valid_connection = False
            
            if valid_connection:
                # DesenÄƒm linia Ã®ntre punctele din conturul original (nu din hull)
                cv2.line(result, closest_contour_p1, closest_contour_p2, 255, 2)
                connections_made += 1
    
    print(f"         âœ… FÄƒcute {connections_made} conexiuni pentru peretele exterior")
    
    # SalvÄƒm rezultatul
    if steps_dir:
        output_path = Path(steps_dir) / "02c_border_constrained_fill.png"
        cv2.imwrite(str(output_path), result)
        print(f"         ğŸ’¾ Salvat: {output_path.name}")
    
    return result

def interval_merging_axis_projections(walls_mask: np.ndarray, steps_dir: str = None, corridor_width: int = 5, max_vacuum_gap: int = 20) -> np.ndarray:
    """
    UneÈ™te segmentele de pereÈ›i folosind Binary Conflict Profiling.
    Algoritm robust care verificÄƒ doar segmentele adiacente È™i foloseÈ™te testele de intruziune È™i Ghost.
    
    Algoritm:
    1. Vectorizare: DetectÄƒm segmentele folosind LSD (Line Segment Detector)
    2. Grupare pe Axe: GrupÄƒm segmentele pe "È™ine" (aceeaÈ™i coordonatÄƒ Y pentru orizontale, X pentru verticale)
    3. Sortare È™i Perechi Adiacente: SortÄƒm segmentele È™i verificÄƒm doar perechile adiacente
    4. Test de Intruziune: VerificÄƒm dacÄƒ existÄƒ pereÈ›i perpendiculari Ã®ntre capete
    5. Test Ghost: VerificÄƒm dacÄƒ zona e prea albÄƒ Ã®n original (camerÄƒ)
    
    Args:
        walls_mask: Masca pereÈ›ilor (255 = perete, 0 = spaÈ›iu liber) - 02_ai_walls_closed.png
        steps_dir: Director pentru salvarea step-urilor de debug (opÈ›ional)
        corridor_width: LÄƒÈ›imea coridorului de scanare (nefolosit Ã®n acest algoritm, pÄƒstrat pentru compatibilitate)
        max_vacuum_gap: NumÄƒrul maxim de pixeli albi consecutivi permisi (default: 20)
    
    Returns:
        Masca pereÈ›ilor cu segmentele unite chirurgical
    """
    h, w = walls_mask.shape[:2]
    
    print(f"      ğŸ“ Binary Conflict Profiling: unesc segmente adiacente cu validare robustÄƒ...")
    
    result = walls_mask.copy()
    
    # ÃncÄƒrcÄƒm imaginea originalÄƒ (Ghost Layer) pentru validare
    ghost_img = None
    if steps_dir:
        original_path = Path(steps_dir) / "00_original.png"
        if original_path.exists():
            print(f"         ğŸ” ÃncÄƒrc imaginea originalÄƒ pentru validare...")
            original_img = cv2.imread(str(original_path))
            if original_img is not None:
                # Convertim la grayscale
                ghost_img = cv2.cvtColor(original_img, cv2.COLOR_BGR2GRAY)
                # RedimensionÄƒm dacÄƒ e necesar
                if ghost_img.shape[:2] != (h, w):
                    ghost_img = cv2.resize(ghost_img, (w, h))
                print(f"         âœ… Imagine originalÄƒ Ã®ncÄƒrcatÄƒ pentru validare")
    
    # Pas 1: Vectorizare prin LSD (Line Segment Detector)
    print(f"         ğŸ” Pas 1: Detectez segmente cu LSD...")
    lsd = cv2.createLineSegmentDetector(0)
    lines = lsd.detect(walls_mask)[0]
    
    if lines is None or len(lines) == 0:
        print(f"         âš ï¸ Nu s-au detectat segmente")
        return result
    
    print(f"         âœ… Detectat {len(lines)} segmente")
    
    # CreÄƒm imagini pentru vizualizare
    if steps_dir:
        vis_segments = cv2.cvtColor(walls_mask, cv2.COLOR_GRAY2BGR)
        vis_grouped = cv2.cvtColor(walls_mask, cv2.COLOR_GRAY2BGR)
        vis_connections = cv2.cvtColor(walls_mask, cv2.COLOR_GRAY2BGR)
        vis_result = cv2.cvtColor(walls_mask, cv2.COLOR_GRAY2BGR)
    
    # PaletÄƒ de culori
    colors = [
        (255, 0, 0), (0, 255, 0), (0, 0, 255), (255, 255, 0),
        (255, 0, 255), (0, 255, 255), (128, 0, 128), (255, 165, 0)
    ]
    
    # DesenÄƒm segmentele detectate
    if steps_dir:
        for i, line in enumerate(lines):
            x1, y1, x2, y2 = line[0].astype(int)
            color = colors[i % len(colors)]
            cv2.line(vis_segments, (x1, y1), (x2, y2), color, 2)
        cv2.imwrite(str(Path(steps_dir) / "02f_01_lsd_segments.png"), vis_segments)
        print(f"         ğŸ’¾ Salvat: 02f_01_lsd_segments.png")
    
    # Pas 2: Grupare pe Axe (È™ine)
    print(f"         ğŸ” Pas 2: Grupez segmente pe axe...")
    horizontal_rails = {}  # cheie: y_coord (rotunjit), valoare: lista de (x1, x2, line_idx)
    vertical_rails = {}    # cheie: x_coord (rotunjit), valoare: lista de (y1, y2, line_idx)
    
    tolerance = 3  # ToleranÈ›Äƒ pentru gruparea pe axe
    
    for line_idx, line in enumerate(lines):
        x1, y1, x2, y2 = line[0]
        
        dx = x2 - x1
        dy = y2 - y1
        length = np.sqrt(dx*dx + dy*dy)
        
        if length < 10:
            continue
        
        # VerificÄƒm dacÄƒ e orizontalÄƒ
        if abs(dy) < tolerance:
            y_coord = int((y1 + y2) / 2)
            y_key = round(y_coord / tolerance) * tolerance
            if y_key not in horizontal_rails:
                horizontal_rails[y_key] = []
            horizontal_rails[y_key].append((min(x1, x2), max(x1, x2), line_idx))
        
        # VerificÄƒm dacÄƒ e verticalÄƒ
        elif abs(dx) < tolerance:
            x_coord = int((x1 + x2) / 2)
            x_key = round(x_coord / tolerance) * tolerance
            if x_key not in vertical_rails:
                vertical_rails[x_key] = []
            vertical_rails[x_key].append((min(y1, y2), max(y1, y2), line_idx))
    
    print(f"         âœ… Grupate: {len(horizontal_rails)} È™ine orizontale, {len(vertical_rails)} È™ine verticale")
    
    if steps_dir:
        for y_key, segments in horizontal_rails.items():
            for x1, x2, line_idx in segments:
                color = colors[line_idx % len(colors)]
                cv2.line(vis_grouped, (int(x1), int(y_key)), (int(x2), int(y_key)), color, 2)
        for x_key, segments in vertical_rails.items():
            for y1, y2, line_idx in segments:
                color = colors[line_idx % len(colors)]
                cv2.line(vis_grouped, (int(x_key), int(y1)), (int(x_key), int(y2)), color, 2)
        cv2.imwrite(str(Path(steps_dir) / "02f_02_grouped_rails.png"), vis_grouped)
        print(f"         ğŸ’¾ Salvat: 02f_02_grouped_rails.png")
    
    # FuncÈ›ii helper pentru validare
    def test_intrusion(start_pt, end_pt, walls_mask_image):
        """
        Test de Intruziune: VerificÄƒ dacÄƒ Ã®ntre capete existÄƒ pereÈ›i perpendiculari.
        """
        # EÈ™antionÄƒm puncte de-a lungul liniei
        dx = end_pt[0] - start_pt[0]
        dy = end_pt[1] - start_pt[1]
        dist = np.sqrt(dx*dx + dy*dy)
        num_samples = max(10, int(dist / 5))
        perpendicular_walls = 0
        
        for t in np.linspace(0.1, 0.9, num_samples):  # EvitÄƒm capetele
            px = int(start_pt[0] + t * dx)
            py = int(start_pt[1] + t * dy)
            
            if 0 <= px < w and 0 <= py < h:
                # VerificÄƒm pe o bandÄƒ perpendicularÄƒ pe linie
                check_radius = 5
                if abs(dx) > abs(dy):  # Linie orizontalÄƒ
                    for offset in range(-check_radius, check_radius + 1):
                        check_y = py + offset
                        if 0 <= check_y < h and abs(offset) > 2:
                            if walls_mask_image[check_y, px] == 255:
                                perpendicular_walls += 1
                                break
                else:  # Linie verticalÄƒ
                    for offset in range(-check_radius, check_radius + 1):
                        check_x = px + offset
                        if 0 <= check_x < w and abs(offset) > 2:
                            if walls_mask_image[py, check_x] == 255:
                                perpendicular_walls += 1
                                break
        
        # DacÄƒ mai mult de 20% din eÈ™antioane au pereÈ›i perpendiculari, respingem
        return perpendicular_walls <= num_samples * 0.2
    
    def test_ghost_profile(start_pt, end_pt, ghost_image):
        """
        Test Ghost: VerificÄƒ profilul de intensitate Ã®n imaginea originalÄƒ.
        DacÄƒ zona e prea albÄƒ (camerÄƒ), respinge conexiunea.
        """
        if ghost_image is None:
            return True
        
        # CreÄƒm o mascÄƒ temporarÄƒ cu linia propusÄƒ
        temp_mask = np.zeros(ghost_image.shape[:2], dtype=np.uint8)
        cv2.line(temp_mask, start_pt, end_pt, 255, 3)
        
        # Extragem pixelii de sub linie
        path_pixels = ghost_image[temp_mask > 0]
        
        if len(path_pixels) == 0:
            return False
        
        # VerificÄƒm raportul de alb (vid)
        white_ratio = np.sum(path_pixels > 245) / len(path_pixels)
        
        # DacÄƒ mai mult de 85% e alb, e camerÄƒ
        return white_ratio < 0.85
    
    # Pas 3: Sortare È™i verificare perechi adiacente
    print(f"         ğŸ” Pas 3: Sortez segmente È™i verific perechi adiacente...")
    connections_made = 0
    
    # ProcesÄƒm È™inele orizontale
    for y_coord, segments in horizontal_rails.items():
        if len(segments) < 2:
            continue
        
        # SortÄƒm segmentele de la stÃ¢nga la dreapta
        segments_sorted = sorted(segments, key=lambda x: x[0])
        
        # VerificÄƒm doar perechile adiacente
        for i in range(len(segments_sorted) - 1):
            x1_end, x1_start, line_idx1 = segments_sorted[i]
            x2_start, x2_end, line_idx2 = segments_sorted[i + 1]
            
            # Capetele segmentelor
            end_pt = (int(x1_end), int(y_coord))
            start_pt = (int(x2_start), int(y_coord))
            
            # VerificÄƒm dacÄƒ existÄƒ un gap
            if x2_start > x1_end:
                gap_size = x2_start - x1_end
                
                # IgnorÄƒm gap-uri prea mari (probabil nu sunt pe aceeaÈ™i axÄƒ)
                if gap_size > w * 0.3:
                    continue
                
                # TEST DE INTRUZIUNE: VerificÄƒm dacÄƒ existÄƒ pereÈ›i perpendiculari
                if not test_intrusion(end_pt, start_pt, walls_mask):
                    if steps_dir:
                        cv2.line(vis_connections, end_pt, start_pt, (0, 0, 255), 2)  # RoÈ™u
                    continue
                
                # TEST GHOST: VerificÄƒm profilul de intensitate
                if not test_ghost_profile(end_pt, start_pt, ghost_img):
                    if steps_dir:
                        cv2.line(vis_connections, end_pt, start_pt, (0, 165, 255), 2)  # Portocaliu
                    continue
                
                # Ambele teste au trecut - unim!
                cv2.line(result, end_pt, start_pt, 255, 3)
                
                if steps_dir:
                    cv2.line(vis_connections, end_pt, start_pt, (0, 255, 0), 2)  # Verde
                    cv2.line(vis_result, end_pt, start_pt, (0, 255, 0), 2)
                
                connections_made += 1
    
    # ProcesÄƒm È™inele verticale (aceeaÈ™i logicÄƒ)
    for x_coord, segments in vertical_rails.items():
        if len(segments) < 2:
            continue
        
        # SortÄƒm segmentele de sus Ã®n jos
        segments_sorted = sorted(segments, key=lambda x: x[0])
        
        # VerificÄƒm doar perechile adiacente
        for i in range(len(segments_sorted) - 1):
            y1_end, y1_start, line_idx1 = segments_sorted[i]
            y2_start, y2_end, line_idx2 = segments_sorted[i + 1]
            
            # Capetele segmentelor
            end_pt = (int(x_coord), int(y1_end))
            start_pt = (int(x_coord), int(y2_start))
            
            # VerificÄƒm dacÄƒ existÄƒ un gap
            if y2_start > y1_end:
                gap_size = y2_start - y1_end
                
                # IgnorÄƒm gap-uri prea mari
                if gap_size > h * 0.3:
                    continue
                
                # TEST DE INTRUZIUNE
                if not test_intrusion(end_pt, start_pt, walls_mask):
                    if steps_dir:
                        cv2.line(vis_connections, end_pt, start_pt, (0, 0, 255), 2)  # RoÈ™u
                    continue
                
                # TEST GHOST
                if not test_ghost_profile(end_pt, start_pt, ghost_img):
                    if steps_dir:
                        cv2.line(vis_connections, end_pt, start_pt, (0, 165, 255), 2)  # Portocaliu
                    continue
                
                # Ambele teste au trecut - unim!
                cv2.line(result, end_pt, start_pt, 255, 3)
                
                if steps_dir:
                    cv2.line(vis_connections, end_pt, start_pt, (0, 255, 0), 2)  # Verde
                    cv2.line(vis_result, end_pt, start_pt, (0, 255, 0), 2)
                
                connections_made += 1
    
    print(f"         âœ… Unite {connections_made} perechi de segmente adiacente")
    
    # SalvÄƒm rezultatele
    if steps_dir:
        cv2.imwrite(str(Path(steps_dir) / "02f_03_connections.png"), vis_connections)
        print(f"         ğŸ’¾ Salvat: 02f_03_connections.png (verde=valide, roÈ™u=intruziune, portocaliu=ghost)")
        
        cv2.imwrite(str(Path(steps_dir) / "02f_04_interval_merging_result.png"), vis_result)
        print(f"         ğŸ’¾ Salvat: 02f_04_interval_merging_result.png")
        
        cv2.imwrite(str(Path(steps_dir) / "02f_05_final_result.png"), result)
        print(f"         ğŸ’¾ Salvat: 02f_05_final_result.png (rezultat final binar)")
    
    return result

def preprocess_image_for_ocr(image: np.ndarray) -> np.ndarray:
    """
    PreproceseazÄƒ imaginea pentru a Ã®mbunÄƒtÄƒÈ›i detecÈ›ia OCR.
    
    AplicÄƒ:
    - Conversie la grayscale dacÄƒ este color
    - Contrast enhancement (CLAHE)
    - Sharpening
    - Thresholding adaptiv pentru text clar
    
    Args:
        image: Imaginea de preprocesat (BGR sau grayscale)
    
    Returns:
        Imaginea preprocesatÄƒ (grayscale)
    """
    # Convertim la grayscale dacÄƒ este color
    if len(image.shape) == 3:
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    else:
        gray = image.copy()
    
    # 1. Contrast enhancement cu CLAHE (Contrast Limited Adaptive Histogram Equalization)
    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))
    enhanced = clahe.apply(gray)
    
    # 2. Sharpening pentru a face textul mai clar
    kernel_sharpen = np.array([[-1, -1, -1],
                               [-1,  9, -1],
                               [-1, -1, -1]])
    sharpened = cv2.filter2D(enhanced, -1, kernel_sharpen)
    
    # 3. Denoising (reducere zgomot)
    denoised = cv2.fastNlMeansDenoising(sharpened, None, h=10, templateWindowSize=7, searchWindowSize=21)
    
    # 4. Thresholding adaptiv pentru text clar (opÈ›ional, dar poate ajuta)
    # Nu aplicÄƒm thresholding direct, ci pÄƒstrÄƒm imaginea grayscale pentru OCR
    # OCR-ul funcÈ›ioneazÄƒ mai bine pe imagini grayscale cu contrast bun
    
    return denoised

def _reconstruct_word_from_chars(chars_list, search_terms, max_horizontal_distance=50, max_vertical_distance=10):
    """
    Reconstituie cuvÃ¢ntul complet din caracterele detectate individual.
    
    Args:
        chars_list: Lista de caractere detectate: [(x, y, width, height, text, conf), ...]
        search_terms: Lista de termeni de cÄƒutat
        max_horizontal_distance: DistanÈ›a maximÄƒ orizontalÄƒ Ã®ntre caractere pentru a fi considerate parte din acelaÈ™i cuvÃ¢nt
        max_vertical_distance: DistanÈ›a maximÄƒ verticalÄƒ Ã®ntre caractere pentru a fi considerate pe aceeaÈ™i linie
    
    Returns:
        Lista de cuvinte reconstituite: [(x, y, width, height, text, conf), ...]
    """
    if not chars_list:
        return []
    
    # SortÄƒm caracterele de la stÃ¢nga la dreapta, apoi de sus Ã®n jos
    sorted_chars = sorted(chars_list, key=lambda c: (c[1], c[0]))  # Sort by y, then x
    
    reconstructed_words = []
    
    # Pentru fiecare termen cÄƒutat, Ã®ncercÄƒm sÄƒ reconstituim cuvÃ¢ntul
    for search_term in search_terms:
        search_term_lower = search_term.lower()
        term_length = len(search_term_lower)
        
        # GrupÄƒm caracterele care sunt pe aceeaÈ™i linie (aproximativ)
        lines = []
        current_line = [sorted_chars[0]]
        
        for char in sorted_chars[1:]:
            prev_char = current_line[-1]
            # VerificÄƒm dacÄƒ caracterul este pe aceeaÈ™i linie (similar y)
            y_diff = abs(char[1] - prev_char[1])
            if y_diff <= max_vertical_distance:
                current_line.append(char)
            else:
                if current_line:
                    lines.append(current_line)
                current_line = [char]
        
        if current_line:
            lines.append(current_line)
        
        # Pentru fiecare linie, Ã®ncercÄƒm sÄƒ gÄƒsim secvenÈ›a care formeazÄƒ cuvÃ¢ntul
        for line in lines:
            # SortÄƒm caracterele din linie de la stÃ¢nga la dreapta
            line_sorted = sorted(line, key=lambda c: c[0])
            
            # CÄƒutÄƒm secvenÈ›e de caractere care pot forma cuvÃ¢ntul
            for start_idx in range(len(line_sorted)):
                # VerificÄƒm dacÄƒ primul caracter face parte din cuvÃ¢ntul cÄƒutat
                first_char_text = line_sorted[start_idx][4].lower()  # text
                
                # VerificÄƒm dacÄƒ primul caracter se potriveÈ™te cu prima literÄƒ din termen
                # OCR poate detecta un singur caracter sau un grup de caractere
                if search_term_lower[0] not in first_char_text:
                    continue
                
                # Construim cuvÃ¢ntul de la acest caracter
                used_indices = [start_idx]
                x_start = line_sorted[start_idx][0]
                y_start = line_sorted[start_idx][1]
                x_end = x_start + line_sorted[start_idx][2]
                y_end = y_start + line_sorted[start_idx][3]
                conf_sum = line_sorted[start_idx][5]
                conf_count = 1
                
                # CÄƒutÄƒm caracterele urmÄƒtoare
                # Construim cuvÃ¢ntul caracter cu caracter, verificÃ¢nd dacÄƒ fiecare caracter detectat
                # se potriveÈ™te cu urmÄƒtoarea literÄƒ din termenul cÄƒutat
                reconstructed_chars = list(first_char_text)  # Lista de caractere reconstituite
                
                for term_idx in range(1, term_length):
                    target_char = search_term_lower[term_idx]
                    
                    # VerificÄƒm dacÄƒ avem deja caracterul Ã®n reconstructed
                    if term_idx < len(reconstructed_chars):
                        # Caracterul a fost deja adÄƒugat (poate fi parte dintr-un grup de caractere detectat)
                        continue
                    
                    # CÄƒutÄƒm urmÄƒtorul caracter care se potriveÈ™te
                    best_match = None
                    best_distance = float('inf')
                    
                    for char_idx, char in enumerate(line_sorted):
                        if char_idx in used_indices:
                            continue
                        
                        char_text = char[4].lower()
                        char_x = char[0]
                        
                        # VerificÄƒm dacÄƒ caracterul conÈ›ine litera cÄƒutatÄƒ
                        # Poate fi un singur caracter sau un grup de caractere
                        if target_char in char_text:
                            # VerificÄƒm distanÈ›a orizontalÄƒ faÈ›Äƒ de ultimul caracter folosit
                            last_char = line_sorted[used_indices[-1]]
                            last_x_end = last_char[0] + last_char[2]
                            distance = char_x - last_x_end
                            
                            # Caracterul trebuie sÄƒ fie la dreapta ultimului È™i la o distanÈ›Äƒ rezonabilÄƒ
                            if distance >= 0 and distance <= max_horizontal_distance:
                                if distance < best_distance:
                                    best_match = char_idx
                                    best_distance = distance
                    
                    if best_match is not None:
                        char = line_sorted[best_match]
                        char_text_lower = char[4].lower()
                        
                        # AdÄƒugÄƒm doar caracterul care se potriveÈ™te (nu Ã®ntregul grup)
                        # DacÄƒ grupul conÈ›ine mai multe caractere, adÄƒugÄƒm doar primul care se potriveÈ™te
                        char_added = False
                        for c in char_text_lower:
                            if c == target_char and len(reconstructed_chars) < term_length:
                                reconstructed_chars.append(c)
                                char_added = True
                                break
                        
                        # DacÄƒ nu am gÄƒsit caracterul exact, Ã®ncercÄƒm sÄƒ adÄƒugÄƒm primul caracter din grup
                        # (poate OCR a detectat greÈ™it sau caracterul este similar)
                        if not char_added and len(reconstructed_chars) < term_length:
                            # AdÄƒugÄƒm primul caracter din grup dacÄƒ este similar cu cel cÄƒutat
                            if len(char_text_lower) > 0:
                                reconstructed_chars.append(char_text_lower[0])
                                char_added = True
                        
                        if char_added:
                            used_indices.append(best_match)
                            x_end = char[0] + char[2]
                            y_end = max(y_end, char[1] + char[3])
                            conf_sum += char[5]
                            conf_count += 1
                        else:
                            # Nu am putut adÄƒuga caracterul, Ã®ncercÄƒm sÄƒ continuÄƒm
                            break
                    else:
                        # Nu am gÄƒsit urmÄƒtorul caracter, Ã®ncercÄƒm sÄƒ continuÄƒm cu ce avem
                        break
                
                # Reconstruim string-ul din caracterele colectate
                reconstructed = ''.join(reconstructed_chars[:term_length])
                
                # VerificÄƒm dacÄƒ cuvÃ¢ntul reconstituit se potriveÈ™te cu termenul cÄƒutat
                if reconstructed == search_term_lower:
                    # CalculÄƒm bounding box-ul pentru Ã®ntregul cuvÃ¢nt
                    width = x_end - x_start
                    height = y_end - y_start
                    avg_conf = conf_sum / conf_count if conf_count > 0 else 0
                    
                    if avg_conf > 60:
                        reconstructed_words.append((x_start, y_start, width, height, search_term, avg_conf))
                        print(f"         ğŸ”¤ Reconstituit cuvÃ¢ntul '{search_term}' din {conf_count} caractere (confidenÈ›Äƒ medie: {avg_conf:.1f}%)")
                        break  # Nu mai cÄƒutÄƒm Ã®n aceastÄƒ linie dacÄƒ am gÄƒsit deja cuvÃ¢ntul
    
    return reconstructed_words


def run_ocr_on_zones(image: np.ndarray, search_terms: list, steps_dir: str = None, 
                     grid_rows: int = 3, grid_cols: int = 3, zoom_factor: float = 2.0) -> list:
    """
    RuleazÄƒ OCR pe zone diferite ale imaginii cu zoom pentru a detecta mai bine textul mic.
    
    Ãmparte imaginea Ã®n grid È™i ruleazÄƒ OCR pe fiecare zonÄƒ, eventual cu zoom.
    DacÄƒ detecteazÄƒ caractere individuale, Ã®ncearcÄƒ sÄƒ reconstituie cuvÃ¢ntul complet.
    
    Args:
        image: Imaginea de analizat (grayscale sau BGR)
        search_terms: Lista de termeni de cÄƒutat
        steps_dir: Director pentru debug (opÈ›ional)
        grid_rows: NumÄƒr de rÃ¢nduri Ã®n grid
        grid_cols: NumÄƒr de coloane Ã®n grid
        zoom_factor: Factor de zoom pentru fiecare zonÄƒ (1.0 = fÄƒrÄƒ zoom)
    
    Returns:
        Lista de text_boxes detectate: [(x, y, width, height, text, conf), ...]
    """
    if not TESSERACT_AVAILABLE:
        return []
    
    h, w = image.shape[:2]
    text_boxes = []
    all_chars = []  # ColectÄƒm toate caracterele detectate pentru reconstituire
    all_detections = []  # ColectÄƒm TOATE detecÈ›iile OCR pentru debug (nu doar cele care se potrivesc)
    
    # PreprocesÄƒm Ã®ntreaga imagine
    processed_full = preprocess_image_for_ocr(image)
    
    # 1. OCR pe Ã®ntreaga imagine preprocesatÄƒ
    print(f"         ğŸ“ OCR pe Ã®ntreaga imagine (preprocesatÄƒ)...")
    ocr_data_full = pytesseract.image_to_data(processed_full, output_type=pytesseract.Output.DICT, lang='deu+eng')
    
    for i, text in enumerate(ocr_data_full['text']):
        if text.strip():
            text_clean = text.strip()
            text_lower = text_clean.lower()
            
            x = ocr_data_full['left'][i]
            y = ocr_data_full['top'][i]
            width = ocr_data_full['width'][i]
            height = ocr_data_full['height'][i]
            conf = ocr_data_full['conf'][i]
            
            # ColectÄƒm TOATE detecÈ›iile pentru debug
            all_detections.append((x, y, width, height, text_clean, conf))
            
            found_term = None
            for term in search_terms:
                term_lower = term.lower()
                # VerificÄƒm doar match exact (case-insensitive)
                # Nu acceptÄƒm fragmente - doar cuvÃ¢ntul complet "terasa" sau variantele sale exacte
                if term_lower == text_lower:
                    found_term = term
                    break
            
            if found_term:
                if conf > 60:
                    text_boxes.append((x, y, width, height, text_clean, conf))
                    print(f"         âœ… Detectat (full): '{text_clean}' la ({x}, {y}) cu confidenÈ›Äƒ {conf:.1f}%")
            else:
                # ColectÄƒm È™i caracterele individuale pentru reconstituire
                # VerificÄƒm dacÄƒ caracterul face parte din unul dintre termenii cÄƒutaÈ›i
                for term in search_terms:
                    term_lower = term.lower()
                    if any(char in text_lower for char in term_lower):
                        all_chars.append((x, y, width, height, text_clean, conf))
                        break
    
    # 2. DacÄƒ nu am gÄƒsit nimic sau am gÄƒsit doar rezultate cu confidence scÄƒzut, Ã®ncercÄƒm pe zone
    if not text_boxes or (text_boxes and max(box[5] for box in text_boxes) < 60):
        print(f"         ğŸ” ÃmpÄƒrÈ›im imaginea Ã®n {grid_rows}x{grid_cols} zone pentru OCR detaliat...")
        
        overlap = 50  # pixeli de overlap Ã®ntre zone
        
        # CalculÄƒm dimensiunile de bazÄƒ ale unei zone (folosind diviziune realÄƒ pentru acoperire completÄƒ)
        zone_height_base = h / grid_rows
        zone_width_base = w / grid_cols
        
        for row in range(grid_rows):
            for col in range(grid_cols):
                # CalculÄƒm coordonatele zonei astfel Ã®ncÃ¢t sÄƒ acopere complet imaginea
                # Prima zonÄƒ Ã®ncepe de la 0
                if row == 0:
                    y_start = 0
                else:
                    # Zonele intermediare au overlap cu zona anterioarÄƒ
                    y_start = max(0, int(row * zone_height_base) - overlap)
                
                # Ultima zonÄƒ merge pÃ¢nÄƒ la marginea imaginii (h)
                if row == grid_rows - 1:
                    y_end = h
                else:
                    # Zonele intermediare se terminÄƒ cu overlap pentru urmÄƒtoarea zonÄƒ
                    y_end = min(h, int((row + 1) * zone_height_base) + overlap)
                
                # AcelaÈ™i lucru pentru coloane
                if col == 0:
                    x_start = 0
                else:
                    x_start = max(0, int(col * zone_width_base) - overlap)
                
                if col == grid_cols - 1:
                    x_end = w
                else:
                    x_end = min(w, int((col + 1) * zone_width_base) + overlap)
                
                # VerificÄƒm cÄƒ zona este validÄƒ
                if x_start >= x_end or y_start >= y_end:
                    print(f"         âš ï¸ Zona ({row+1},{col+1}) invalidÄƒ: x=[{x_start},{x_end}), y=[{y_start},{y_end})")
                    continue
                
                # Extragem zona
                zone = image[y_start:y_end, x_start:x_end]
                
                if zone.size == 0:
                    continue
                
                # Dimensiunile zonei originale (Ã®nainte de zoom)
                zone_orig_h, zone_orig_w = zone.shape[:2]
                
                # PreprocesÄƒm zona
                zone_processed = preprocess_image_for_ocr(zone)
                
                # AplicÄƒm zoom dacÄƒ este necesar
                if zoom_factor > 1.0:
                    zone_h_scaled = int(zone_processed.shape[0] * zoom_factor)
                    zone_w_scaled = int(zone_processed.shape[1] * zoom_factor)
                    zone_zoomed = cv2.resize(zone_processed, (zone_w_scaled, zone_h_scaled), 
                                            interpolation=cv2.INTER_CUBIC)
                else:
                    zone_zoomed = zone_processed
                
                # SalvÄƒm zonele procesate pentru debug (toate zonele)
                if steps_dir:
                    debug_path = Path(steps_dir) / f"02g_zone_{row+1}_{col+1}_processed.png"
                    cv2.imwrite(str(debug_path), zone_zoomed)
                    # SalvÄƒm È™i zona originalÄƒ pentru comparaÈ›ie
                    debug_path_orig = Path(steps_dir) / f"02g_zone_{row+1}_{col+1}_original.png"
                    cv2.imwrite(str(debug_path_orig), zone)
                
                # OCR pe zonÄƒ
                try:
                    ocr_data_zone = pytesseract.image_to_data(zone_zoomed, output_type=pytesseract.Output.DICT, lang='deu+eng')
                    
                    for i, text in enumerate(ocr_data_zone['text']):
                        if text.strip():
                            text_clean = text.strip()
                            text_lower = text_clean.lower()
                            
                            # Coordonatele Ã®n zona zoomed
                            rel_x_zoomed = ocr_data_zone['left'][i]
                            rel_y_zoomed = ocr_data_zone['top'][i]
                            rel_width_zoomed = ocr_data_zone['width'][i]
                            rel_height_zoomed = ocr_data_zone['height'][i]
                            
                            # Convertim coordonatele din zona zoomed la zona originalÄƒ (fÄƒrÄƒ zoom)
                            if zoom_factor > 1.0:
                                # Coordonatele relative Ã®n zona originalÄƒ (fÄƒrÄƒ zoom)
                                rel_x = rel_x_zoomed / zoom_factor
                                rel_y = rel_y_zoomed / zoom_factor
                                rel_width = rel_width_zoomed / zoom_factor
                                rel_height = rel_height_zoomed / zoom_factor
                            else:
                                rel_x = rel_x_zoomed
                                rel_y = rel_y_zoomed
                                rel_width = rel_width_zoomed
                                rel_height = rel_height_zoomed
                            
                            # Convertim la coordonatele absolute Ã®n imaginea completÄƒ
                            orig_x = int(rel_x) + x_start
                            orig_y = int(rel_y) + y_start
                            orig_width = int(rel_width)
                            orig_height = int(rel_height)
                            
                            # VerificÄƒm cÄƒ coordonatele sunt Ã®n limitele imaginii
                            orig_x = max(0, min(orig_x, w - 1))
                            orig_y = max(0, min(orig_y, h - 1))
                            orig_width = min(orig_width, w - orig_x)
                            orig_height = min(orig_height, h - orig_y)
                            
                            conf = ocr_data_zone['conf'][i]
                            
                            # ColectÄƒm TOATE detecÈ›iile pentru debug
                            all_detections.append((orig_x, orig_y, orig_width, orig_height, text_clean, conf))
                            
                            found_term = None
                            for term in search_terms:
                                term_lower = term.lower()
                                # VerificÄƒm doar match exact (case-insensitive)
                                # Nu acceptÄƒm fragmente - doar cuvÃ¢ntul complet "terasa" sau variantele sale exacte
                                if term_lower == text_lower:
                                    found_term = term
                                    break
                            
                            if found_term:
                                if conf > 60:
                                    text_boxes.append((orig_x, orig_y, orig_width, orig_height, text_clean, conf))
                                    print(f"         âœ… Detectat (zona {row+1},{col+1}): '{text_clean}' la ({orig_x}, {orig_y}) cu confidenÈ›Äƒ {conf:.1f}%")
                            else:
                                # ColectÄƒm È™i caracterele individuale pentru reconstituire
                                # VerificÄƒm dacÄƒ caracterul face parte din unul dintre termenii cÄƒutaÈ›i
                                for term in search_terms:
                                    term_lower = term.lower()
                                    if any(char in text_lower for char in term_lower):
                                        all_chars.append((orig_x, orig_y, orig_width, orig_height, text_clean, conf))
                                        break
                except Exception as e:
                    print(f"         âš ï¸ Eroare OCR pe zona {row+1},{col+1}: {e}")
                    continue
    
    # 3. DacÄƒ nu am gÄƒsit cuvinte complete, Ã®ncercÄƒm sÄƒ reconstituim din caractere
    if not text_boxes and all_chars:
        print(f"         ğŸ”¤ Am detectat {len(all_chars)} caractere individuale. Ãncerc sÄƒ reconstitui cuvÃ¢ntul...")
        reconstructed = _reconstruct_word_from_chars(all_chars, search_terms)
        text_boxes.extend(reconstructed)
    
    # 4. GenerÄƒm imagine de debug cu TOATE detecÈ›iile OCR
    if steps_dir and all_detections:
        # Convertim imaginea la BGR dacÄƒ este grayscale
        if len(image.shape) == 2:
            debug_img = cv2.cvtColor(image, cv2.COLOR_GRAY2BGR)
        elif len(image.shape) == 3 and image.shape[2] == 3:
            debug_img = image.copy()
        else:
            debug_img = cv2.cvtColor(image, cv2.COLOR_GRAY2BGR)
        
        print(f"         ğŸ“Š GenerÃ¢nd imagine de debug cu {len(all_detections)} detecÈ›ii OCR...")
        
        # DesenÄƒm toate detecÈ›iile
        for x, y, width, height, text, conf in all_detections:
            # VerificÄƒm dacÄƒ detecÈ›ia se potriveÈ™te cu unul dintre termenii cÄƒutaÈ›i
            text_lower = text.lower()
            is_match = False
            for term in search_terms:
                if term.lower() == text_lower:
                    is_match = True
                    break
            
            # Culoare: verde pentru match-uri, albastru pentru restul
            color = (0, 255, 0) if is_match else (255, 0, 0)
            thickness = 3 if is_match else 2
            
            # DesenÄƒm dreptunghiul
            cv2.rectangle(debug_img, (x, y), (x + width, y + height), color, thickness)
            
            # DesenÄƒm textul cu procentajul
            label = f"{text} ({conf:.0f}%)"
            
            # CalculÄƒm dimensiunea fontului Ã®n funcÈ›ie de Ã®nÄƒlÈ›imea detecÈ›iei
            font_scale = max(0.4, height / 30.0)
            font_thickness = max(1, int(font_scale * 2))
            
            # CalculÄƒm dimensiunea textului pentru a-l poziÈ›iona corect
            (text_width, text_height), baseline = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, font_scale, font_thickness)
            
            # PoziÈ›ionÄƒm textul deasupra dreptunghiului (sau dedesubt dacÄƒ nu Ã®ncape)
            text_y = y - 5 if y - 5 > text_height else y + height + text_height + 5
            
            # DesenÄƒm fundal pentru text (pentru lizibilitate)
            cv2.rectangle(debug_img, 
                         (x, text_y - text_height - baseline), 
                         (x + text_width, text_y + baseline), 
                         color, -1)
            
            # DesenÄƒm textul
            cv2.putText(debug_img, label, (x, text_y), 
                       cv2.FONT_HERSHEY_SIMPLEX, font_scale, (255, 255, 255), font_thickness)
        
        # SalvÄƒm imaginea de debug
        debug_path = Path(steps_dir) / "02g_02_all_ocr_detections.png"
        cv2.imwrite(str(debug_path), debug_img)
        print(f"         ğŸ’¾ Salvat: 02g_02_all_ocr_detections.png ({len(all_detections)} detecÈ›ii)")
    
    return text_boxes

def fill_terrace_room(walls_mask: np.ndarray, steps_dir: str = None) -> np.ndarray:
    """
    DetecteazÄƒ cuvÃ¢ntul "terasa" (sau variante Ã®n germanÄƒ) Ã®n plan È™i umple camera respectivÄƒ cu flood fill.
    
    FoloseÈ™te overlay-ul de 50% (ghost) pentru a detecta textul, apoi face flood fill
    Ã®n zona respectivÄƒ pentru a Ã®nchide camera.
    
    Args:
        walls_mask: Masca pereÈ›ilor (255 = perete, 0 = spaÈ›iu liber)
        steps_dir: Director pentru salvarea step-urilor de debug (opÈ›ional)
    
    Returns:
        Masca pereÈ›ilor cu camera terasei umplutÄƒ (dacÄƒ a fost detectatÄƒ)
    """
    # Folosim o metodÄƒ alternativÄƒ dacÄƒ OCR nu este disponibil
    use_ocr = TESSERACT_AVAILABLE
    if not use_ocr:
        print(f"      âš ï¸ pytesseract nu este disponibil. Folosesc metoda alternativÄƒ de detectare text.")
    
    h, w = walls_mask.shape[:2]
    result = walls_mask.copy()
    
    print(f"      ğŸ¡ Detectez È™i umplu camere (terasa/etc)...")
    
    # Pas 1: ÃncÄƒrcÄƒm overlay-ul sau original-ul pentru OCR
    overlay_path = None
    original_path = None
    if steps_dir:
        overlay_path = Path(steps_dir) / "02d_walls_closed_overlay.png"
        original_path = Path(steps_dir) / "00_original.png"
    
    # PreferÄƒm imaginea originalÄƒ pentru detectarea textului (textul este mai clar acolo)
    ocr_image = None
    if original_path and original_path.exists():
        ocr_image = cv2.imread(str(original_path), cv2.IMREAD_COLOR)
        if ocr_image is None:
            ocr_image = cv2.imread(str(original_path), cv2.IMREAD_GRAYSCALE)
            if ocr_image is not None:
                ocr_image = cv2.cvtColor(ocr_image, cv2.COLOR_GRAY2BGR)
        print(f"         ğŸ“¸ Folosesc original pentru OCR: {original_path.name}")
    elif overlay_path and overlay_path.exists():
        ocr_image = cv2.imread(str(overlay_path), cv2.IMREAD_COLOR)
        print(f"         ğŸ“¸ Folosesc overlay pentru OCR (fallback): {overlay_path.name}")
    
    if ocr_image is None:
        print(f"         âš ï¸ Nu s-a gÄƒsit overlay sau original. Skip detectarea terasei.")
        return result
    
    # RedimensionÄƒm dacÄƒ este necesar
    if ocr_image.shape[:2] != (h, w):
        ocr_image = cv2.resize(ocr_image, (w, h))
    
    # Pas 2: DetectÄƒm textul folosind OCR cu preprocesare È™i analizÄƒ pe zone
    print(f"         ğŸ” Pas 1: Detectez text (terasa/etc)...")
    
    # Variante ale cuvÃ¢ntului "terasa" (fÄƒrÄƒ "erdgeschoss" care Ã®nseamnÄƒ parter)
    search_terms = [
        "terrasse", "Terrasse", "TERRASSE", "terasa", "Terasa", "TERASA",
        "terrace", "Terrace", "TERRACE",  # englezÄƒ
        "terrasa", "Terrasa", "TERRASA",  # variante
        "terras", "Terras", "TERRAS"  # variante scurte
    ]
    
    text_found = False
    text_boxes = []
    
    try:
        if use_ocr:
            # Metoda Ã®mbunÄƒtÄƒÈ›itÄƒ: OCR cu preprocesare È™i analizÄƒ pe zone
            print(f"         ğŸ“ Folosesc OCR cu preprocesare È™i analizÄƒ pe zone...")
            
            # Salvez imaginea preprocesatÄƒ pentru debug
            if steps_dir:
                processed_img = preprocess_image_for_ocr(ocr_image)
                cv2.imwrite(str(Path(steps_dir) / "02g_00_preprocessed.png"), processed_img)
                print(f"         ğŸ’¾ Salvat: 02g_00_preprocessed.png (imagine preprocesatÄƒ)")
            
            # RuleazÄƒ OCR pe zone cu zoom
            text_boxes = run_ocr_on_zones(ocr_image, search_terms, steps_dir, 
                                         grid_rows=3, grid_cols=3, zoom_factor=2.0)
            
            if text_boxes:
                text_found = True
        else:
            # Metoda 2: FÄ‚RÄ‚ OCR nu putem identifica specific cuvÃ¢ntul "terasa"
            # Deci nu mai detectÄƒm zone de text generic, ci doar returnÄƒm fÄƒrÄƒ sÄƒ facem nimic
            print(f"         âš ï¸ FÄƒrÄƒ OCR nu pot identifica specific cuvÃ¢ntul 'terasa'.")
            print(f"         âš ï¸ Metoda alternativÄƒ este dezactivatÄƒ - necesitÄƒ OCR pentru identificare precisÄƒ.")
            text_found = False
            text_boxes = []
        
        # SelectÄƒm rezultatul cu confidence maxim (dacÄƒ existÄƒ)
        if text_boxes:
            # SortÄƒm dupÄƒ confidence (descrescÄƒtor) È™i luÄƒm primul
            text_boxes.sort(key=lambda box: box[5], reverse=True)  # box[5] = confidence
            best_box = text_boxes[0]
            text_boxes = [best_box]  # PÄƒstrÄƒm doar cel mai bun rezultat
            print(f"         ğŸ¯ Selectat rezultatul cu confidence maxim: '{best_box[4]}' cu {best_box[5]:.1f}%")
        
        if not text_found:
            print(f"         âš ï¸ Nu s-a detectat text (terasa/etc) Ã®n plan sau toate au confidence < 60%.")
            if steps_dir:
                vis_ocr = ocr_image.copy()
                cv2.imwrite(str(Path(steps_dir) / "02g_01_ocr_result.png"), vis_ocr)
                print(f"         ğŸ’¾ Salvat: 02g_01_ocr_result.png")
            return result
        
        # Pas 3: VizualizÄƒm textul detectat
        if steps_dir:
            vis_ocr = ocr_image.copy()
            for x, y, width, height, text, conf in text_boxes:
                cv2.rectangle(vis_ocr, (x, y), (x + width, y + height), (0, 255, 0), 2)
                cv2.putText(vis_ocr, f"{text} ({conf:.0f}%)", (x, y - 5), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)
            cv2.imwrite(str(Path(steps_dir) / "02g_01_ocr_result.png"), vis_ocr)
            print(f"         ğŸ’¾ Salvat: 02g_01_ocr_result.png (text detectat)")
        
        # Pas 4: Pentru fiecare text detectat, gÄƒsim zona camerei È™i facem flood fill
        print(f"         ğŸ” Pas 2: GÄƒsesc zona camerei È™i fac flood fill...")
        
        # ÃncÄƒrcÄƒm overlay-ul combinat (pereti + original cu 50% transparency)
        overlay_combined = None
        if steps_dir:
            overlay_path = Path(steps_dir) / "02d_walls_closed_overlay.png"
            if overlay_path.exists():
                overlay_combined = cv2.imread(str(overlay_path), cv2.IMREAD_COLOR)
                if overlay_combined is not None:
                    # RedimensionÄƒm dacÄƒ este necesar
                    if overlay_combined.shape[:2] != (h, w):
                        overlay_combined = cv2.resize(overlay_combined, (w, h))
                    print(f"         ğŸ“¸ Folosesc overlay combinat (pereti + original 50%) pentru flood fill")
                else:
                    print(f"         âš ï¸ Nu pot Ã®ncÄƒrca overlay-ul. Folosesc walls_mask simplu.")
        
        # DacÄƒ nu avem overlay, folosim walls_mask simplu
        if overlay_combined is None:
            # CreÄƒm o mascÄƒ pentru spaÈ›iile libere (inversul pereÈ›ilor)
            spaces_mask = cv2.bitwise_not(walls_mask)
        else:
            # Convertim overlay-ul la grayscale
            overlay_gray = cv2.cvtColor(overlay_combined, cv2.COLOR_BGR2GRAY)
            
            # BinarizÄƒm overlay-ul pentru a identifica pereÈ›ii
            # PereÈ›ii Ã®n overlay sunt mai Ã®nchiÈ™i (din combinaÈ›ia de 50% original + 50% walls)
            # Folosim un threshold adaptiv pentru a separa pereÈ›ii de spaÈ›iile libere
            _, overlay_binary = cv2.threshold(overlay_gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
            
            # InversÄƒm pentru a obÈ›ine spaÈ›iile libere (0 = perete, 255 = spaÈ›iu liber)
            spaces_mask = cv2.bitwise_not(overlay_binary)
            print(f"         ğŸ“Š Overlay binarizat: pereÈ›i identificaÈ›i din combinaÈ›ie")
        
        # ÃncÄƒrcÄƒm outdoor_mask pentru a identifica zonele exterioare (terase)
        outdoor_mask = None
        if steps_dir:
            outdoor_mask_path = Path(steps_dir) / "03_outdoor_mask.png"
            if outdoor_mask_path.exists():
                outdoor_mask = cv2.imread(str(outdoor_mask_path), cv2.IMREAD_GRAYSCALE)
                if outdoor_mask is not None and outdoor_mask.shape[:2] != (h, w):
                    outdoor_mask = cv2.resize(outdoor_mask, (w, h))
                print(f"         ğŸ“¸ Folosesc outdoor_mask pentru filtrare")
        
        # ProcesÄƒm DOAR rezultatul cu confidence maxim (dacÄƒ existÄƒ)
        rooms_filled = 0
        if text_boxes:
            # ProcesÄƒm doar primul (È™i singurul) rezultat - cel cu confidence maxim
            box_idx = 0
            x, y, width, height, text, conf = text_boxes[0]
            # Centrul textului
            center_x = x + width // 2
            center_y = y + height // 2
            
            # VerificÄƒm dacÄƒ centrul este Ã®ntr-un spaÈ›iu liber (nu pe perete)
            if 0 <= center_y < h and 0 <= center_x < w:
                if spaces_mask[center_y, center_x] == 255:  # SpaÈ›iu liber
                    # DacÄƒ OCR a detectat textul, facem flood fill direct (fÄƒrÄƒ filtrare suplimentarÄƒ)
                    # pentru cÄƒ OCR deja a identificat specific cuvÃ¢ntul "terasa"
                    if not use_ocr:
                        # DacÄƒ nu avem OCR, nu mai facem nimic (metoda alternativÄƒ este dezactivatÄƒ)
                        print(f"         âš ï¸ FÄƒrÄƒ OCR nu pot identifica specific cuvÃ¢ntul. Skip.")
                    else:
                        print(f"         ğŸ¯ GÄƒsit cuvÃ¢ntul '{text}' (confidence {conf:.1f}%) - fac flood fill pentru aceastÄƒ camerÄƒ...")
                        # Facem flood fill din centrul textului pe overlay-ul combinat
                        # Flood fill-ul se va opri automat cÃ¢nd Ã®ntÃ¢lneÈ™te pereÈ›i (linii Ã®nchise Ã®n overlay)
                        flood_mask = np.zeros((h + 2, w + 2), np.uint8)
                        flood_fill_flags = 4  # 4-conectivitate
                        flood_fill_flags |= cv2.FLOODFILL_MASK_ONLY
                        flood_fill_flags |= (255 << 8)  # Fill value
                        
                        seed_point = (center_x, center_y)
                        
                        # Folosim overlay-ul combinat pentru flood fill
                        if overlay_combined is not None:
                            # Convertim overlay-ul la grayscale pentru flood fill
                            overlay_for_fill = cv2.cvtColor(overlay_combined, cv2.COLOR_BGR2GRAY)
                            # ToleranÈ›Äƒ pentru a se opri la pereÈ›i (pereÈ›ii sunt mai Ã®nchiÈ™i Ã®n overlay)
                            lo_diff = 30  # ToleranÈ›Äƒ pentru diferenÈ›e de culoare
                            up_diff = 30
                            fill_image = overlay_for_fill.copy()
                            print(f"         ğŸ¨ Folosesc overlay combinat pentru flood fill")
                        else:
                            # Fallback la spaces_mask dacÄƒ nu avem overlay
                            fill_image = spaces_mask.copy()
                            lo_diff = 0  # Nu acceptÄƒ diferenÈ›e - se opreÈ™te exact la pereÈ›i
                            up_diff = 0
                            print(f"         âš ï¸ Folosesc spaces_mask simplu (overlay indisponibil)")
                        
                        # Facem flood fill pe imaginea combinatÄƒ
                        # Flood fill-ul se va opri automat cÃ¢nd Ã®ntÃ¢lneÈ™te pereÈ›i (valori diferite)
                        _, _, _, rect = cv2.floodFill(
                            fill_image, 
                            flood_mask, 
                            seed_point, 
                            128,  # Valoare de fill (nu este folositÄƒ cu FLOODFILL_MASK_ONLY)
                            lo_diff, 
                            up_diff, 
                            flood_fill_flags
                        )
                        
                        # Extragem zona umplutÄƒ din mask
                        filled_region = (flood_mask[1:h+1, 1:w+1] == 255).astype(np.uint8) * 255
                        
                        # VerificÄƒm cÄƒ nu am umplut peste pereÈ›i (safety check)
                        # Zona umplutÄƒ nu trebuie sÄƒ conÈ›inÄƒ pixeli de perete
                        overlap_with_walls = np.sum((filled_region > 0) & (walls_mask > 0))
                        if overlap_with_walls > 0:
                            print(f"         âš ï¸ Flood fill a depÄƒÈ™it pereÈ›ii ({overlap_with_walls} pixeli). Corectez...")
                            # EliminÄƒm pixeli care sunt pe pereÈ›i
                            filled_region = cv2.bitwise_and(filled_region, cv2.bitwise_not(walls_mask))
                        
                        # VerificÄƒm dacÄƒ am umplut o zonÄƒ suficient de mare (minim 1000 pixeli)
                        filled_area = np.count_nonzero(filled_region)
                        if filled_area > 1000:
                            print(f"         ğŸ” Extrag conturul zonei detectate pentru a completa golurile...")
                            
                            # IniÈ›ializÄƒm variabilele pentru vizualizare
                            gaps = None
                            wall_border = None
                            contours = None
                            
                            # Extragem conturul zonei umplute
                            contours, _ = cv2.findContours(filled_region, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
                            
                            if contours:
                                # GÄƒsim cel mai mare contur (zona principalÄƒ)
                                largest_contour = max(contours, key=cv2.contourArea)
                                
                                # CreÄƒm o mascÄƒ pentru conturul complet
                                contour_mask = np.zeros((h, w), dtype=np.uint8)
                                # DesenÄƒm conturul cu grosime adaptivÄƒ (grosimea peretelui)
                                wall_thickness = max(3, int(min(w, h) * 0.003))  # Grosime adaptivÄƒ
                                cv2.drawContours(contour_mask, [largest_contour], -1, 255, wall_thickness)
                                
                                # IdentificÄƒm golurile: unde conturul existÄƒ dar pereÈ›ii nu existÄƒ
                                # Golurile sunt Ã®n contur_mask dar nu Ã®n walls_mask
                                gaps = cv2.bitwise_and(contour_mask, cv2.bitwise_not(walls_mask))
                                
                                # CompletÄƒm doar golurile (nu desenÄƒm peste pereÈ›ii existenÈ›i)
                                walls_to_add = gaps
                                
                                # AdÄƒugÄƒm pereÈ›ii noi (doar golurile) la masca finalÄƒ
                                result = cv2.bitwise_or(result, walls_to_add)
                                
                                gaps_area = np.count_nonzero(gaps)
                                print(f"         âœ… Completat {gaps_area} pixeli de goluri Ã®n pereÈ›i conform conturului")
                            else:
                                print(f"         âš ï¸ Nu s-au gÄƒsit contururi Ã®n zona umplutÄƒ")
                                # Fallback la metoda veche (dilatare)
                                kernel_size = max(3, int(min(w, h) * 0.005))  # Adaptiv
                                kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (kernel_size, kernel_size))
                                filled_dilated = cv2.dilate(filled_region, kernel, iterations=1)
                                wall_border = cv2.subtract(filled_dilated, filled_region)
                                result = cv2.bitwise_or(result, wall_border)
                            
                            rooms_filled += 1
                            
                            print(f"         âœ… Umplut camera '{text}': {filled_area} pixeli")
                            
                            # VizualizÄƒm zona umplutÄƒ È™i golurile completate
                            if steps_dir:
                                vis_fill = cv2.cvtColor(walls_mask, cv2.COLOR_GRAY2BGR)
                                # DesenÄƒm zona umplutÄƒ cu transparenÈ›Äƒ (galben)
                                filled_colored = np.zeros_like(vis_fill)
                                filled_colored[filled_region > 0] = [0, 255, 255]  # Galben
                                vis_fill = cv2.addWeighted(vis_fill, 0.7, filled_colored, 0.3, 0)
                                
                                # DesenÄƒm conturul complet (albastru)
                                if contours and len(contours) > 0:
                                    largest_contour = max(contours, key=cv2.contourArea)
                                    cv2.drawContours(vis_fill, [largest_contour], -1, (255, 0, 0), 2)  # Albastru pentru contur
                                
                                # DesenÄƒm golurile completate (verde)
                                if gaps is not None:
                                    gaps_colored = np.zeros_like(vis_fill)
                                    gaps_colored[gaps > 0] = [0, 255, 0]  # Verde pentru goluri completate
                                    vis_fill = cv2.addWeighted(vis_fill, 0.5, gaps_colored, 0.5, 0)
                                elif wall_border is not None:
                                    # Fallback: desenÄƒm pereÈ›ii noi (verde)
                                    wall_border_colored = np.zeros_like(vis_fill)
                                    wall_border_colored[wall_border > 0] = [0, 255, 0]  # Verde
                                    vis_fill = cv2.addWeighted(vis_fill, 0.5, wall_border_colored, 0.5, 0)
                                
                                # DesenÄƒm centrul textului (roÈ™u)
                                cv2.circle(vis_fill, (center_x, center_y), 5, (0, 0, 255), -1)
                                cv2.rectangle(vis_fill, (x, y), (x + width, y + height), (0, 255, 0), 2)
                                
                                output_path = Path(steps_dir) / f"02g_02_terrace_fill_{box_idx + 1}.png"
                                cv2.imwrite(str(output_path), vis_fill)
                                print(f"         ğŸ’¾ Salvat: {output_path.name}")
                            
                            # Am umplut camera terasei
                            print(f"         âœ… Gata! Am umplut camera terasei.")
                        else:
                            print(f"         âš ï¸ Zona detectatÄƒ prea micÄƒ ({filled_area} pixeli). Skip.")
                else:
                    print(f"         âš ï¸ Centrul textului '{text}' este pe un perete. Skip.")
        
        if rooms_filled > 0:
            print(f"         âœ… Umplut {rooms_filled} camere de tip 'terasa'")
        else:
            print(f"         âš ï¸ Nu s-au umplut camere (zone prea mici sau pe pereÈ›i)")
        
        # Pas 5: SalvÄƒm rezultatul final
        if steps_dir:
            vis_final = cv2.cvtColor(result, cv2.COLOR_GRAY2BGR)
            # ComparÄƒm cu originalul
            diff = cv2.subtract(result, walls_mask)
            diff_colored = np.zeros_like(vis_final)
            diff_colored[diff > 0] = [0, 255, 0]  # Verde pentru pereÈ›ii noi
            vis_final = cv2.addWeighted(vis_final, 0.7, diff_colored, 0.3, 0)
            
            cv2.imwrite(str(Path(steps_dir) / "02g_03_final_result.png"), vis_final)
            print(f"         ğŸ’¾ Salvat: 02g_03_final_result.png (verde=pereÈ›i noi)")
    
    except Exception as e:
        print(f"         âŒ Eroare la detectarea/umplerea terasei: {e}")
        import traceback
        traceback.print_exc()
        return result
    
    return result

def detect_and_visualize_wall_closures(mask: np.ndarray, steps_dir: str = None) -> None:
    """
    DetecteazÄƒ camerele (spaÈ›iile libere Ã®nchise de pereÈ›i) folosind Watershed cu Distance Transform.
    
    AceastÄƒ metodÄƒ detecteazÄƒ atÃ¢t camerele complet Ã®nchise cÃ¢t È™i cele cu goluri Ã®n pereÈ›i,
    fÄƒrÄƒ a distorsiona structura prin operaÈ›ii morfologice agresive.
    
    Args:
        mask: Masca pereÈ›ilor (255 = perete, 0 = spaÈ›iu liber)
        steps_dir: Director pentru salvarea step-urilor de debug (opÈ›ional)
    """
    h, w = mask.shape[:2]
    
    print(f"      ğŸ¨ Detectez camere folosind Watershed + Distance Transform...")
    
    # CreÄƒm o imagine coloratÄƒ (BGR) pentru vizualizare
    vis_image = cv2.cvtColor(mask, cv2.COLOR_GRAY2BGR)
    
    # Masca: negru (0) = spaÈ›iu liber, alb (255) = perete
    spaces_mask = cv2.bitwise_not(mask)
    
    # PaletÄƒ de culori (BGR format pentru OpenCV)
    colors = [
        (255, 0, 0),      # Albastru
        (0, 255, 0),      # Verde
        (0, 0, 255),      # RoÈ™u
        (255, 255, 0),    # Cyan
        (255, 0, 255),    # Magenta
        (0, 255, 255),    # Galben
        (128, 0, 128),    # Mov
        (255, 165, 0),    # Portocaliu
        (0, 128, 255),    # Albastru deschis
        (128, 255, 0),    # Verde deschis
        (255, 192, 203),  # Roz
        (0, 255, 127),    # Verde primÄƒvarÄƒ
        (255, 20, 147),   # Roz adÃ¢nc
        (0, 191, 255),    # Albastru cer
        (255, 140, 0),    # Portocaliu Ã®nchis
    ]
    
    # Pas 1: CalculeazÄƒ Distance Transform
    # DistanÈ›a de la fiecare pixel la cel mai apropiat perete
    dist_transform = cv2.distanceTransform(spaces_mask, cv2.DIST_L2, 5)
    
    # NormalizÄƒm pentru vizualizare (opÈ›ional, pentru debug)
    if steps_dir:
        dist_vis = cv2.normalize(dist_transform, None, 0, 255, cv2.NORM_MINMAX, dtype=cv2.CV_8U)
        cv2.imwrite(str(Path(steps_dir) / "02c_distance_transform.png"), dist_vis)
    
    # Pas 2: GÄƒseÈ™te maximale locale (centrele camerelor)
    # Folosim o metodÄƒ mai robustÄƒ: gÄƒsim maximale locale Ã®n distance transform
    # Threshold adaptiv bazat pe distribuÈ›ia distanÈ›elor
    max_dist = np.max(dist_transform)
    # Pentru camere cu goluri mari, folosim un threshold mai mic (30% din maxim)
    min_distance_threshold = max(5, max_dist * 0.3)
    
    # AplicÄƒm threshold pentru a gÄƒsi zonele cu distanÈ›Äƒ mare (centrele camerelor)
    _, sure_fg = cv2.threshold(dist_transform, min_distance_threshold, 255, cv2.THRESH_BINARY)
    sure_fg = sure_fg.astype(np.uint8)
    
    # GÄƒsim maximale locale folosind peak detection
    # Folosim morphological operations pentru a gÄƒsi vÃ¢rfurile locale
    kernel_peak = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))
    local_maxima = cv2.morphologyEx(dist_transform, cv2.MORPH_TOPHAT, kernel_peak)
    _, local_maxima = cv2.threshold(local_maxima, max_dist * 0.2, 255, cv2.THRESH_BINARY)
    local_maxima = local_maxima.astype(np.uint8)
    
    # CombinÄƒm threshold-ul simplu cu maximalele locale
    sure_fg = cv2.bitwise_or(sure_fg, local_maxima)
    
    # Pas 3: GÄƒseÈ™te maximale locale folosind connected components
    num_markers, markers = cv2.connectedComponents(sure_fg)
    
    # Pas 4: AplicÄƒ Watershed
    # MarcÄƒm pereÈ›ii ca fiind sigur fundal
    sure_bg = cv2.dilate(mask, cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3)), iterations=1)
    unknown = cv2.subtract(sure_bg, sure_fg)
    
    # AdÄƒugÄƒm pereÈ›ii la markeri (label 0 = fundal/pereÈ›i)
    markers = markers + 1
    markers[unknown == 255] = 0
    
    # AplicÄƒ Watershed pe o copie a imaginii (watershed modificÄƒ imaginea)
    watershed_image = vis_image.copy()
    cv2.watershed(watershed_image, markers)
    
    # Pas 5: DeseneazÄƒ fiecare camerÄƒ detectatÄƒ
    room_count = 0
    detected_rooms = []
    # MascÄƒ pentru zonele deja desenate (pentru a evita suprapunerea)
    drawn_mask = np.zeros((h, w), dtype=np.uint8)
    
    # ColectÄƒm toate camerele cu ariile lor pentru a le sorta È™i procesa corect
    rooms_data = []
    for label_id in range(2, num_markers + 1):  # Ãncepem de la 2 (1 este pereÈ›ii)
        room_mask = (markers == label_id).astype(np.uint8) * 255
        area = np.count_nonzero(room_mask)
        
        if area < 500:  # IgnorÄƒm camere foarte mici
            continue
        
        rooms_data.append((label_id, area, room_mask))
    
    # SortÄƒm camerele dupÄƒ arie (descrescÄƒtor) pentru a procesa mai Ã®ntÃ¢i camerele mari
    rooms_data.sort(key=lambda x: x[1], reverse=True)
    
    # ProcesÄƒm fiecare camerÄƒ
    for label_id, area, room_mask in rooms_data:
        # VerificÄƒm suprapunerea cu zonele deja desenate
        overlap = cv2.bitwise_and(room_mask, drawn_mask)
        overlap_area = np.count_nonzero(overlap)
        overlap_ratio = overlap_area / area if area > 0 else 0
        
        # DacÄƒ mai mult de 15% din camerÄƒ este deja desenatÄƒ, o ignorÄƒm
        if overlap_ratio > 0.15:
            continue
        
        # DetectÄƒm conturul camerei
        contours, _ = cv2.findContours(room_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        
        if len(contours) == 0:
            continue
        
        # LuÄƒm cel mai mare contur
        contour = max(contours, key=cv2.contourArea)
        
        if len(contour) < 3:
            continue
        
        # CalculÄƒm forma geometricÄƒ
        x, y, w_box, h_box = cv2.boundingRect(contour)
        bbox_area = w_box * h_box
        area_ratio = area / bbox_area if bbox_area > 0 else 0
        
        # DeterminÄƒm forma de desenat
        if area_ratio >= 0.6:  # CamerÄƒ bine definitÄƒ
            if len(contour) > 3:
                shape_to_draw = cv2.convexHull(contour)
            else:
                shape_to_draw = contour
        else:  # CamerÄƒ parÈ›ial Ã®nchisÄƒ
            if len(contour) > 3:
                hull = cv2.convexHull(contour)
                hull_area = cv2.contourArea(hull)
                if hull_area > 0 and area / hull_area >= 0.6:
                    shape_to_draw = hull
                else:
                    shape_to_draw = np.array([[x, y], [x + w_box, y], 
                                             [x + w_box, y + h_box], [x, y + h_box]], dtype=np.int32)
            else:
                shape_to_draw = np.array([[x, y], [x + w_box, y], 
                                         [x + w_box, y + h_box], [x, y + h_box]], dtype=np.int32)
        
        # SelectÄƒm culoarea
        color = colors[room_count % len(colors)]
        
        # MarcÄƒm zona ca desenatÄƒ ÃNAINTE de a desena (pentru a evita suprapunerea)
        cv2.fillPoly(drawn_mask, [shape_to_draw], 255)
        
        # DesenÄƒm camera
        overlay = vis_image.copy()
        cv2.fillPoly(overlay, [shape_to_draw], color)
        cv2.addWeighted(overlay, 0.4, vis_image, 0.6, 0, vis_image)
        cv2.polylines(vis_image, [shape_to_draw], True, color, 3)
        
        # MarcÄƒm camera ca detectatÄƒ
        detected_rooms.append((label_id, area, room_mask))
        room_count += 1
    
    # SalvÄƒm imaginea coloratÄƒ
    if steps_dir:
        output_path = Path(steps_dir) / "02c_wall_closures_visualized.png"
        cv2.imwrite(str(output_path), vis_image)
        print(f"      âœ… Detectat {room_count} camere, salvat Ã®n {output_path.name}")
    else:
        print(f"      âœ… Detectat {room_count} camere")

def bridge_wall_gaps(walls_raw: np.ndarray, image_dims: tuple, steps_dir: str = None) -> np.ndarray:
    """Umple DOAR gap-urile Ã®ntre pereÈ›i - Ã®mbunÄƒtÄƒÈ›it pentru VPS."""
    h, w = image_dims
    min_dim = min(h, w)
    
    # MÄƒrit kernel size de la 1.6% la 2.0% pentru VPS
    kernel_size = max(15, int(min_dim * 0.020))  # MÄƒrit de la 0.016 la 0.020
    if kernel_size % 2 == 0: kernel_size += 1
    
    print(f"      ğŸŒ‰ Bridging gaps between walls: kernel {kernel_size}x{kernel_size} (Ã®mbunÄƒtÄƒÈ›it pentru VPS)")
    
    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (kernel_size, kernel_size))
    
    original_walls = walls_raw.copy()
    if steps_dir:
        save_step("bridge_01_original", original_walls, steps_dir)
    
    # MÄƒrit iteraÈ›iile de la 2 la 3 pentru VPS
    walls_closed = cv2.morphologyEx(walls_raw, cv2.MORPH_CLOSE, kernel, iterations=3)
    if steps_dir:
        save_step("bridge_02_closed", walls_closed, steps_dir)
    
    gaps_filled = cv2.subtract(walls_closed, original_walls)
    if steps_dir:
        save_step("bridge_03_gaps_only", gaps_filled, steps_dir)
    
    kernel_tiny = cv2.getStructuringElement(cv2.MORPH_RECT, (3, 3))
    gaps_cleaned = cv2.morphologyEx(gaps_filled, cv2.MORPH_OPEN, kernel_tiny, iterations=1)
    if steps_dir:
        save_step("bridge_04_gaps_cleaned", gaps_cleaned, steps_dir)
    
    walls_final = cv2.bitwise_or(original_walls, gaps_cleaned)
    if steps_dir:
        save_step("bridge_05_final", walls_final, steps_dir)
    
    return walls_final

def smart_wall_closing(walls_raw: np.ndarray, image_dims: tuple, steps_dir: str = None) -> np.ndarray:
    """Closing inteligent - Ã®mbunÄƒtÄƒÈ›it pentru VPS."""
    h, w = image_dims
    min_dim = min(h, w)
    
    print(f"      ğŸ§  Smart closing: detecting intentional openings (Ã®mbunÄƒtÄƒÈ›it pentru VPS)...")
    
    # MÄƒrit kernel size de la 0.9% la 1.2% pentru VPS
    kernel_large = max(9, int(min_dim * 0.012))  # MÄƒrit de la 0.009 la 0.012
    if kernel_large % 2 == 0: kernel_large += 1
    
    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (kernel_large, kernel_large))
    # MÄƒrit iteraÈ›iile de la 2 la 3 pentru VPS
    walls_fully_closed = cv2.morphologyEx(walls_raw, cv2.MORPH_CLOSE, kernel, iterations=3)
    
    if steps_dir:
        save_step("smart_01_fully_closed", walls_fully_closed, steps_dir)
    
    gaps_filled = cv2.subtract(walls_fully_closed, walls_raw)
    
    if steps_dir:
        save_step("smart_02_gaps_detected", gaps_filled, steps_dir)
    
    contours, _ = cv2.findContours(gaps_filled, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    
    DOOR_THRESHOLD = int(min_dim * 0.02)
    
    mask_small_gaps = np.zeros_like(walls_raw)
    mask_large_openings = np.zeros_like(walls_raw)
    
    small_gap_count = 0
    large_opening_count = 0
    
    for cnt in contours:
        x, y, w_box, h_box = cv2.boundingRect(cnt)
        max_size = max(w_box, h_box)
        area = cv2.contourArea(cnt)
        
        if max_size < DOOR_THRESHOLD and area < (DOOR_THRESHOLD ** 2):
            cv2.drawContours(mask_small_gaps, [cnt], -1, 255, -1)
            small_gap_count += 1
        else:
            cv2.drawContours(mask_large_openings, [cnt], -1, 255, -1)
            large_opening_count += 1
    
    print(f"         Found {small_gap_count} small gaps (will close) and {large_opening_count} large openings (will keep)")
    
    if steps_dir:
        save_step("smart_03_small_gaps", mask_small_gaps, steps_dir)
        save_step("smart_04_large_openings", mask_large_openings, steps_dir)
    
    walls_smart = cv2.bitwise_or(walls_raw, mask_small_gaps)
    
    if steps_dir:
        save_step("smart_05_final", walls_smart, steps_dir)
    
    kernel_cleanup = cv2.getStructuringElement(cv2.MORPH_RECT, (3, 3))
    walls_smart = cv2.morphologyEx(walls_smart, cv2.MORPH_CLOSE, kernel_cleanup, iterations=1)
    
    return walls_smart

def get_strict_1px_outline(mask):
    if np.count_nonzero(mask) == 0:
        return np.zeros_like(mask)
    kernel = np.array([[0, 1, 0], [1, 1, 1], [0, 1, 0]], dtype=np.uint8)
    eroded = cv2.erode(mask, kernel, iterations=1)
    return cv2.subtract(mask, eroded)

def smooth_walls_mask(mask):
    """Netezire pentru eliminarea jitter-ului (nemodificatÄƒ)."""
    if np.count_nonzero(mask) == 0:
        return mask
    blurred = cv2.GaussianBlur(mask, (5, 5), 0)
    _, smoothed = cv2.threshold(blurred, 127, 255, cv2.THRESH_BINARY)
    return smoothed

def straighten_mask(mask, epsilon_factor=0.003):
    """Ãndreptare vizualÄƒ pentru overlay (nemodificatÄƒ)."""
    if np.count_nonzero(mask) == 0:
        return mask
    contours, _ = cv2.findContours(mask, cv2.RETR_CCOMP, cv2.CHAIN_APPROX_SIMPLE)
    clean_mask = np.zeros_like(mask)
    for cnt in contours:
        epsilon = epsilon_factor * cv2.arcLength(cnt, True)
        approx = cv2.approxPolyDP(cnt, epsilon, True)
        cv2.drawContours(clean_mask, [approx], -1, 255, -1)
    return clean_mask

def skeletonize_and_straighten_walls(walls_mask: np.ndarray, steps_dir: str = None) -> np.ndarray:
    """
    Skeletonizare, Ã®ndreptare È™i uniformizare grosime pereÈ›i.
    
    PaÈ™i:
    1. CalculeazÄƒ media grosimii pereÈ›ilor
    2. AplicÄƒ skeletonizarea pentru a obÈ›ine linii de 1 pixel
    3. Ãndrepte liniile care nu sunt foarte drepte
    4. AplicÄƒ aceeaÈ™i grosime (media calculatÄƒ) la toate liniile
    
    Args:
        walls_mask: Masca pereÈ›ilor (255 = perete, 0 = spaÈ›iu liber)
        steps_dir: Director pentru salvarea step-urilor de debug (opÈ›ional)
    
    Returns:
        Masca pereÈ›ilor cu skeletonizare, Ã®ndreptare È™i grosime uniformÄƒ
    """
    if np.count_nonzero(walls_mask) == 0:
        return walls_mask
    
    h, w = walls_mask.shape[:2]
    print(f"      ğŸ¦´ Skeletonizare È™i Ã®ndreptare pereÈ›i...")
    
    # Pas 1: CalculÄƒm media grosimii pereÈ›ilor
    print(f"         ğŸ” Pas 1: Calculez media grosimii pereÈ›ilor...")
    
    # Distance transform pe masca de pereÈ›i (distanÈ›a de la centru la margine)
    # Aceasta ne dÄƒ distanÈ›a de la fiecare pixel de perete la marginea cea mai apropiatÄƒ
    dist_from_edge = cv2.distanceTransform(walls_mask, cv2.DIST_L2, 5)
    
    # Grosimea Ã®ntr-un punct = 2 * distanÈ›a (pentru cÄƒ avem douÄƒ margini)
    wall_thicknesses = dist_from_edge[walls_mask > 0] * 2
    
    if len(wall_thicknesses) == 0:
        print(f"         âš ï¸ Nu s-au gÄƒsit pereÈ›i pentru calcul grosime")
        return walls_mask
    
    # CalculÄƒm media grosimii (folosim median pentru a fi mai robust la outliers)
    avg_thickness = np.median(wall_thicknesses)
    avg_thickness_int = max(3, int(round(avg_thickness)))
    
    print(f"         âœ… Media grosimii pereÈ›ilor: {avg_thickness:.2f}px (folosit: {avg_thickness_int}px)")
    
    if steps_dir:
        # VizualizÄƒm distance transform pentru debug
        dist_vis = cv2.normalize(dist_from_edge, None, 0, 255, cv2.NORM_MINMAX, dtype=cv2.CV_8U)
        cv2.imwrite(str(Path(steps_dir) / "02h_01_distance_transform.png"), dist_vis)
        print(f"         ğŸ’¾ Salvat: 02h_01_distance_transform.png")
    
    # Pas 2: Skeletonizare
    print(f"         ğŸ” Pas 2: Aplic skeletonizare...")
    
    # Convertim Ã®n format binar (0 È™i 1) pentru skeletonize
    binary_norm = (walls_mask / 255).astype(bool)
    
    # AplicÄƒm Skeletonization
    skeleton = skeletonize(binary_norm)
    
    # Convertim Ã®napoi Ã®n format OpenCV (0-255)
    skeleton_img = (skeleton.astype(np.uint8) * 255)
    
    # ConectÄƒm liniile discontinue folosind morphological closing pe distanÈ›e mici
    # Folosim un kernel mic pentru a conecta doar liniile foarte apropiate
    connect_kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))
    skeleton_img = cv2.morphologyEx(skeleton_img, cv2.MORPH_CLOSE, connect_kernel, iterations=1)
    
    skeleton_pixels = np.count_nonzero(skeleton_img)
    print(f"         âœ… Schelet generat: {skeleton_pixels:,} pixeli (1px grosime)")
    
    if steps_dir:
        cv2.imwrite(str(Path(steps_dir) / "02h_02_skeleton.png"), skeleton_img)
        print(f"         ğŸ’¾ Salvat: 02h_02_skeleton.png")
    
    # Pas 3: Ãndreptare liniilor
    print(f"         ğŸ” Pas 3: Ãndrept liniile (NU È™terg pixeli, doar Ã®nlocuiesc liniile tremurate)...")
    
    # Pornim cu skeleton-ul original - NU È™tergem nimic
    skeleton_straight = skeleton_img.copy()
    
    # Folosim HoughLinesP pentru a detecta linii drepte
    lines = cv2.HoughLinesP(
        skeleton_img,
        rho=1,
        theta=np.pi / 180,
        threshold=30,
        minLineLength=max(15, int(min(h, w) * 0.015)),
        maxLineGap=max(10, int(min(h, w) * 0.008))
    )
    
    if lines is None or len(lines) == 0:
        print(f"         âš ï¸ Nu s-au detectat linii cu HoughLinesP, pÄƒstrez skeleton-ul original")
        # PÄƒstrÄƒm skeleton-ul original fÄƒrÄƒ modificÄƒri
    else:
        print(f"         âœ… Detectat {len(lines)} linii cu HoughLinesP - Ã®nlocuiesc segmentele tremurate")
        
        # Pentru fiecare linie detectatÄƒ, gÄƒsim pixeli din skeleton care sunt aproape
        # È™i Ã®i Ã®nlocuim cu linia dreaptÄƒ
        for line in lines:
            x1, y1, x2, y2 = line[0]
            
            # CalculÄƒm distanÈ›a de la fiecare pixel din skeleton la linia dreaptÄƒ
            # DacÄƒ distanÈ›a este micÄƒ, Ã®nlocuim pixelul cu linia dreaptÄƒ
            line_length = np.sqrt((x2 - x1)**2 + (y2 - y1)**2)
            if line_length < 5:  # IgnorÄƒm linii prea scurte
                continue
            
            # GÄƒsim toÈ›i pixelii din skeleton care sunt aproape de aceastÄƒ linie
            # Folosim o bandÄƒ de toleranÈ›Äƒ de ~3 pixeli
            tolerance = 3
            
            # CreÄƒm o mascÄƒ temporarÄƒ pentru linia dreaptÄƒ cu o bandÄƒ de toleranÈ›Äƒ
            line_mask = np.zeros_like(skeleton_img)
            cv2.line(line_mask, (x1, y1), (x2, y2), 255, tolerance * 2 + 1)
            
            # GÄƒsim intersecÈ›ia dintre skeleton È™i banda de toleranÈ›Äƒ
            pixels_near_line = cv2.bitwise_and(skeleton_img, line_mask)
            
            # DacÄƒ existÄƒ pixeli aproape de linie, Ã®i Ã®nlocuim cu linia dreaptÄƒ
            if np.count_nonzero(pixels_near_line) > 0:
                # È˜tergem pixeli din skeleton care sunt Ã®n banda de toleranÈ›Äƒ
                skeleton_straight = cv2.bitwise_and(skeleton_straight, cv2.bitwise_not(line_mask))
                # AdÄƒugÄƒm linia dreaptÄƒ
                cv2.line(skeleton_straight, (x1, y1), (x2, y2), 255, 1)
    
    # AplicÄƒm straighten_mask pentru a netezi colÈ›urile (dar pÄƒstrÄƒm toÈ›i pixelii)
    skeleton_straight = straighten_mask(skeleton_straight, epsilon_factor=0.002)
    
    if steps_dir:
        vis_straight = cv2.cvtColor(skeleton_straight, cv2.COLOR_GRAY2BGR)
        if lines is not None and len(lines) > 0:
            for line in lines:
                x1, y1, x2, y2 = line[0]
                cv2.line(vis_straight, (x1, y1), (x2, y2), (0, 255, 0), 1)
        cv2.imwrite(str(Path(steps_dir) / "02h_03_straightened.png"), vis_straight)
        print(f"         ğŸ’¾ Salvat: 02h_03_straightened.png")
    
    # Pas 4: AplicÄƒm aceeaÈ™i grosime la toate liniile (grosimea calculatÄƒ ÃNAINTE de skeletonizare)
    print(f"         ğŸ” Pas 4: Aplic grosime uniformÄƒ ({avg_thickness_int}px - grosimea originalÄƒ)...")
    
    # CreÄƒm un kernel pÄƒtrat pentru a da grosime uniformÄƒ È™i pereÈ›i pÄƒtraÈ›i
    # Pentru a obÈ›ine grosimea avg_thickness_int, folosim un kernel mai mic (pentru a compensa)
    # CÃ¢nd dilatezi cu un kernel pÄƒtrat de dimensiune N, adaugi (N-1)/2 pixeli pe fiecare parte
    # Folosim kernel_size mai mic pentru a obÈ›ine grosimea exactÄƒ
    kernel_size = max(3, avg_thickness_int - 2)
    # AsigurÄƒm cÄƒ e impar (pentru kernel-uri morfologice)
    if kernel_size % 2 == 0:
        kernel_size += 1
    
    # Folosim MORPH_RECT pentru pereÈ›i pÄƒtraÈ›i (nu MORPH_ELLIPSE care face pereÈ›i rotunjiÈ›i)
    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (kernel_size, kernel_size))
    
    # DilatÄƒm skeleton-ul Ã®ndreptat cu grosimea calculatÄƒ iniÈ›ial
    # Folosim o singurÄƒ iteraÈ›ie pentru a obÈ›ine grosimea exactÄƒ
    walls_final = cv2.dilate(skeleton_straight, kernel, iterations=1)
    
    # VerificÄƒm grosimea rezultatÄƒ pentru debug
    if steps_dir:
        # CalculÄƒm grosimea efectivÄƒ a pereÈ›ilor rezultaÈ›i
        walls_final_inv = cv2.bitwise_not(walls_final)
        dist_final = cv2.distanceTransform(walls_final_inv, cv2.DIST_L2, 5)
        final_thicknesses = dist_final[walls_final > 0] * 2
        if len(final_thicknesses) > 0:
            final_avg_thickness = np.median(final_thicknesses)
            print(f"         ğŸ“Š Grosime efectivÄƒ rezultatÄƒ: {final_avg_thickness:.2f}px (È›intÄƒ: {avg_thickness_int}px, kernel: {kernel_size}px)")
    
    final_pixels = np.count_nonzero(walls_final)
    print(f"         âœ… PereÈ›i finalizaÈ›i: {final_pixels:,} pixeli (grosime uniformÄƒ {avg_thickness_int}px)")
    
    if steps_dir:
        # Vizualizare comparativÄƒ
        vis_comparison = np.zeros((h, w * 3, 3), dtype=np.uint8)
        
        # Original
        vis_comparison[:, :w] = cv2.cvtColor(walls_mask, cv2.COLOR_GRAY2BGR)
        cv2.putText(vis_comparison, "Original", (10, 30), 
                   cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
        
        # Skeleton
        vis_comparison[:, w:2*w] = cv2.cvtColor(skeleton_img, cv2.COLOR_GRAY2BGR)
        cv2.putText(vis_comparison, "Skeleton", (w + 10, 30), 
                   cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
        
        # Final
        vis_comparison[:, 2*w:] = cv2.cvtColor(walls_final, cv2.COLOR_GRAY2BGR)
        cv2.putText(vis_comparison, "Final", (2*w + 10, 30), 
                   cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
        
        cv2.imwrite(str(Path(steps_dir) / "02h_04_comparison.png"), vis_comparison)
        print(f"         ğŸ’¾ Salvat: 02h_04_comparison.png")
        
        # Rezultat final
        cv2.imwrite(str(Path(steps_dir) / "02h_05_final_walls.png"), walls_final)
        print(f"         ğŸ’¾ Salvat: 02h_05_final_walls.png")
    
    return walls_final

def generate_interior_walls_skeleton(walls_mask: np.ndarray, outdoor_mask: np.ndarray) -> np.ndarray:
    """
    GenereazÄƒ scheletul (skeleton) pereÈ›ilor interiori cu grosime de 1 pixel.
    
    Args:
        walls_mask: Masca cu pereÈ›ii (alb = pereÈ›i, negru = spaÈ›iu liber)
        outdoor_mask: Masca exterioarÄƒ (alb = exterior, negru = interior)
    
    Returns:
        Imagine binarÄƒ cu scheletul pereÈ›ilor interiori (1 pixel grosime)
    """
    if np.count_nonzero(walls_mask) == 0:
        return np.zeros_like(walls_mask)
    
    print("   ğŸ¦´ Generez scheletul pereÈ›ilor interiori...")
    
    # 1. Pre-procesare: CurÄƒÈ›Äƒm imaginea de zgomot mic
    kernel_clean = cv2.getStructuringElement(cv2.MORPH_RECT, (3, 3))
    walls_cleaned = cv2.morphologyEx(walls_mask, cv2.MORPH_OPEN, kernel_clean, iterations=1)
    walls_cleaned = cv2.morphologyEx(walls_cleaned, cv2.MORPH_CLOSE, kernel_clean, iterations=1)
    
    # 2. Convertim Ã®n format binar (0 È™i 1) pentru skeletonize
    binary_norm = (walls_cleaned / 255).astype(bool)
    
    # 3. AplicÄƒm Skeletonization
    skeleton = skeletonize(binary_norm)
    
    # 4. Convertim Ã®napoi Ã®n format OpenCV (0-255)
    skeleton_img = (skeleton.astype(np.uint8) * 255)
    
    # 5. EliminÄƒm pereÈ›ii exteriori
    # MÄƒrim puÈ›in masca exterioarÄƒ (Dilation) ca sÄƒ fim siguri cÄƒ acoperim tot perimetrul
    kernel_dilate = cv2.getStructuringElement(cv2.MORPH_RECT, (5, 5))
    exterior_dilated = cv2.dilate(outdoor_mask, kernel_dilate, iterations=1)
    
    # EliminÄƒm perimetrul exterior din schelet
    interior_skeleton = cv2.bitwise_and(skeleton_img, cv2.bitwise_not(exterior_dilated))
    
    # 6. OpÈ›ional: EliminÄƒm "crenguÈ›e" mici (spur pixels) - pruning simplu
    # EliminÄƒm componente conectate foarte mici (mai mici de 10 pixeli)
    num_labels, labels, stats, _ = cv2.connectedComponentsWithStats(interior_skeleton, connectivity=8)
    cleaned_skeleton = np.zeros_like(interior_skeleton)
    
    min_component_size = 10  # PÄƒstrÄƒm doar componentele cu cel puÈ›in 10 pixeli
    for label_id in range(1, num_labels):  # Skip background (label 0)
        component_size = stats[label_id, cv2.CC_STAT_AREA]
        if component_size >= min_component_size:
            cleaned_skeleton[labels == label_id] = 255
    
    skeleton_pixels = np.count_nonzero(cleaned_skeleton)
    print(f"      âœ… Schelet generat: {skeleton_pixels:,} pixeli (1px grosime)")
    
    return cleaned_skeleton

def flood_fill_room(indoor_mask, seed_pt, search_radius=10):
    """Flood fill pentru segmentare camere (nemodificatÄƒ)."""
    h, w = indoor_mask.shape[:2]
    x0, y0 = seed_pt
    x0 = max(0, min(w - 1, x0))
    y0 = max(0, min(h - 1, y0))

    if indoor_mask[y0, x0] == 0:
        found = False
        for dy in range(-search_radius, search_radius + 1):
            yy = y0 + dy
            if 0 <= yy < h:
                for dx in range(-search_radius, search_radius + 1):
                    xx = x0 + dx
                    if 0 <= xx < w:
                        if indoor_mask[yy, xx] > 0:
                            x0, y0 = xx, yy
                            found = True
                            break
                if found:
                    break
        if not found:
            return np.zeros_like(indoor_mask), 0, seed_pt

    mask = np.zeros((h + 2, w + 2), dtype=np.uint8)
    temp = indoor_mask.copy()
    try:
        _, _, mask, _ = cv2.floodFill(
            temp, mask, (x0, y0), 255,
            loDiff=(0,), upDiff=(0,),
            flags=cv2.FLOODFILL_MASK_ONLY | 4
        )
    except:
        return np.zeros_like(indoor_mask), 0, (x0, y0)

    room_mask = (mask[1:-1, 1:-1] != 0).astype(np.uint8) * 255
    area_px = int(np.count_nonzero(room_mask))
    return room_mask, area_px, (x0, y0)


# ============================================
# 3D RENDERER (nemodificatÄƒ)
# ============================================

def render_obj_to_image(vertices_raw, faces_raw, output_image_path, width=1024, height=1024):
    """RandeazÄƒ o previzualizare 3D simplÄƒ."""
    if not vertices_raw or not faces_raw: return
    print("   ğŸ“¸ Randez previzualizarea 3D...")
    
    verts = []
    faces = []
    for v_line in vertices_raw: 
        parts = v_line.split()
        verts.append([float(parts[1]), float(parts[2]), float(parts[3])])
    verts = np.array(verts)
    
    for f_line in faces_raw: 
        parts = f_line.split()
        faces.append([int(parts[1])-1, int(parts[2])-1, int(parts[3])-1, int(parts[4])-1])

    angle_y = np.radians(45)
    angle_x = np.radians(30)
    
    Ry = np.array([[np.cos(angle_y), 0, np.sin(angle_y)], [0, 1, 0], [-np.sin(angle_y), 0, np.cos(angle_y)]])
    Rx = np.array([[1, 0, 0], [0, np.cos(angle_x), -np.sin(angle_x)], [0, np.sin(angle_x), np.cos(angle_x)]])
    
    center = verts.mean(axis=0)
    verts_centered = verts - center
    rotated_verts = verts_centered @ Ry.T @ Rx.T
    
    range_val = np.max(rotated_verts) - np.min(rotated_verts)
    if range_val == 0: range_val = 1
    
    scale = min(width, height) / range_val * 0.7
    
    projected_2d = rotated_verts[:, :2] * scale
    projected_2d[:, 0] += width / 2
    projected_2d[:, 1] += height / 2
    projected_2d[:, 1] = height - projected_2d[:, 1]

    face_depths = []
    for idx, face in enumerate(faces): 
        face_depths.append((np.mean(rotated_verts[face, 2]), idx))
    face_depths.sort(key=lambda x: x[0])

    canvas = np.zeros((height, width, 3), dtype=np.uint8)
    wall_color = (200, 200, 200)
    edge_color = (50, 50, 50)
    
    for _, face_idx in face_depths:
        pts = projected_2d[faces[face_idx]].astype(np.int32)
        cv2.fillPoly(canvas, [pts], wall_color)
        cv2.polylines(canvas, [pts], True, edge_color, 1, cv2.LINE_AA)
        
    cv2.imwrite(str(output_image_path), canvas)


def export_walls_to_obj(walls_mask, output_path, scale_m_px, wall_height_m=2.5, image_output_path=None):
    """ExportÄƒ masca pereÈ›ilor Ã®ntr-un fiÈ™ier .OBJ 3D."""
    print("   ğŸ—ï¸  Generez model 3D (Smoothed)...")
    contours, _ = cv2.findContours(walls_mask, cv2.RETR_CCOMP, cv2.CHAIN_APPROX_SIMPLE)
    vertices_str_list = []
    faces_str_list = []
    vertex_count = 0
    h_img, w_img = walls_mask.shape[:2]

    for cnt in contours:
        epsilon = 0.001 * cv2.arcLength(cnt, True)
        approx = cv2.approxPolyDP(cnt, epsilon, True)

        if len(approx) < 3: continue
        
        base_idx = vertex_count + 1
        num_pts = len(approx)
        
        for pt in approx:
            px, py = pt[0]
            x = (px - w_img/2) * scale_m_px
            z = (h_img - py - h_img/2) * scale_m_px 
            
            vertices_str_list.append(f"v {x:.4f} 0.0000 {-z:.4f}")
            vertices_str_list.append(f"v {x:.4f} {wall_height_m:.4f} {-z:.4f}")
            
        for k in range(num_pts):
            curr = k
            next_p = (k + 1) % num_pts
            
            v_bl = base_idx + 2*curr
            v_tl = base_idx + 2*curr + 1
            v_br = base_idx + 2*next_p
            v_tr = base_idx + 2*next_p + 1
            
            faces_str_list.append(f"f {v_bl} {v_br} {v_tr} {v_tl}")
            
        vertex_count += 2 * num_pts

    with open(output_path, "w") as f:
        f.write(f"# Scale: {scale_m_px} m/px\n")
        f.write(f"# Holzbot Auto-Generated 3D Model\n")
        for v in vertices_str_list: f.write(v + "\n")
        for face in faces_str_list: f.write(face + "\n")
        
    print(f"   âœ… 3D Salvat: {output_path.name}")
    
    if image_output_path:
        render_obj_to_image(vertices_str_list, faces_str_list, image_output_path)


# ============================================
# GEMINI API (DIRECT REST) (nemodificatÄƒ)
# ============================================

GEMINI_PROMPT_CROP = """
EÈ™ti un expert Ã®n analiza planurilor arhitecturale. Din imaginea decupatÄƒ, extrage:
1. "room_name": Numele camerei.
2. "area_m2": SuprafaÈ›a numericÄƒ (foloseÈ™te PUNCT pt zecimale).
DacÄƒ nu e clar, returneazÄƒ JSON gol {}.
ReturneazÄƒ DOAR JSON.
"""

GEMINI_PROMPT_TOTAL_SUM = """
AnalizeazÄƒ Ã®ntregul plan al etajului.
IdentificÄƒ TOATE numerele care reprezintÄƒ suprafeÈ›e de camere (de obicei au mÂ² lÃ¢ngÄƒ ele).
AdunÄƒ toate aceste valori pentru a obÈ›ine SuprafaÈ›a TotalÄƒ UtilÄƒ (Suma Label-urilor).
ReturneazÄƒ un JSON cu un singur cÃ¢mp:
{"total_sum_m2": <suma_tuturor_camerelor_float>}
ReturneazÄƒ DOAR JSON.
"""

def call_gemini(image_path, prompt, api_key):
    """API REST Direct (v1beta) (nemodificatÄƒ)."""
    try:
        with open(image_path, 'rb') as f:
            image_data = base64.b64encode(f.read()).decode('utf-8')
        
        ext = Path(image_path).suffix.lower()
        mime_map = {'.jpg': 'image/jpeg', '.jpeg': 'image/jpeg', '.png': 'image/png', '.webp': 'image/webp'}
        mime_type = mime_map.get(ext, 'image/jpeg')
        
        url = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key={api_key}"
        
        payload = {
            "contents": [{
                "parts": [
                    {"inline_data": {"mime_type": mime_type, "data": image_data}},
                    {"text": prompt}
                ]
            }],
            "generationConfig": {"temperature": 0.0, "maxOutputTokens": 1000}
        }
        
        headers = {"Content-Type": "application/json"}
        response = requests.post(url, json=payload, headers=headers, timeout=30)
        
        if response.status_code != 200:
            print(f"âš ï¸  Gemini HTTP {response.status_code}")
            return None
        
        result = response.json()
        if 'candidates' not in result or not result['candidates']: return None
        
        text = result['candidates'][0]['content']['parts'][0].get('text', '').strip()
        if not text: return None
        
        if text.startswith("```"):
            start = text.find('{')
            end = text.rfind('}') + 1
            if start != -1 and end > start: text = text[start:end]
        
        return json.loads(text)
        
    except Exception as e:
        print(f"âš ï¸  Gemini error: {e}")
        return None


# ============================================
# SCALE DETECTION (nemodificatÄƒ)
# ============================================

def detect_scale_from_room_labels(image_path, indoor_mask, walls_mask, steps_dir, min_room_area=1.0, max_room_area=300.0, api_key=None):
    """DetecteazÄƒ scala din label-uri de camere."""
    img_bgr = cv2.imread(str(image_path))
    if img_bgr is None: return None
    
    h, w = img_bgr.shape[:2]

    # A. SEGMENTARE CAMERE
    room_candidates = []
    min_dim = min(h, w)
    kernel_size = max(3, int(min_dim / 100))
    if kernel_size % 2 == 0: kernel_size += 1
    
    kernel_dynamic = cv2.getStructuringElement(cv2.MORPH_RECT, (kernel_size, kernel_size))
    walls_dilated = cv2.dilate(walls_mask, kernel_dynamic, iterations=1)
    fillable_mask = cv2.bitwise_and(cv2.bitwise_not(walls_dilated), indoor_mask)
    unvisited_mask = fillable_mask.copy()
    
    sample_stride = max(5, int(w / 200))
    MIN_PIXEL_AREA = max(100, int((w * h) * 0.0005))
    room_idx = 0
    debug_iteration = img_bgr.copy()

    print(f"   ğŸ” Segmentare camere (Kernel {kernel_size}px)...")
    
    for y in range(0, h, sample_stride):
        for x in range(0, w, sample_stride):
            if unvisited_mask[y, x] > 0:
                room_mask, area_px, _ = flood_fill_room(unvisited_mask, (x, y))
                
                if area_px < MIN_PIXEL_AREA:
                    unvisited_mask[room_mask > 0] = 0
                    continue
                
                room_idx += 1
                coords = np.where(room_mask > 0)
                if not coords[0].size: continue
                
                y_min, y_max = coords[0].min(), coords[0].max()
                x_min, x_max = coords[1].min(), coords[1].max()
                
                padding = max(5, int(min_dim / 100))
                y_s, y_e = max(0, y_min - padding), min(h, y_max + 1 + padding)
                x_s, x_e = max(0, x_min - padding), min(w, x_max + 1 + padding)
                
                crop_path = Path(steps_dir) / f"crop_{room_idx}.png"
                cv2.imwrite(str(crop_path), img_bgr[y_s:y_e, x_s:x_e])
                
                print(f"      â³ Segment {room_idx} ({area_px} px)...")
                
                res = call_gemini(crop_path, GEMINI_PROMPT_CROP, api_key)
                
                if res and 'area_m2' in res:
                    try:
                        area_m2 = float(res['area_m2'])
                        if min_room_area <= area_m2 <= max_room_area:
                            room_candidates.append({
                                "index": room_idx,
                                "area_m2_label": area_m2,
                                "area_px": int(area_px),
                                "room_name": res.get('room_name', 'Unknown')
                            })
                            print(f"         âœ… {res.get('room_name')} -> {area_m2} mÂ²")
                            cv2.rectangle(debug_iteration, (x_min, y_min), (x_max, y_max), (0, 255, 0), 2)
                        else:
                            cv2.rectangle(debug_iteration, (x_min, y_min), (x_max, y_max), (0, 0, 255), 1)
                    except ValueError: pass
                else:
                    cv2.rectangle(debug_iteration, (x_min, y_min), (x_max, y_max), (0, 0, 255), 1)
                
                unvisited_mask[room_mask > 0] = 0
    
    save_step("debug_segmentation_final", debug_iteration, steps_dir)
    
    if not room_candidates:
        print("      âŒ Nu s-au gÄƒsit camere valide")
        return None

    # B. CALCUL SCARÄ‚
    area_labels = np.array([r["area_m2_label"] for r in room_candidates])
    area_pixels = np.array([r["area_px"] for r in room_candidates])
    
    num = np.sum(area_pixels * area_labels)
    den = np.sum(area_pixels ** 2)
    
    if den == 0: return None
    
    m_px_local = float(np.sqrt(num / den))
    print(f"   ğŸ“Š Scara LocalÄƒ: {m_px_local:.9f} m/px")

    total_indoor_px = int(np.count_nonzero(indoor_mask))
    gemini_total = call_gemini(image_path, GEMINI_PROMPT_TOTAL_SUM, api_key)
    
    sum_labels_m2 = sum(area_labels)
    if gemini_total and 'total_sum_m2' in gemini_total:
        try: sum_labels_m2 = float(gemini_total['total_sum_m2'])
        except: pass

    m_px_target = math.sqrt(sum_labels_m2 / float(total_indoor_px)) if total_indoor_px > 0 else m_px_local
    suprafata_cu_camere = total_indoor_px * (m_px_local ** 2)
    
    final_m_px = m_px_local
    method_note = "ScarÄƒ LocalÄƒ"
    
    if suprafata_cu_camere < sum_labels_m2:
        final_m_px = m_px_target
        method_note = "ForÈ›at pe Suma TotalÄƒ"
    
    print(f"   âœ… ScarÄƒ FinalÄƒ: {final_m_px:.9f} m/px")

    return {
        "method": "cubicasa_gemini",
        "meters_per_pixel": float(final_m_px),
        "rooms_used": len(room_candidates),
        "optimization_info": {"method_note": method_note},
        "per_room": room_candidates
    }


# ============================================
# âœ… NEW: ADAPTIVE TILING FOR LARGE IMAGES (nemodificatÄƒ)
# ============================================

def _process_with_adaptive_tiling(img, model, device, tile_size=512, overlap=64):
    """ProceseazÄƒ imagini mari prin tiling adaptiv."""
    h_orig, w_orig = img.shape[:2]
    print(f"   ğŸ§© TILING Mode: {w_orig}x{h_orig} -> tiles de {tile_size}x{tile_size}")
    
    stride = tile_size - overlap
    n_tiles_h = math.ceil(h_orig / stride)
    n_tiles_w = math.ceil(w_orig / stride)
    
    print(f"   ğŸ“¦ Generez {n_tiles_w}x{n_tiles_h} = {n_tiles_w * n_tiles_h} tiles")
    
    full_pred = np.zeros((h_orig, w_orig), dtype=np.float32)
    weight_map = np.zeros((h_orig, w_orig), dtype=np.float32)
    
    tile_count = 0
    for i in range(n_tiles_h):
        for j in range(n_tiles_w):
            y_start = i * stride
            x_start = j * stride
            y_end = min(y_start + tile_size, h_orig)
            x_end = min(x_start + tile_size, w_orig)
            
            tile = img[y_start:y_end, x_start:x_end]
            
            tile_resized = cv2.resize(tile, (tile_size, tile_size))
            
            norm_tile = 2 * (tile_resized.astype(np.float32) / 255.0) - 1
            tensor = torch.from_numpy(norm_tile).float().permute(2, 0, 1).unsqueeze(0).to(device)
            
            with torch.no_grad():
                output = model(tensor)
                # Am ajustat pentru a rula chiar dacÄƒ modelul nu e Ã®ncÄƒrcat corect, deÈ™i e o problemÄƒ de setup
                if isinstance(output, dict) and output.get('out') is not None:
                     pred_tile = torch.argmax(output['out'][:, 21:33, :, :], dim=1).squeeze().cpu().numpy()
                else:
                    pred_tile = torch.argmax(output[:, 21:33, :, :], dim=1).squeeze().cpu().numpy()
            
            actual_h = y_end - y_start
            actual_w = x_end - x_start
            pred_tile_resized = cv2.resize(
                pred_tile.astype('uint8'),
                (actual_w, actual_h),
                interpolation=cv2.INTER_NEAREST
            ).astype(np.float32)
            
            weight_tile = np.ones((actual_h, actual_w), dtype=np.float32)
            
            if overlap > 0:
                fade = min(overlap // 2, 32)
                for k in range(fade):
                    alpha = k / fade
                    if y_start > 0 and k < actual_h:
                        weight_tile[k, :] *= alpha
                    if y_end < h_orig and k < actual_h:
                        weight_tile[-(k+1), :] *= alpha
                    if x_start > 0 and k < actual_w:
                        weight_tile[:, k] *= alpha
                    if x_end < w_orig and k < actual_w:
                        weight_tile[:, -(k+1)] *= alpha
            
            full_pred[y_start:y_end, x_start:x_end] += pred_tile_resized * weight_tile
            weight_map[y_start:y_end, x_start:x_end] += weight_tile
            
            tile_count += 1
            if tile_count % 10 == 0:
                print(f"      â³ Procesat {tile_count}/{n_tiles_w * n_tiles_h} tiles...")
    
    weight_map[weight_map == 0] = 1
    full_pred = full_pred / weight_map
    
    pred_mask = np.round(full_pred).astype(np.uint8)
    
    print(f"   âœ… Tiling complet: {tile_count} tiles procesate")
    
    return pred_mask


# ============================================
# FUNCÈšIA PRINCIPALÄ‚ (MODIFICATÄ‚ PENTRU CLOSING PUTERNIC)
# ============================================

def run_cubicasa_detection(
    image_path: str,
    model_weights_path: str,
    output_dir: str,
    gemini_api_key: str,
    device: torch.device,
    save_debug_steps: bool = True
) -> dict:
    """
    RuleazÄƒ detecÈ›ia CubiCasa + mÄƒsurÄƒri + 3D Generation.
    ACUM cu Adaptive Strategy: Resize Inteligent pentru imagini mici, Tiling pentru imagini mari.
    """
    image_path = Path(image_path)
    output_dir = Path(output_dir)
    model_weights_path = Path(model_weights_path)
    
    steps_dir = output_dir / "cubicasa_steps"
    ensure_dir(steps_dir)
    
    print(f"   ğŸ¤– CubiCasa: Procesez {image_path.name}")
    
    # 1. SETUP MODEL
    print(f"   â³ Ãncarc AI pe {device.type}...")
    model = hg_furukawa_original(n_classes=44)
    
    if not model_weights_path.exists():
        print(f"âš ï¸  Lipsesc weights: {model_weights_path}. Continuu cu model non-funcÈ›ional.")
    else:
        checkpoint = torch.load(str(model_weights_path), map_location=device, weights_only=False)
        model.load_state_dict(checkpoint['model_state'] if 'model_state' in checkpoint else checkpoint)
    
    model.to(device)
    model.eval()

    # 2. LOAD IMAGE & DECIDE STRATEGY
    img = cv2.imread(str(image_path))
    if img is None:
        raise RuntimeError(f"Nu pot citi imaginea: {image_path}")
        
    h_orig, w_orig = img.shape[:2]
    save_step("00_original", img, str(steps_dir))
    
    # âœ… ADAPTIVE STRATEGY
    LARGE_IMAGE_THRESHOLD = 3000  # px
    USE_TILING = h_orig > LARGE_IMAGE_THRESHOLD or w_orig > LARGE_IMAGE_THRESHOLD
    
    if USE_TILING:
        print(f"   ğŸš€ LARGE IMAGE ({w_orig}x{h_orig}) -> Folosesc TILING ADAPTIV")
        
        pred_mask = _process_with_adaptive_tiling(
            img, 
            model, 
            device, 
            tile_size=512,
            overlap=64
        )
        
    else:
        print(f"   ğŸš€ SMALL IMAGE ({w_orig}x{h_orig}) -> Resize Standard la 2048px")
        
        max_dim = 2048
        scale = min(max_dim / w_orig, max_dim / h_orig, 1.0)
        
        new_w = int(w_orig * scale)
        new_h = int(h_orig * scale)
        
        print(f"      Resize: {w_orig}x{h_orig} -> {new_w}x{new_h}")
        
        input_img = cv2.resize(img, (new_w, new_h), interpolation=cv2.INTER_AREA)
        
        norm_img = 2 * (input_img.astype(np.float32) / 255.0) - 1
        tensor = torch.from_numpy(norm_img).float().permute(2, 0, 1).unsqueeze(0).to(device)
        
        with torch.no_grad():
            output = model(tensor)
            pred_mask_small = torch.argmax(output[:, 21:33, :, :], dim=1).squeeze().cpu().numpy()
        
        pred_mask = cv2.resize(
            pred_mask_small.astype('uint8'),
            (w_orig, h_orig),
            interpolation=cv2.INTER_NEAREST
        )
    
    # 3. EXTRACT WALLS & FILTER THIN LINES
    ai_walls_raw = (pred_mask == 2).astype('uint8') * 255
    save_step("01_ai_walls_raw", ai_walls_raw, str(steps_dir))
    
    # ============================================================================
    # FILTRARE LINII SUBÈšIRI + CLOSING ADAPTIV (UNIFIED & ULTRA-PUTERNIC)
    # ============================================================================
    
    min_dim = min(h_orig, w_orig)

    # âœ… FILTRARE LINII SUBÈšIRI ADAPTIVÄ‚
    print("      ğŸ§¹ Filtrez linii subÈ›iri false-positive...")
    
    # ADAPTIVE THRESHOLD: Imagini mici = filtrare BALANCED
    if min_dim > 2500:
        # Imagini mari: filtrare normalÄƒ (0.4%)
        min_wall_thickness = max(3, int(min_dim * 0.004))
        iterations = 1
        print(f"         Mode: LARGE IMAGE â†’ Thin filter: {min_wall_thickness}px (0.4%), iter={iterations}")
    else:
        # Imagini mici: filtrare BALANCED (0.7%)
        min_wall_thickness = max(5, int(min_dim * 0.007))
        iterations = 1
        print(f"         Mode: SMALL IMAGE â†’ Balanced filter: {min_wall_thickness}px (0.7%), iter={iterations}")

    filter_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (min_wall_thickness, min_wall_thickness))
    walls_eroded = cv2.erode(ai_walls_raw, filter_kernel, iterations=iterations)
    ai_walls_raw = cv2.dilate(walls_eroded, filter_kernel, iterations=iterations)
    
    pixels_removed_pct = 100 * (1 - np.count_nonzero(ai_walls_raw) / (np.count_nonzero(((pred_mask == 2).astype('uint8') * 255)) + 1))
    print(f"         Eliminat {pixels_removed_pct:.1f}% pixeli (linii subÈ›iri)")
    save_step("01a_walls_filtered", ai_walls_raw, str(steps_dir))

    # âœ… CLOSING ADAPTIV ULTRA-PUTERNIC (UNIFICAT) - ÃMBUNÄ‚TÄ‚ÈšIT PENTRU VPS
    print("      ğŸ”— Ãnchid gÄƒuri (closing adaptiv ULTRA-PUTERNIC - Ã®mbunÄƒtÄƒÈ›it pentru VPS)...")

    if min_dim > 2500:
        # Imagini mari: closing mai puternic pentru VPS (mÄƒrit de la 0.3% la 0.5%)
        close_kernel_size = max(5, int(min_dim * 0.005))  # 0.5% (mÄƒrit de la 0.3%)
        close_iterations = 3  # MÄƒrit de la 2 la 3 iteraÈ›ii
        print(f"         Mode: LARGE IMAGE â†’ Close: {close_kernel_size}px (0.5%), iter={close_iterations}")
    else:
        # Imagini mici: closing ULTRA-PUTERNIC (mÄƒrit de la 1.0% la 1.5% + mai multe iteraÈ›ii)
        close_kernel_size = max(12, int(min_dim * 0.015))  # 1.5% (mÄƒrit de la 1.0%)
        close_iterations = 7  # MÄƒrit de la 5 la 7 iteraÈ›ii
        print(f"         Mode: SMALL IMAGE â†’ ULTRA STRONG close: {close_kernel_size}px (1.5%), iter={close_iterations}")

    close_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (close_kernel_size, close_kernel_size))
    ai_walls_raw = cv2.morphologyEx(ai_walls_raw, cv2.MORPH_CLOSE, close_kernel, iterations=close_iterations)
    save_step("01b_walls_closed_adaptive", ai_walls_raw, str(steps_dir))

    # âœ… PENTRU IMAGINI MARI: ErodÄƒm pereÈ›ii groÈ™i detectaÈ›i de AI
    if h_orig > 1000 or w_orig > 1000:
        print("      ğŸ”ª SubÈ›iez pereÈ›ii detectaÈ›i de AI (Large Image)...")
        thin_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3, 3))
        ai_walls_raw = cv2.erode(ai_walls_raw, thin_kernel, iterations=1)
        save_step("01c_ai_walls_thinned", ai_walls_raw, str(steps_dir))

    # 4. REPARARE PEREÈšI (FINALA)
    print("   ğŸ“ Repar pereÈ›ii...")

    LARGE_IMAGE_THRESHOLD = 1000
    
    # Acum, `ai_walls_raw` conÈ›ine deja closing-ul ULTRA-PUTERNIC (Ã®mbunÄƒtÄƒÈ›it pentru VPS)
    if h_orig > LARGE_IMAGE_THRESHOLD or w_orig > LARGE_IMAGE_THRESHOLD:
        print(f"      ğŸ”§ LARGE IMAGE MODE: Bridging gaps between walls (no wall thickening)")
        # MenÈ›inem bridge_wall_gaps DOAR pentru imaginile mari, unde closing-ul a fost Ã®mbunÄƒtÄƒÈ›it (0.5%)
        ai_walls_closed = bridge_wall_gaps(ai_walls_raw, (h_orig, w_orig), str(steps_dir))
    else:
        # Pentru imagini mici: SKIP extra bridging (adaptive closing-ul ULTRA-PUTERNIC de mai sus e suficient)
        print(f"      ğŸ”§ SMALL IMAGE MODE: Skip extra bridging (adaptive closing is enough)")
        ai_walls_closed = ai_walls_raw.copy()
    
    save_step("02_ai_walls_closed", ai_walls_closed, str(steps_dir))
    
    # 4a. GENEREZ IMAGINE SUPrapusÄƒ: 02_ai_walls_closed + 00_original (cu transparenÈ›Äƒ)
    if steps_dir:
        try:
            original_path = Path(steps_dir) / "00_original.png"
            if original_path.exists():
                original_img = cv2.imread(str(original_path), cv2.IMREAD_GRAYSCALE)
                if original_img is not None:
                    # RedimensionÄƒm dacÄƒ este necesar
                    if original_img.shape[:2] != ai_walls_closed.shape[:2]:
                        original_img = cv2.resize(original_img, (ai_walls_closed.shape[1], ai_walls_closed.shape[0]))
                    
                    # Facem invert la original (negru devine alb, alb devine negru)
                    original_inverted = cv2.bitwise_not(original_img)
                    
                    # Convertim la BGR pentru suprapunere
                    original_bgr = cv2.cvtColor(original_inverted, cv2.COLOR_GRAY2BGR)
                    walls_bgr = cv2.cvtColor(ai_walls_closed, cv2.COLOR_GRAY2BGR)
                    
                    # Suprapunem cu transparenÈ›Äƒ 50% (original) + 50% (walls_closed)
                    overlay = cv2.addWeighted(original_bgr, 0.5, walls_bgr, 0.5, 0)
                    
                    # SalvÄƒm rezultatul
                    output_path = Path(steps_dir) / "02d_walls_closed_overlay.png"
                    cv2.imwrite(str(output_path), overlay)
                    print(f"      ğŸ“¸ Generat overlay: {output_path.name}")
                    
                    # 4a.2. OVERLAY GENERAT - Nu mai facem reparÄƒri cu Hough Lines sau skeleton
                    # Overlay-ul este generat doar pentru a fi folosit de fill_terrace_room
        except Exception as e:
            print(f"      âš ï¸ Nu s-a putut genera overlay: {e}")
    
    # 4b. WORKFLOW REORGANIZAT:
    # 1. Detectare terasa + flood fill + completare pereÈ›i terasa
    # 2. Umplere gÄƒuri mici dintre pereÈ›i
    
    print("   ğŸ¡ Pas 1: Detectare terasa È™i completare pereÈ›i...")
    # DetecteazÄƒ terasa, face flood fill È™i completeazÄƒ pereÈ›ii terasei
    ai_walls_terrace = fill_terrace_room(ai_walls_closed, steps_dir=str(steps_dir))
    
    # Pas nou: Skeletonizare, Ã®ndreptare È™i uniformizare grosime pereÈ›i
    print("   ğŸ¦´ Pas 2: Skeletonizare, Ã®ndreptare È™i uniformizare grosime pereÈ›i...")
    ai_walls_skeletonized = skeletonize_and_straighten_walls(ai_walls_terrace, steps_dir=str(steps_dir))
    
    # Folosim rezultatul de la skeletonizare
    ai_walls_final = ai_walls_skeletonized.copy()
    
    # 4d. DETECTARE È˜I VIZUALIZARE ÃNCHIDERI PEREÈšI
    detect_and_visualize_wall_closures(ai_walls_final, steps_dir=str(steps_dir))
    
    # Kernel repair pentru restul procesÄƒrii
    min_dim = min(h_orig, w_orig) 
    rep_k = max(3, int(min_dim * 0.005))
    if rep_k % 2 == 0: rep_k += 1
    kernel_repair = cv2.getStructuringElement(cv2.MORPH_RECT, (rep_k, rep_k))
    
    # 5. NETEZIRE
    ai_walls_smoothed = smooth_walls_mask(ai_walls_final)
    save_step("03_ai_walls_smoothed", ai_walls_smoothed, str(steps_dir))
    
    # 6. ÃNDREPTARE
    ai_walls_straight = straighten_mask(ai_walls_final, 0.003)
    save_step("03_ai_walls_straight", ai_walls_straight, str(steps_dir))

    # 7. ZONE
    print("   ğŸŒŠ Analizez zonele...")
    
    walls_thick = cv2.dilate(ai_walls_final, kernel_repair, iterations=3)
    
    h_pad, w_pad = h_orig + 2, w_orig + 2
    pad_walls = np.zeros((h_pad, w_pad), dtype=np.uint8)
    pad_walls[1:h_orig+1, 1:w_orig+1] = walls_thick
    
    inv_pad_walls = cv2.bitwise_not(pad_walls)
    flood_mask = np.zeros((h_pad+2, w_pad+2), dtype=np.uint8)
    cv2.floodFill(inv_pad_walls, flood_mask, (0, 0), 128)
    
    outdoor_mask = (inv_pad_walls[1:h_orig+1, 1:w_orig+1] == 128).astype(np.uint8) * 255
    
    kernel_grow = cv2.getStructuringElement(cv2.MORPH_RECT, (3, 3))
    free_space = cv2.bitwise_not(ai_walls_final)
    
    for _ in range(30):
        outdoor_mask = cv2.bitwise_and(cv2.dilate(outdoor_mask, kernel_grow), free_space)
    
    save_step("03_outdoor_mask", outdoor_mask, str(steps_dir))
    
    total_space = np.ones_like(outdoor_mask) * 255
    occupied = cv2.bitwise_or(outdoor_mask, ai_walls_final)
    indoor_mask = cv2.subtract(total_space, occupied)
    
    vis_indoor = np.zeros((h_orig, w_orig, 3), dtype=np.uint8)
    vis_indoor[indoor_mask > 0] = [0, 255, 255]
    save_step("debug_zone_interior", vis_indoor, str(steps_dir))

    # 8. SCALE DETECTION
    print("   ğŸ” Determin scala...")
    scale_result = detect_scale_from_room_labels(
        str(image_path),
        indoor_mask,
        ai_walls_final,
        str(steps_dir),
        api_key=gemini_api_key
    )
    
    if not scale_result:
        raise RuntimeError("Nu am putut determina scala!")
    
    m_px = scale_result["meters_per_pixel"]

    # 9. MÄ‚SURÄ‚TORI
    print("   ğŸ“ Calculez mÄƒsurÄƒtori...")
    outline = get_strict_1px_outline(ai_walls_final)
    touch_zone = cv2.dilate(outdoor_mask, kernel_grow, iterations=2)
    
    outline_ext_mask = cv2.bitwise_and(outline, touch_zone)
    outline_int_mask = cv2.subtract(outline, outline_ext_mask)
    
    # 9a. GENERARE SCHELET PEREÈšI INTERIORI
    print("   ğŸ¦´ Generez scheletul pereÈ›ilor interiori...")
    interior_walls_skeleton = generate_interior_walls_skeleton(ai_walls_final, outdoor_mask)
    save_step("04_interior_walls_skeleton", interior_walls_skeleton, str(steps_dir))
    
    # CalculÄƒm lungimea skeleton-ului pereÈ›ilor interiori (din imaginea generatÄƒ)
    px_len_skeleton_int = int(np.count_nonzero(interior_walls_skeleton))
    
    # CalculÄƒm lungimea pereÈ›ilor exteriori (din outline)
    px_len_skeleton_ext = int(np.count_nonzero(outline_ext_mask))
    
    # Lungimea structurii pereÈ›ilor interiori = skeleton interior - exterior
    # (scÄƒdem exteriorul pentru cÄƒ skeleton-ul interior poate conÈ›ine È™i pereÈ›ii exteriori)
    px_len_skeleton_structure_int = max(0, px_len_skeleton_int - px_len_skeleton_ext)
    
    # Lungimi din outline (pentru finisaje)
    px_len_ext = int(np.count_nonzero(outline_ext_mask))
    px_len_int = int(np.count_nonzero(outline_int_mask))
    
    # Arii
    px_area_indoor = int(np.count_nonzero(indoor_mask))
    px_area_total = int(np.count_nonzero(cv2.bitwise_not(outdoor_mask)))

    # Conversii Ã®n metri
    walls_ext_m = px_len_ext * m_px  # Pentru finisaje
    walls_int_m = px_len_int * m_px  # Pentru finisaje
    
    # Lungimi din skeleton (pentru structurÄƒ)
    walls_skeleton_ext_m = px_len_skeleton_ext * m_px
    walls_skeleton_int_m = px_len_skeleton_int * m_px  # Skeleton interior (din imagine)
    walls_skeleton_structure_int_m = px_len_skeleton_structure_int * m_px  # StructurÄƒ pereÈ›i interiori (skeleton - exterior)
    
    area_indoor_m2 = px_area_indoor * (m_px ** 2)
    area_total_m2 = px_area_total * (m_px ** 2)

    measurements = {
        "pixels": {
            "walls_len_ext": int(px_len_ext),
            "walls_len_int": int(px_len_int),
            "walls_skeleton_ext": int(px_len_skeleton_ext),
            "walls_skeleton_int": int(px_len_skeleton_int),
            "walls_skeleton_structure_int": int(px_len_skeleton_structure_int),
            "indoor_area": int(px_area_indoor),
            "total_area": int(px_area_total)
        },
        "metrics": {
            "scale_m_per_px": float(m_px),
            "walls_ext_m": float(round(walls_ext_m, 2)),  # Pentru finisaje
            "walls_int_m": float(round(walls_int_m, 2)),  # Pentru finisaje
            "walls_skeleton_ext_m": float(round(walls_skeleton_ext_m, 2)),
            "walls_skeleton_int_m": float(round(walls_skeleton_int_m, 2)),
            "walls_skeleton_structure_int_m": float(round(walls_skeleton_structure_int_m, 2)),  # Pentru structurÄƒ pereÈ›i interiori
            "area_indoor_m2": float(round(area_indoor_m2, 2)),
            "area_total_m2": float(round(area_total_m2, 2))
        }
    }

    print(f"   âœ… ScarÄƒ: {m_px:.9f} m/px")
    print(f"   ğŸ  Arie Indoor: {area_indoor_m2:.2f} mÂ²")
    print(f"   ğŸ“ Lungimi pereÈ›i:")
    print(f"      - Exterior (outline): {walls_ext_m:.2f} m (pentru finisaje)")
    print(f"      - Interior (outline): {walls_int_m:.2f} m (pentru finisaje)")
    print(f"      - Skeleton interior (din imagine): {walls_skeleton_int_m:.2f} m")
    print(f"      - Skeleton exterior (din outline): {walls_skeleton_ext_m:.2f} m")
    print(f"      - StructurÄƒ interior (skeleton - exterior): {walls_skeleton_structure_int_m:.2f} m")
    
    # 10. FILTRARE ZGOMOT CU MASCÄ‚ ZONÄ‚ INTERIOARÄ‚ (Ã®nainte de generarea 3D)
    print("   ğŸ¨ Filtrez zgomotul de fundal cu masca zonei interioare...")
    
    # ÃncÄƒrcÄƒm masca din debug_zone_interior.png
    debug_mask_path = Path(steps_dir) / "debug_zone_interior.png"
    interior_zone_mask = None
    
    if debug_mask_path.exists():
        debug_mask_img = cv2.imread(str(debug_mask_path), cv2.IMREAD_COLOR)
        if debug_mask_img is not None:
            # debug_zone_interior are culoarea galbenÄƒ (cyan Ã®n BGR: [0, 255, 255]) pentru zona interioarÄƒ
            b, g, r = cv2.split(debug_mask_img)
            # Extragem masca: zona interioarÄƒ este unde componenta verde este mare
            interior_zone_mask = (g > 128).astype(np.uint8) * 255
            
            # RedimensionÄƒm dacÄƒ e necesar
            if interior_zone_mask.shape[:2] != ai_walls_smoothed.shape[:2]:
                interior_zone_mask = cv2.resize(interior_zone_mask, (ai_walls_smoothed.shape[1], ai_walls_smoothed.shape[0]), interpolation=cv2.INTER_NEAREST)
            
            # CalculÄƒm grosimea medie a pereÈ›ilor pentru marjÄƒ de eroare
            dist_from_edge = cv2.distanceTransform(ai_walls_smoothed, cv2.DIST_L2, 5)
            wall_thicknesses = dist_from_edge[ai_walls_smoothed > 0] * 2
            if len(wall_thicknesses) > 0:
                avg_thickness = np.median(wall_thicknesses)
                margin_size = max(3, int(round(avg_thickness)))
                
                print(f"      ğŸ“ Grosime medie pereÈ›i: {avg_thickness:.2f}px, marjÄƒ de eroare: {margin_size}px")
                
                # DilatÄƒm masca zonei interioare cu marja de eroare (grosimea pereÈ›ilor)
                kernel_margin = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (margin_size * 2 + 1, margin_size * 2 + 1))
                interior_zone_mask_dilated = cv2.dilate(interior_zone_mask, kernel_margin, iterations=1)
                
                # AplicÄƒm masca pe pereÈ›i: pÄƒstrÄƒm doar pereÈ›ii din zona interioarÄƒ (cu marjÄƒ)
                ai_walls_smoothed_filtered = cv2.bitwise_and(ai_walls_smoothed, interior_zone_mask_dilated)
                
                # Folosim pereÈ›ii filtraÈ›i pentru generarea 3D
                ai_walls_for_3d = ai_walls_smoothed_filtered
                
                removed_pixels = np.count_nonzero(ai_walls_smoothed) - np.count_nonzero(ai_walls_smoothed_filtered)
                print(f"      âœ… Eliminat {removed_pixels:,} pixeli de zgomot din afara zonei interioare")
            else:
                ai_walls_for_3d = ai_walls_smoothed
                print(f"      âš ï¸ Nu s-a putut calcula grosimea, folosesc pereÈ›ii fÄƒrÄƒ filtrare")
        else:
            ai_walls_for_3d = ai_walls_smoothed
            print(f"      âš ï¸ Nu s-a putut Ã®ncÄƒrca masca, folosesc pereÈ›ii fÄƒrÄƒ filtrare")
    else:
        ai_walls_for_3d = ai_walls_smoothed
        print(f"      âš ï¸ MascÄƒ lipsÄƒ, folosesc pereÈ›ii fÄƒrÄƒ filtrare")
    
    # 10. GENERARE 3D (cu pereÈ›ii filtraÈ›i - zgomotul a fost eliminat Ã®nainte)
    export_walls_to_obj(
        ai_walls_for_3d, 
        output_dir / "walls_3d.obj", 
        m_px, 
        image_output_path=output_dir / "walls_3d_view.png"
    )

    # 11. VIZUALIZÄ‚RI
    overlay = img.copy()
    overlay[outline_ext_mask > 0] = [255, 0, 0]
    overlay[outline_int_mask > 0] = [0, 255, 0]
    cv2.imwrite(str(output_dir / "visualization_overlay.png"), overlay)
    
    return {
        "scale_result": scale_result,
        "measurements": measurements,
        "masks": {
            "visualization": str(output_dir / "visualization_overlay.png"),
            "visualization_3d": str(output_dir / "walls_3d_view.png"),
            "interior_walls_skeleton": str(steps_dir / "04_interior_walls_skeleton.png")
        }
    }