# file: engine/runner/segmenter/clusters.py
from __future__ import annotations

import math
import os
import io
import base64
from pathlib import Path

import cv2
import numpy as np
from PIL import Image
from dotenv import load_dotenv

from sklearn.cluster import KMeans

from .common import STEP_DIRS, save_debug, resize_bgr_max_side, get_output_dir
from .classifier import setup_gemini_client

load_dotenv()


def _box_inside(inner: list[int], outer: list[int], tolerance: int = 2) -> bool:
    """True dacÄƒ cutia inner este Ã®n Ã®ntregime Ã®n interiorul cutiei outer (cu tolerance pixeli)."""
    ix1, iy1, ix2, iy2 = inner
    ox1, oy1, ox2, oy2 = outer
    return (
        ix1 >= ox1 - tolerance and iy1 >= oy1 - tolerance and
        ix2 <= ox2 + tolerance and iy2 <= oy2 + tolerance
    )


def _split_constituents_into_n_groups(
    constituents: list[list[int]], n: int, by_vertical: bool
) -> list[list[int]]:
    """
    Ãmparte constituenÈ›ii Ã®n N grupuri dupÄƒ poziÈ›ie (verticalÄƒ pentru etaje, orizontalÄƒ pentru clÄƒdiri).
    ReturneazÄƒ N cutii: fiecare cutie = bounding box al constituenÈ›ilor din acel grup.
    """
    if not constituents or n < 2:
        return []
    # Centru pentru fiecare cutie
    if by_vertical:
        # Etaje stacked: sortÄƒm dupÄƒ y (centru vertical)
        sorted_const = sorted(
            constituents,
            key=lambda b: (b[1] + b[3]) / 2,
        )
    else:
        # ClÄƒdiri side-by-side: sortÄƒm dupÄƒ x (centru orizontal)
        sorted_const = sorted(
            constituents,
            key=lambda b: (b[0] + b[2]) / 2,
        )
    k = len(sorted_const)
    groups: list[list[list[int]]] = [[] for _ in range(n)]
    for i, box in enumerate(sorted_const):
        g = min(i * n // k, n - 1)  # partiÈ›ionare uniformÄƒ
        groups[g].append(box)
    result: list[list[int]] = []
    for grp in groups:
        if not grp:
            continue
        x1 = min(b[0] for b in grp)
        y1 = min(b[1] for b in grp)
        x2 = max(b[2] for b in grp)
        y2 = max(b[3] for b in grp)
        result.append([x1, y1, x2, y2])
    return result if len(result) >= 2 else []


def _is_overlapping(box1: list[int], box2: list[int], threshold: float = 0.60) -> bool:
    """
    VerificÄƒ dacÄƒ douÄƒ cutii se suprapun semnificativ.
    FoloseÈ™te Intersection over Minimum Area pentru a detecta
    dacÄƒ una este conÈ›inutÄƒ Ã®n cealaltÄƒ sau dacÄƒ sunt aproape identice.
    """
    x1, y1, x2, y2 = box1
    xx1, yy1, xx2, yy2 = box2

    # Coordonatele intersecÈ›iei
    ix1 = max(x1, xx1)
    iy1 = max(y1, yy1)
    ix2 = min(x2, xx2)
    iy2 = min(y2, yy2)

    inter_w = max(0, ix2 - ix1)
    inter_h = max(0, iy2 - iy1)
    inter_area = inter_w * inter_h

    if inter_area == 0:
        return False

    area1 = (x2 - x1) * (y2 - y1)
    area2 = (xx2 - xx1) * (yy2 - yy1)
    
    # Raportul faÈ›Äƒ de cea mai micÄƒ arie (detecteazÄƒ containment)
    min_area = min(area1, area2)
    if min_area == 0: return False

    overlap_ratio = inter_area / min_area
    return overlap_ratio > threshold


def _draw_debug_boxes(img: np.ndarray, boxes: list[list[int]], color: tuple[int, int, int], label_prefix: str = "") -> np.ndarray:
    vis = img.copy()
    for i, (x1, y1, x2, y2) in enumerate(boxes):
        cv2.rectangle(vis, (x1, y1), (x2, y2), color, 2)
        label = f"{label_prefix}{i}"
        cv2.putText(vis, label, (x1, y1 - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)
    return vis


def _draw_detailed_fallback_debug(img: np.ndarray, all_candidates: list[list[int]], accepted_indices: set[int], ref_index: int, parent_box: list[int]) -> np.ndarray:
    """
    Vizualizare Fallback:
    - GRI: PÄƒrintele (ignorat).
    - ALBASTRU (CYAN): REFERINÈšA REALÄ‚ (Cel mai mare copil valid).
    - VERDE: Acceptat.
    - ROÈ˜U: Respins.
    """
    vis = img.copy()
    
    # 1. DesenÄƒm PÄƒrintele (doar contur, pentru context)
    px1, py1, px2, py2 = parent_box
    cv2.rectangle(vis, (px1, py1), (px2, py2), (100, 100, 100), 1)
    cv2.putText(vis, "PARENT (IGNORED)", (px1+10, py1+20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (100, 100, 100), 1)

    # 2. DesenÄƒm elementele respinse
    for i, box in enumerate(all_candidates):
        if i == ref_index or i in accepted_indices: continue
        x1, y1, x2, y2 = box
        cv2.rectangle(vis, (x1, y1), (x2, y2), (0, 0, 255), 1) # RoÈ™u

    # 3. DesenÄƒm elementele acceptate
    for i in accepted_indices:
        if i == ref_index: continue
        x1, y1, x2, y2 = all_candidates[i]
        cv2.rectangle(vis, (x1, y1), (x2, y2), (0, 255, 0), 2) # Verde
        cv2.putText(vis, f"OK {i}", (x1, y1 - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)

    # 4. DesenÄƒm REFERINÈšA
    if ref_index >= 0:
        x1, y1, x2, y2 = all_candidates[ref_index]
        cv2.rectangle(vis, (x1, y1), (x2, y2), (255, 255, 0), 4) # Cyan gros
        label = "REFERINTA (MAX CHILD)"
        (w, h), _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2)
        cv2.rectangle(vis, (x1, y1 - 25), (x1 + w, y1), (255, 255, 0), -1)
        cv2.putText(vis, label, (x1, y1 - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 0), 2)

    return vis


def split_large_cluster(region: np.ndarray, x1: int, y1: int, idx: int) -> list[list[int]]:
    h, w = region.shape
    area = h * w
    if area < 30000:
        return [[x1, y1, x1 + w, y1 + h]]

    col_sum = np.sum(region > 0, axis=0)
    row_sum = np.sum(region > 0, axis=1)
    col_smooth = cv2.GaussianBlur(col_sum.astype(np.float32), (51, 1), 0)
    row_smooth = cv2.GaussianBlur(row_sum.astype(np.float32), (1, 51), 0)
    col_norm = col_smooth / (np.max(col_smooth) + 1e-5)
    row_norm = row_smooth / (np.max(row_smooth) + 1e-5)

    col_split = np.where(col_norm < 0.10)[0]
    row_split = np.where(row_norm < 0.10)[0]

    boxes: list[list[int]] = []

    if len(col_split) > 0:
        gaps = np.diff(col_split)
        big_gaps = np.where(gaps > 50)[0]
        if len(big_gaps) > 0:
            mid = int(np.median(col_split))
            if 0.3 * w < mid < 0.7 * w:
                save_debug(region, STEP_DIRS["clusters"]["split"], f"split_col_{idx}.jpg")
                for part, offset in [(region[:, :mid], 0), (region[:, mid:], mid)]:
                    num, _, stats, _ = cv2.connectedComponentsWithStats(part, 8)
                    for x, y, ww, hh, a in stats[1:]:
                        if a > 0.02 * area:
                            boxes.append([x1 + offset + x, y1 + y, x1 + offset + x + ww, y1 + y + hh])
                return boxes

    if len(row_split) > 0:
        gaps = np.diff(row_split)
        big_gaps = np.where(gaps > 50)[0]
        if len(big_gaps) > 0:
            mid = int(np.median(row_split))
            if 0.3 * h < mid < 0.7 * h:
                save_debug(region, STEP_DIRS["clusters"]["split"], f"split_row_{idx}.jpg")
                for part, offset in [(region[:mid, :], 0), (region[mid:, :], mid)]:
                    num, _, stats, _ = cv2.connectedComponentsWithStats(part, 8)
                    for x, y, ww, hh, a in stats[1:]:
                        if a > 0.02 * area:
                            boxes.append([x1 + x, y1 + offset + y, x1 + x + ww, y1 + offset + y + hh])
                return boxes

    return [[x1, y1, x1 + w, y1 + h]]


def merge_overlapping_boxes(boxes: list[list[int]], shape: tuple[int, int]) -> list[list[int]]:
    h, w = shape[:2]
    diag = math.hypot(h, w)
    prox = 0.005 * diag

    merged = True
    while merged:
        merged = False
        new_boxes: list[list[int]] = []
        while boxes:
            x1, y1, x2, y2 = boxes.pop(0)
            mbox = [x1, y1, x2, y2]
            keep: list[list[int]] = []

            for (xx1, yy1, xx2, yy2) in boxes:
                inter_x1, inter_y1 = max(x1, xx1), max(y1, yy1)
                inter_x2, inter_y2 = min(x2, xx2), min(y2, yy2)
                inter_area = max(0, inter_x2 - inter_x1) * max(0, inter_y2 - inter_y1)

                area1 = (x2 - x1) * (y2 - y1)
                area2 = (xx2 - xx1) * (yy2 - yy1)
                smaller_ratio = min(area1, area2) / max(area1, area2) if max(area1, area2) > 0 else 0

                dx = max(0, max(x1 - xx2, xx1 - x2))
                dy = max(0, max(y1 - yy2, yy1 - y2))
                dist = math.hypot(dx, dy)

                if inter_area > 0 or (dist <= prox and smaller_ratio < 0.3):
                    mbox = [
                        min(mbox[0], xx1),
                        min(mbox[1], yy1),
                        max(mbox[2], xx2),
                        max(mbox[3], yy2),
                    ]
                    merged = True
                else:
                    keep.append([xx1, yy1, xx2, yy2])

            boxes = keep
            new_boxes.append(mbox)
        boxes = new_boxes

    return boxes


# Maxim 10 clustere finale; fiecare cluster trebuie sÄƒ aibÄƒ minim 10% din suprafaÈ›a imaginii
MAX_CLUSTERS = 10
MIN_CLUSTER_AREA_RATIO = 0.10


def _box_distance(box1: list[int], box2: list[int]) -> float:
    """DistanÈ›a Ã®ntre douÄƒ cutii (0 dacÄƒ se suprapun). Gap pe x/y = max(0, inter_stanga - inter_dreapta)."""
    x1, y1, x2, y2 = box1
    xx1, yy1, xx2, yy2 = box2
    inter_x1, inter_x2 = max(x1, xx1), min(x2, xx2)
    inter_y1, inter_y2 = max(y1, yy1), min(y2, yy2)
    dx = max(0, inter_x1 - inter_x2)
    dy = max(0, inter_y1 - inter_y2)
    return math.hypot(dx, dy)


def merge_nearby_boxes(
    boxes: list[list[int]],
    shape: tuple[int, int],
    max_boxes: int = MAX_CLUSTERS,
    proximity_ratio: float = 0.06,
) -> list[list[int]]:
    """
    Uneste cutii apropiate pÃ¢nÄƒ cÃ¢nd avem cel mult max_boxes.
    proximity_ratio: douÄƒ cutii se unesc dacÄƒ distanÈ›a Ã®ntre ele <= proximity_ratio * diagonala imaginii.
    """
    if len(boxes) <= max_boxes:
        return boxes
    h, w = shape[:2]
    diag = math.hypot(h, w)
    prox = proximity_ratio * diag
    current = list(boxes)
    while len(current) > max_boxes:
        best_i, best_j = -1, -1
        best_dist = 1e9
        for i in range(len(current)):
            for j in range(i + 1, len(current)):
                d = _box_distance(current[i], current[j])
                if d <= prox and d < best_dist:
                    best_dist = d
                    best_i, best_j = i, j
        if best_i < 0:
            # Niciun pair Ã®n proximity â€“ Ã®mpreunÄƒm oricum perechea cea mai apropiatÄƒ pÃ¢nÄƒ la max_boxes
            for i in range(len(current)):
                for j in range(i + 1, len(current)):
                    d = _box_distance(current[i], current[j])
                    if d < best_dist:
                        best_dist = d
                        best_i, best_j = i, j
        if best_i < 0:
            break
        b1, b2 = current[best_i], current[best_j]
        merged_box = [
            min(b1[0], b2[0]),
            min(b1[1], b2[1]),
            max(b1[2], b2[2]),
            max(b1[3], b2[3]),
        ]
        current = [b for k, b in enumerate(current) if k != best_i and k != best_j]
        current.append(merged_box)
    return current


def _iou_boxes(b1: list[int], b2: list[int]) -> float:
    """IoU Ã®ntre douÄƒ box-uri [x1,y1,x2,y2]. ReturneazÄƒ valoare Ã®n [0, 1]."""
    x1, y1, x2, y2 = b1
    xx1, yy1, xx2, yy2 = b2
    inter_x1 = max(x1, xx1)
    inter_y1 = max(y1, yy1)
    inter_x2 = min(x2, xx2)
    inter_y2 = min(y2, yy2)
    if inter_x2 <= inter_x1 or inter_y2 <= inter_y1:
        return 0.0
    inter_area = (inter_x2 - inter_x1) * (inter_y2 - inter_y1)
    area1 = (x2 - x1) * (y2 - y1)
    area2 = (xx2 - xx1) * (yy2 - yy1)
    union = area1 + area2 - inter_area
    return inter_area / union if union > 0 else 0.0


def _merge_clusters_into_zones(
    boxes: list[list[int]], shape: tuple[int, int]
) -> list[list[int]] | None:
    """
    DacÄƒ avem >10 cutii, verificÄƒ dacÄƒ sunt grupate Ã®n cel puÈ›in 2 zone spaÈ›iale.
    Grupare cu KMeans pe centrele cutiilor; pentru fiecare zonÄƒ facem union bbox.
    ReturneazÄƒ lista de zone (bbox-uri) dacÄƒ existÄƒ â‰¥2 zone bine separate, altfel None.
    """
    if len(boxes) <= 10:
        return None
    centers = np.array(
        [((b[0] + b[2]) / 2, (b[1] + b[3]) / 2) for b in boxes],
        dtype=np.float64,
    )
    h, w = shape[:2]
    max_k = min(10, len(boxes))
    # ÃncercÄƒm k=2, 3, ... pÃ¢nÄƒ gÄƒsim zone bine separate (perechi cu IoU mic)
    for k in range(2, max_k + 1):
        if k > len(boxes):
            break
        km = KMeans(n_clusters=k, random_state=42, n_init=10).fit(centers)
        labels = km.labels_
        zone_boxes: list[list[int]] = []
        for c in range(k):
            indices = np.where(labels == c)[0]
            if len(indices) == 0:
                continue
            sub = [boxes[i] for i in indices]
            union_box = [
                min(b[0] for b in sub),
                min(b[1] for b in sub),
                max(b[2] for b in sub),
                max(b[3] for b in sub),
            ]
            zone_boxes.append(union_box)
        if len(zone_boxes) < 2:
            continue
        # VerificÄƒm cÄƒ zonele sunt bine separate: orice pereche are IoU < 0.2
        ok = True
        for i in range(len(zone_boxes)):
            for j in range(i + 1, len(zone_boxes)):
                if _iou_boxes(zone_boxes[i], zone_boxes[j]) >= 0.2:
                    ok = False
                    break
            if not ok:
                break
        if ok:
            return zone_boxes
    # DacÄƒ niciun k nu dÄƒ zone bine separate, returnÄƒm totuÈ™i 2 zone (k=2) ca fallback
    km = KMeans(n_clusters=2, random_state=42, n_init=10).fit(centers)
    labels = km.labels_
    zone_boxes = []
    for c in range(2):
        indices = np.where(labels == c)[0]
        sub = [boxes[i] for i in indices]
        zone_boxes.append([
            min(b[0] for b in sub),
            min(b[1] for b in sub),
            max(b[2] for b in sub),
            max(b[3] for b in sub),
        ])
    return zone_boxes if len(zone_boxes) >= 2 else None


def expand_cluster(mask: np.ndarray, x1: int, y1: int, x2: int, y2: int) -> list[int]:
    h, w = mask.shape
    while True:
        expanded = False
        if y1 > 0 and np.any(mask[y1 - 1, x1:x2] == 255):
            y1 -= 1
            expanded = True
        if y2 < h and np.any(mask[y2 - 1, x1:x2] == 255):
            y2 += 1
            expanded = True
        if x1 > 0 and np.any(mask[y1:y2, x1 - 1] == 255):
            x1 -= 1
            expanded = True
        if x2 < w and np.any(mask[y1:y2, x2 - 1] == 255):
            x2 += 1
            expanded = True
        if not expanded:
            break
    return [x1, y1, x2, y2]


def _bgr_to_base64_png(bgr_img: np.ndarray, max_size: int = 2000) -> str: 
    h, w = bgr_img.shape[:2]
    
    if max(h, w) > max_size:
        scale = max_size / max(h, w)
        new_w = max(1, int(w * scale))
        new_h = max(1, int(h * scale))
        bgr_img = cv2.resize(bgr_img, (new_w, new_h), interpolation=cv2.INTER_LANCZOS4)
    
    rgb_img = cv2.cvtColor(bgr_img, cv2.COLOR_BGR2RGB)
    pil_img = Image.fromarray(rgb_img)
    
    buf = io.BytesIO()
    pil_img.save(buf, format="JPEG", quality=95) 
    return base64.b64encode(buf.getvalue()).decode("ascii")


def _ask_ai_how_many_plans(orig_img: np.ndarray) -> int:
    openai_key = os.getenv("OPENAI_API_KEY")
    if not openai_key:
        print("  âš ï¸  [AI] OPENAI_API_KEY lipsÄƒ - returnez 1 (default)")
        return 1
    
    try:
        from openai import OpenAI
        client = OpenAI(api_key=openai_key)
    except Exception as e:
        print(f"  âš ï¸  [AI] Eroare iniÈ›ializare client OpenAI: {e}")
        return 1
    
    print("  ğŸ¤– [AI] Trimit imaginea pentru numÄƒrare planuri...")
    b64_img = _bgr_to_base64_png(orig_img)
    
    prompt = """You are an architectural floor plan analyst.
Look at this image and count how many DISTINCT house floor plans you can see.
IMPORTANT:
- If you see multiple floor plans arranged on the same page, count each one.
- If you see just ONE floor plan (even with borders), return 1.
- If you see a site plan, elevation view, or text-heavy page, return 1.
Return ONLY a number (1, 2, 3...). No text."""

    try:
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": prompt},
                        {
                            "type": "image_url",
                            "image_url": {"url": f"data:image/jpeg;base64,{b64_img}"}
                        }
                    ]
                }
            ],
            max_tokens=10,
            temperature=0.0
        )
        
        answer = response.choices[0].message.content.strip()
        print(f"  ğŸ’¬ [AI] RÄƒspuns brut: '{answer}'")
        
        import re
        match = re.search(r'\d+', answer)
        if match:
            count = int(match.group())
            print(f"  âœ… [AI] Detectat: {count} planuri.")
            return count
        else:
            print(f"  âš ï¸  [AI] Nu am gÄƒsit un numÄƒr Ã®n rÄƒspuns. Returnez 1.")
            return 1
            
    except Exception as e:
        print(f"  âš ï¸  [AI] Eroare la apelare API: {e}")
        return 1


def _detect_frame_geometric(orig_img: np.ndarray, largest_cluster_box: list[int]) -> bool:
    """
    âœ… FUNCÈšIE NOUÄ‚: Detectare geometricÄƒ a frame-ului fix de la marginile imaginii.
    VerificÄƒ dacÄƒ cluster-ul mare este un frame fix la marginile absolute ale imaginii.
    
    Args:
        orig_img: Imaginea originalÄƒ (BGR)
        largest_cluster_box: [x1, y1, x2, y2] pentru cel mai mare cluster
    
    Returns:
        True dacÄƒ detectÄƒm un frame fix la marginile imaginii, False altfel.
    """
    h, w = orig_img.shape[:2]
    x1, y1, x2, y2 = largest_cluster_box
    
    # Convertim la grayscale pentru analizÄƒ
    gray = cv2.cvtColor(orig_img, cv2.COLOR_BGR2GRAY) if len(orig_img.shape) == 3 else orig_img
    
    # Threshold pentru a obÈ›ine o imagine binarÄƒ
    _, binary = cv2.threshold(gray, 127, 255, cv2.THRESH_BINARY_INV)
    
    # Marginile absolute ale imaginii (primele/ultimele N pixeli)
    edge_threshold = max(10, min(w, h) // 100)  # 1% din dimensiunea minimÄƒ, minim 10 pixeli
    
    # Verificare 1: Cluster-ul este foarte aproape de marginile imaginii?
    margin_tolerance = max(5, min(w, h) // 200)  # 0.5% din dimensiunea minimÄƒ, minim 5 pixeli
    
    near_top = y1 <= margin_tolerance
    near_bottom = y2 >= (h - margin_tolerance)
    near_left = x1 <= margin_tolerance
    near_right = x2 >= (w - margin_tolerance)
    
    # Verificare 2: Cluster-ul ocupÄƒ aproape toatÄƒ imaginea?
    width_ratio = (x2 - x1) / w
    height_ratio = (y2 - y1) / h
    
    # Verificare 3: ExistÄƒ linii dense de pixeli la marginile absolute?
    # VerificÄƒm primele/ultimele edge_threshold pixeli de la fiecare margine
    top_edge = binary[0:edge_threshold, :]
    bottom_edge = binary[h-edge_threshold:h, :]
    left_edge = binary[:, 0:edge_threshold]
    right_edge = binary[:, w-edge_threshold:w]
    
    # CalculÄƒm densitatea de pixeli negri (contururi) la fiecare margine
    top_density = np.sum(top_edge == 0) / (edge_threshold * w)
    bottom_density = np.sum(bottom_edge == 0) / (edge_threshold * w)
    left_density = np.sum(left_edge == 0) / (edge_threshold * h)
    right_density = np.sum(right_edge == 0) / (edge_threshold * h)
    
    # DacÄƒ toate marginile au densitate mare (>30%), probabil e un frame
    edge_density_threshold = 0.30
    all_edges_dense = (
        top_density > edge_density_threshold and
        bottom_density > edge_density_threshold and
        left_density > edge_density_threshold and
        right_density > edge_density_threshold
    )
    
    # Verificare 4: Cluster-ul formeazÄƒ un dreptunghi aproape complet la marginile imaginii?
    is_near_all_edges = near_top and near_bottom and near_left and near_right
    is_almost_full_size = width_ratio > 0.95 and height_ratio > 0.95
    
    # Logging detaliat pentru debugging
    print(f"   ğŸ” [GEOMETRIC] VerificÄƒri:")
    print(f"      Cluster box: [{x1}, {y1}, {x2}, {y2}] din [{w}x{h}]")
    print(f"      Margin tolerance: {margin_tolerance}px")
    print(f"      Aproape de margini: top={near_top} (y1={y1}), bottom={near_bottom} (y2={y2}, h={h}), left={near_left} (x1={x1}), right={near_right} (x2={x2}, w={w})")
    print(f"      Dimensiuni: {width_ratio*100:.1f}% x {height_ratio*100:.1f}% din imagine")
    print(f"      DensitÄƒÈ›i margini: top={top_density:.2f}, bottom={bottom_density:.2f}, left={left_density:.2f}, right={right_density:.2f}")
    print(f"      CondiÈ›ii: is_near_all_edges={is_near_all_edges}, is_almost_full_size={is_almost_full_size}, all_edges_dense={all_edges_dense}")
    
    # DacÄƒ toate condiÈ›iile sunt Ã®ndeplinite, probabil e un frame
    if is_near_all_edges and is_almost_full_size:
        if all_edges_dense:
            print(f"   âœ… [GEOMETRIC] Detectat frame fix la marginile imaginii!")
            return True
        else:
            print(f"   âš ï¸  [GEOMETRIC] Cluster mare dar densitÄƒÈ›ile marginilor nu confirmÄƒ frame clar")
            return False
    
    # DacÄƒ cluster-ul ocupÄƒ aproape toatÄƒ imaginea dar nu este aproape de toate marginile,
    # sau dacÄƒ este aproape de toate marginile dar densitÄƒÈ›ile nu confirmÄƒ, totuÈ™i poate fi un frame
    if is_almost_full_size and (is_near_all_edges or (near_top and near_bottom) or (near_left and near_right)):
        # RelaxÄƒm condiÈ›iile: dacÄƒ cel puÈ›in 2 margini opuse au densitate mare, probabil e frame
        opposite_edges_dense = (
            (top_density > edge_density_threshold and bottom_density > edge_density_threshold) or
            (left_density > edge_density_threshold and right_density > edge_density_threshold)
        )
        
        if opposite_edges_dense:
            print(f"   âœ… [GEOMETRIC] Detectat frame (condiÈ›ii relaxate: margini opuse dense)")
            return True
    
    return False


def _ask_ai_if_has_frame(orig_img: np.ndarray) -> bool:
    """
    âœ… FUNCÈšIE NOUÄ‚: ÃntreabÄƒ AI-ul (Gemini sau ChatGPT) dacÄƒ imaginea conÈ›ine un frame/cadru Ã®n jurul planului.
    ÃncearcÄƒ mai Ã®ntÃ¢i Gemini (mai bun), apoi ChatGPT ca fallback.
    ReturneazÄƒ True dacÄƒ existÄƒ frame, False altfel.
    """
    # Prompt simplificat È™i mai neutru pentru a evita blocarea de safety
    prompt = """Analyze this architectural floor plan image. Check if there is a rectangular border line at the absolute edges of the image (top, bottom, left, right edges).

A border frame is a thin rectangular line that forms a complete outline around the entire image, touching or very close to all four edges.

Answer ONLY: YES or NO"""
    
    # ========== ÃNCERCARE 1: GEMINI (mai bun) ==========
    # CreÄƒm un model nou cu safety settings mai permisive direct aici
    try:
        import google.generativeai as genai
        load_dotenv()
        api_key = os.getenv("GEMINI_API_KEY")
        
        if api_key:
            genai.configure(api_key=api_key)
            
            # Safety settings foarte permisive pentru a evita blocarea
            safety_settings = [
                {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_NONE"},
                {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_NONE"},
                {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_NONE"},
                {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_NONE"}
            ]
            
            # CreÄƒm modelul direct cu safety settings (Gemini 3 Flash primary, fallback la 2.5/2.0/1.5)
            models_to_try = ['gemini-3-flash-preview', 'gemini-2.5-flash', 'gemini-2.0-flash', 'gemini-1.5-flash']
            gemini_model = None
            
            for model_name in models_to_try:
                try:
                    gemini_model = genai.GenerativeModel(model_name, safety_settings=safety_settings)
                    break
                except Exception:
                    continue
            
            if gemini_model:
                try:
                    # Convertim numpy array (BGR) la PIL Image (RGB)
                    rgb_img = cv2.cvtColor(orig_img, cv2.COLOR_BGR2RGB)
                    pil_img = Image.fromarray(rgb_img)
                    
                    # Resize dacÄƒ e prea mare (pentru a evita erori Gemini)
                    w, h = pil_img.size
                    max_size = 2000
                    if max(w, h) > max_size:
                        scale = max_size / max(w, h)
                        new_w = max(1, int(w * scale))
                        new_h = max(1, int(h * scale))
                        # Folosim LANCZOS (nu LANCZOS4 care nu existÄƒ)
                        try:
                            pil_img = pil_img.resize((new_w, new_h), Image.Resampling.LANCZOS)
                        except AttributeError:
                            # Fallback pentru versiuni mai vechi de PIL
                            pil_img = pil_img.resize((new_w, new_h), Image.LANCZOS)
                    
                    print("  ğŸ¤– [AI] Trimit imaginea cÄƒtre Gemini pentru verificare frame...")
                    
                    # ApelÄƒm cu safety_settings suplimentare Ã®n generate_content pentru siguranÈ›Äƒ
                    response = gemini_model.generate_content(
                        [prompt, pil_img],
                        generation_config={
                            "temperature": 0.0,
                            "max_output_tokens": 10,
                        },
                        safety_settings=safety_settings  # Re-iterÄƒm safety settings aici
                    )
                    
                    # VerificÄƒm finish_reason pentru a vedea dacÄƒ rÄƒspunsul a fost blocat
                    finish_reason = None
                    if hasattr(response, 'candidates') and response.candidates:
                        finish_reason = response.candidates[0].finish_reason
                        print(f"  ğŸ” [AI] Gemini finish reason: {finish_reason}")
                        
                        # finish_reason=2 Ã®nseamnÄƒ SAFETY (blocat din motive de siguranÈ›Äƒ)
                        if finish_reason == 2:
                            print(f"  âš ï¸  [AI] Gemini a blocat rÄƒspunsul (SAFETY). Ãncerc ChatGPT...")
                        else:
                            # ÃncercÄƒm sÄƒ obÈ›inem rÄƒspunsul de la Gemini
                            answer = None
                            
                            if response.parts:
                                try:
                                    answer = response.text.strip().upper()
                                except Exception as e:
                                    print(f"  âš ï¸  [AI] Eroare la accesare response.text: {e}")
                            
                            # Fallback: Ã®ncercÄƒm sÄƒ accesÄƒm direct text-ul
                            if not answer:
                                try:
                                    if hasattr(response, 'text') and response.text:
                                        answer = response.text.strip().upper()
                                except Exception as e:
                                    print(f"  âš ï¸  [AI] Eroare la fallback text access: {e}")
                            
                            if answer:
                                print(f"  ğŸ’¬ [AI] Gemini rÄƒspuns: '{answer}'")
                                
                                has_frame = "YES" in answer or "DA" in answer
                                print(f"  âœ… [AI] Gemini detectat frame: {has_frame}")
                                return has_frame
                    
                    # DacÄƒ ajungem aici, Gemini nu a dat rÄƒspuns valid
                    if finish_reason != 2:
                        print(f"  âš ï¸  [AI] Gemini nu a returnat rÄƒspuns valid. Ãncerc ChatGPT...")
                        
                except Exception as e:
                    import traceback
                    print(f"  âš ï¸  [AI] Eroare la apelare Gemini: {e}")
                    print(f"  âš ï¸  [AI] Traceback: {traceback.format_exc()}")
                    print(f"  âš ï¸  [AI] Ãncerc ChatGPT ca fallback...")
            else:
                print("  âš ï¸  [AI] Nu s-a putut crea model Gemini. Ãncerc ChatGPT...")
        else:
            print("  âš ï¸  [AI] GEMINI_API_KEY lipsÄƒ. Ãncerc ChatGPT...")
    except Exception as e:
        import traceback
        print(f"  âš ï¸  [AI] Eroare iniÈ›ializare Gemini: {e}")
        print(f"  âš ï¸  [AI] Traceback: {traceback.format_exc()}")
        print(f"  âš ï¸  [AI] Ãncerc ChatGPT ca fallback...")
    
    # ========== ÃNCERCARE 2: CHATGPT (fallback) ==========
    openai_key = os.getenv("OPENAI_API_KEY")
    if not openai_key:
        print("  âš ï¸  [AI] OPENAI_API_KEY lipsÄƒ - returnez False (nu eliminÄƒm cluster)")
        return False
    
    try:
        from openai import OpenAI
        client = OpenAI(api_key=openai_key)
    except Exception as e:
        print(f"  âš ï¸  [AI] Eroare iniÈ›ializare client OpenAI: {e}")
        return False
    
    try:
        print("  ğŸ¤– [AI] Trimit imaginea cÄƒtre ChatGPT pentru verificare frame...")
        
        b64_img = _bgr_to_base64_png(orig_img)
        
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": prompt},
                        {
                            "type": "image_url",
                            "image_url": {"url": f"data:image/jpeg;base64,{b64_img}"}
                        }
                    ]
                }
            ],
            max_tokens=10,
            temperature=0.0
        )
        
        if response.choices and response.choices[0].message.content:
            answer = response.choices[0].message.content.strip().upper()
            print(f"  ğŸ’¬ [AI] ChatGPT rÄƒspuns: '{answer}'")
            
            has_frame = "YES" in answer or "DA" in answer
            print(f"  âœ… [AI] ChatGPT detectat frame: {has_frame}")
            return has_frame
        else:
            print(f"  âš ï¸  [AI] ChatGPT nu a returnat rÄƒspuns valid. Returnez False (nu eliminÄƒm cluster).")
            return False
            
    except Exception as e:
        import traceback
        print(f"  âš ï¸  [AI] Eroare la apelare ChatGPT: {e}")
        print(f"  âš ï¸  [AI] Traceback: {traceback.format_exc()}")
        return False


def _detect_sub_clusters_in_image(crop_img: np.ndarray) -> list[list[int]]:
    """
    DetecteazÄƒ sub-clustere Ã®n imagine folosind morphological operations È™i analizÄƒ avansatÄƒ.
    ReturneazÄƒ o listÄƒ de box-uri [x1, y1, x2, y2] pentru fiecare zonÄƒ de interes detectatÄƒ.
    NU foloseÈ™te gap-uri pentru a nu tÄƒia conÈ›inutul.
    """
    # Convertim la grayscale È™i binarizÄƒm
    gray = cv2.cvtColor(crop_img, cv2.COLOR_BGR2GRAY) if len(crop_img.shape) == 3 else crop_img
    _, binary = cv2.threshold(gray, 127, 255, cv2.THRESH_BINARY_INV)
    
    h, w = binary.shape
    print(f"     ğŸ“ Dimensiuni imagine: {w}x{h}px")

    def _clip_box(box: list[int]) -> list[int]:
        x1, y1, x2, y2 = box
        x1 = int(max(0, min(w, x1)))
        y1 = int(max(0, min(h, y1)))
        x2 = int(max(0, min(w, x2)))
        y2 = int(max(0, min(h, y2)))
        if x2 < x1:
            x1, x2 = x2, x1
        if y2 < y1:
            y1, y2 = y2, y1
        return [x1, y1, x2, y2]

    def _box_area(box: list[int]) -> int:
        x1, y1, x2, y2 = box
        return max(0, x2 - x1) * max(0, y2 - y1)

    def _content_area_in_box(box: list[int]) -> int:
        x1, y1, x2, y2 = box
        if x2 <= x1 or y2 <= y1:
            return 0
        region = binary[y1:y2, x1:x2]
        return int(np.sum(region > 0))

    def _is_inside(inner: list[int], outer: list[int], margin: int = 0) -> bool:
        ix1, iy1, ix2, iy2 = inner
        ox1, oy1, ox2, oy2 = outer
        return (
            ix1 >= ox1 + margin and iy1 >= oy1 + margin and
            ix2 <= ox2 - margin and iy2 <= oy2 - margin
        )

    def _postprocess_nested_boxes(raw_boxes: list[list[int]], reason: str) -> list[list[int]]:
        """
        Heuristic:
        - If we got 2+ boxes and the largest is basically the whole image while another is nested inside it,
          then "subtract" the nested box from the big one by choosing the best remaining rectangle
          around the nested box (top/bottom/left/right).
        - If the top 2 boxes are approximate peers (similar area), do NOT subtract.
        """
        if not raw_boxes:
            return raw_boxes

        boxes = [_clip_box(b) for b in raw_boxes]
        boxes = [b for b in boxes if _box_area(b) > 0]
        if len(boxes) < 2:
            return boxes

        # Sort by area desc
        boxes.sort(key=_box_area, reverse=True)
        big = boxes[0]
        small = None

        big_area = _box_area(big)
        img_area = w * h
        big_ratio = big_area / float(img_area + 1e-9)

        print(f"     ğŸ” Postprocess({reason}): Analizez {len(boxes)} box-uri")
        print(f"        ğŸ“¦ Box mare: {big} (aria={big_area}px, ratio={big_ratio:.2f})")

        # Find a clearly nested smaller box
        # Strategy: if big covers most of the image (>80%) and we have a significantly smaller box,
        # treat it as nested even if it touches the edges
        for cand in boxes[1:]:
            cand_area = _box_area(cand)
            cand_ratio = cand_area / float(img_area + 1e-9)
            peer_ratio_check = max(cand_area, 1) / float(max(big_area, 1))
            
            print(f"        ğŸ“¦ Candidat: {cand} (aria={cand_area}px, ratio={cand_ratio:.2f}, small/big={peer_ratio_check:.2f})")
            
            # Check if candidate is significantly smaller (not a peer)
            if peer_ratio_check >= 0.55:
                print(f"        âš ï¸  Candidat este peer (small/big={peer_ratio_check:.2f} >= 0.55) â†’ skip")
                continue
            
            # If big covers most of the image, be more lenient with edge detection
            is_strictly_inside = _is_inside(cand, big, margin=2)
            is_loosely_inside = _is_inside(cand, big, margin=0)
            
            # Check if candidate overlaps significantly with big (at least 80% of small is inside big)
            cx1, cy1, cx2, cy2 = cand
            bx1, by1, bx2, by2 = big
            
            # Calculate intersection
            inter_x1 = max(cx1, bx1)
            inter_y1 = max(cy1, by1)
            inter_x2 = min(cx2, bx2)
            inter_y2 = min(cy2, by2)
            
            if inter_x2 > inter_x1 and inter_y2 > inter_y1:
                inter_area = (inter_x2 - inter_x1) * (inter_y2 - inter_y1)
                overlap_ratio = inter_area / float(cand_area + 1e-9)
                
                print(f"        ğŸ” Overlap: {overlap_ratio:.2f} (strict={is_strictly_inside}, loose={is_loosely_inside})")
                
                # If big covers most of image AND small overlaps significantly with big, treat as nested
                if big_ratio >= 0.80 and overlap_ratio >= 0.80:
                    small = cand
                    print(f"        âœ… Detectat ca inclus (big_ratio={big_ratio:.2f} >= 0.80, overlap={overlap_ratio:.2f} >= 0.80)")
                    break
                elif is_strictly_inside:
                    small = cand
                    print(f"        âœ… Detectat ca inclus (strict check cu margin=2)")
                    break
                elif is_loosely_inside:
                    small = cand
                    print(f"        âœ… Detectat ca inclus (loose check fÄƒrÄƒ margin)")
                    break
            else:
                print(f"        âŒ Nu se intersecteazÄƒ cu box-ul mare")

        if small is None:
            print(f"     â„¹ï¸  Postprocess({reason}): Nu am gÄƒsit box mic inclus Ã®n box-ul mare â†’ pÄƒstrez toate box-urile.")
            return boxes

        small_area = _box_area(small)
        # If the top 2 are approximate peers, keep them (no subtraction).
        peer_ratio = max(small_area, 1) / float(max(big_area, 1))
        if peer_ratio >= 0.55:
            print(f"     â„¹ï¸  Postprocess({reason}): 2 box-uri aproximative (small/big={peer_ratio:.2f}) â†’ NU scot din cluster.")
            return boxes

        # Only do subtraction when big is "almost whole image" (or very dominant).
        if big_ratio < 0.80:
            print(f"     â„¹ï¸  Postprocess({reason}): big_ratio={big_ratio:.2f} (<0.80) â†’ NU scot din cluster.")
            return boxes

        print(
            f"     ğŸ§© Postprocess({reason}): detectat box mare + box mic inclus â†’ scot box-ul mic din box-ul mare\n"
            f"        - big={big} (ratio={big_ratio:.2f})\n"
            f"        - small={small} (small/big={peer_ratio:.2f})"
        )

        bx1, by1, bx2, by2 = big
        sx1, sy1, sx2, sy2 = small

        pad = max(10, min(w, h) // 80)  # small pad to avoid cutting near the boundary

        candidates = []
        # Top region
        top_box = _clip_box([bx1, by1, bx2, sy1 - pad])
        candidates.append(("top", top_box))
        # Bottom region
        bottom_box = _clip_box([bx1, sy2 + pad, bx2, by2])
        candidates.append(("bottom", bottom_box))
        # Left region
        left_box = _clip_box([bx1, by1, sx1 - pad, by2])
        candidates.append(("left", left_box))
        # Right region
        right_box = _clip_box([sx2 + pad, by1, bx2, by2])
        candidates.append(("right", right_box))

        print(f"        ğŸ” Generez 4 candidaÈ›i pentru 'big minus small':")
        for name, c in candidates:
            a = _box_area(c)
            print(f"          - {name}: {c} (aria={a}px)")

        # Score candidates by (content_area, area) and keep the best one(s)
        scored = []
        min_keep_area = max(5000, img_area // 200)  # 0.5% of image or 5k px
        print(f"        ğŸ“Š Min area pentru candidat valid: {min_keep_area}px")
        
        for name, c in candidates:
            a = _box_area(c)
            if a < min_keep_area:
                print(f"          âŒ {name}: aria={a}px < {min_keep_area}px â†’ REJECTED")
                continue
            ca = _content_area_in_box(c)
            if ca < min_keep_area:
                print(f"          âŒ {name}: content_area={ca}px < {min_keep_area}px â†’ REJECTED")
                continue
            scored.append((ca, a, name, c))
            print(f"          âœ… {name}: content_area={ca}px, aria={a}px â†’ ACCEPTED")

        scored.sort(key=lambda t: (t[0], t[1]), reverse=True)

        if not scored:
            print(f"     âš ï¸  Postprocess({reason}): nu am gÄƒsit o zonÄƒ validÄƒ dupÄƒ scÄƒdere â†’ pÄƒstrez box-urile originale.")
            return boxes

        best_name = scored[0][2]
        best_big_minus_small = scored[0][3]
        print(
            f"        âœ… big_minus_small ales ({best_name}): {best_big_minus_small} "
            f"(content={scored[0][0]}px, area={scored[0][1]}px)"
        )

        # Return: small (explicit) + big-minus-small (replacing big).
        # Also keep any other boxes that are neither big nor small, to allow overlaps if present.
        # IMPORTANT: Exclude 'big' explicitly since we're replacing it with 'best_big_minus_small'
        others = [b for b in boxes[1:] if b != small and b != big and not _is_inside(b, small, margin=0)]
        out = [small, best_big_minus_small] + others
        
        print(f"        ğŸ“¤ Return: {len(out)} box-uri (small={small}, big_minus_small={best_big_minus_small}, others={len(others)})")
        
        # Dedup (stable)
        dedup = []
        for b in out:
            if b not in dedup:
                dedup.append(b)
        
        print(f"        âœ… Final: {len(dedup)} box-uri unice returnate")
        return dedup
    
    # Strategie 1: Folosim morphological operations pentru a separa zonele conectate
    # AplicÄƒm eroziune pentru a separa zonele care sunt conectate doar prin linii subÈ›iri
    print(f"     ğŸ” Strategie 1: Separare zone folosind morphological operations...")
    kernel_size = max(15, min(w, h) // 30)  # Kernel adaptiv pentru separare
    if kernel_size % 2 == 0:
        kernel_size += 1
    print(f"        âš™ï¸  Kernel size: {kernel_size}x{kernel_size}px")
    
    # MORPH_OPEN pentru a separa zonele conectate prin linii subÈ›iri
    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (kernel_size, kernel_size))
    opened = cv2.morphologyEx(binary, cv2.MORPH_OPEN, kernel, iterations=2)
    
    # MORPH_CLOSE pentru a umple gÄƒurile mici Ã®n fiecare zonÄƒ
    closed = cv2.morphologyEx(opened, cv2.MORPH_CLOSE, kernel, iterations=2)
    
    # Connected components pe imaginea procesatÄƒ
    num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(closed, connectivity=8)
    print(f"        ğŸ”— GÄƒsite {num_labels-1} componente conectate")
    
    components = []
    min_area = max(5000, (w * h) // 100)  # Minim 1% din imagine pentru a fi considerat zonÄƒ de interes
    print(f"        ğŸ“Š Min area pentru zonÄƒ de interes: {min_area}px")
    
    for i in range(1, num_labels):
        x, y, w_comp, h_comp, area = stats[i]
        if area > min_area:
            # Extindem bounding box-ul pentru a include conÈ›inutul complet din zona originalÄƒ
            margin = max(20, min(w, h) // 30)
            x_expanded = max(0, x - margin)
            y_expanded = max(0, y - margin)
            x2_expanded = min(w, x + w_comp + margin)
            y2_expanded = min(h, y + h_comp + margin)
            
            # VerificÄƒm dacÄƒ existÄƒ conÈ›inut Ã®n zona extinsÄƒ din imaginea originalÄƒ
            expanded_region = binary[y_expanded:y2_expanded, x_expanded:x2_expanded]
            content_area = np.sum(expanded_region > 0)
            if content_area > min_area:
                components.append((i, area, content_area, x_expanded, y_expanded, x2_expanded, y2_expanded))
                print(f"        âœ… ComponentÄƒ {i}: bbox=[{x_expanded}, {y_expanded}, {x2_expanded}, {y2_expanded}], comp_area={area}px, content_area={content_area}px")
    
    components.sort(key=lambda x: x[2], reverse=True)  # SortÄƒm dupÄƒ content_area
    
    boxes = []
    padding = max(10, min(w, h) // 50)
    
    # LuÄƒm primele 2-3 componente mari
    for i, (label_idx, comp_area, content_area, x1, y1, x2, y2) in enumerate(components[:3]):
        box = [
            max(0, x1 - padding),
            max(0, y1 - padding),
            min(w, x2 + padding),
            min(h, y2 + padding)
        ]
        boxes.append(box)
        print(f"        ğŸ“¦ Zone {i+1}: bbox={box}, dimensiuni={box[2]-box[0]}x{box[3]-box[1]}px, content_area={content_area}px")
    
    if len(boxes) >= 2:
        print(f"     âœ… Strategie 1 SUCCES: Detectat {len(boxes)} zone de interes folosind morphological separation.")
        return boxes
    else:
        print(f"     âš ï¸  Strategie 1 FAILED: GÄƒsite doar {len(boxes)} zone (necesare minim 2)")
    
    # Strategie 2: Folosim Watershed algorithm pentru separare avansatÄƒ
    print(f"     ğŸ” Strategie 2: Watershed algorithm pentru separare avansatÄƒ...")
    try:
        # Distance transform pentru a gÄƒsi centroidele zonelor
        dist_transform = cv2.distanceTransform(binary, cv2.DIST_L2, 5)
        
        # GÄƒsim maximurile locale (centroidele zonelor)
        _, sure_fg = cv2.threshold(dist_transform, 0.7 * dist_transform.max(), 255, 0)
        sure_fg = np.uint8(sure_fg)
        
        # Marker-based watershed
        _, markers = cv2.connectedComponents(sure_fg)
        markers = markers + 1
        markers[binary == 0] = 0
        
        # AplicÄƒm watershed
        markers = cv2.watershed(cv2.cvtColor(binary, cv2.COLOR_GRAY2BGR), markers)
        
        # Extragem zonele unice (excludem marker-ul -1 care reprezintÄƒ graniÈ›ele)
        unique_markers = np.unique(markers)
        unique_markers = unique_markers[unique_markers > 1]  # Excludem background (0) È™i graniÈ›e (-1)
        print(f"        ğŸ”— GÄƒsite {len(unique_markers)} zone distincte cu watershed")
        
        boxes = []
        padding = max(10, min(w, h) // 50)
        min_area = max(5000, (w * h) // 100)
        
        for marker_id in unique_markers:
            mask = (markers == marker_id).astype(np.uint8) * 255
            coords = np.where(mask > 0)
            if coords[0].size > min_area:
                y_min, y_max = coords[0].min(), coords[0].max()
                x_min, x_max = coords[1].min(), coords[1].max()
                area = coords[0].size
                box = [
                    max(0, x_min - padding),
                    max(0, y_min - padding),
                    min(w, x_max + padding),
                    min(h, y_max + padding)
                ]
                boxes.append((area, box))
                print(f"        ğŸ“¦ Zone watershed {marker_id}: bbox={box}, dimensiuni={box[2]-box[0]}x{box[3]-box[1]}px, area={area}px")
        
        # SortÄƒm dupÄƒ arie È™i luÄƒm primele 2-3
        boxes.sort(key=lambda x: x[0], reverse=True)
        boxes = [box for _, box in boxes[:3]]
        
        if len(boxes) >= 2:
            print(f"     âœ… Strategie 2 SUCCES: Detectat {len(boxes)} zone de interes folosind watershed.")
            return boxes
        else:
            print(f"     âš ï¸  Strategie 2 FAILED: GÄƒsite doar {len(boxes)} zone (necesare minim 2)")
    except Exception as e:
        print(f"     âš ï¸  Strategie 2 FAILED: Eroare la watershed - {e}")
    
    # Strategie 3 (Fallback): Folosim contururi pentru a gÄƒsi zonele de conÈ›inut
    print(f"     ğŸ” Strategie 3: Folosesc contururi pentru a gÄƒsi zonele de conÈ›inut...")
    contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    print(f"        ğŸ”— GÄƒsite {len(contours)} contururi")
    
    # SortÄƒm contururile dupÄƒ arie
    contours = sorted(contours, key=cv2.contourArea, reverse=True)
    
    boxes = []
    padding = max(10, min(w, h) // 50)
    min_area = max(5000, (w * h) // 100)  # Minim 1% din imagine
    print(f"        ğŸ“Š Min area pentru contur: {min_area}px")
    
    # LuÄƒm primele 2-3 contururi mari
    for i, contour in enumerate(contours[:5]):  # VerificÄƒm primele 5 pentru a avea opÈ›iuni
        area = cv2.contourArea(contour)
        if area > min_area:
            x, y, w_cont, h_cont = cv2.boundingRect(contour)
            box = [
                max(0, x - padding),
                max(0, y - padding),
                min(w, x + w_cont + padding),
                min(h, y + h_cont + padding)
            ]
            boxes.append((area, box))
            print(f"        ğŸ“¦ Contur {i+1}: bbox={box}, dimensiuni={box[2]-box[0]}x{box[3]-box[1]}px, aria={area}px")
    
    # SortÄƒm dupÄƒ arie È™i luÄƒm primele 2-3
    boxes.sort(key=lambda x: x[0], reverse=True)
    boxes = [box for _, box in boxes[:3]]
    
    if len(boxes) >= 2:
        print(f"     âœ… Strategie 3 SUCCES: Detectat {len(boxes)} zone de interes folosind contururi.")
        return boxes
    else:
        print(f"     âš ï¸  Strategie 3 FAILED: GÄƒsite doar {len(boxes)} zone (necesare minim 2)")
    
    # Strategie 4 (Ultim fallback): folosim analiza densitÄƒÈ›ii pentru a gÄƒsi zonele de conÈ›inut maxim
    # NU folosim gap-uri, ci doar identificÄƒm zonele cu conÈ›inut maxim pe fiecare jumÄƒtate
    print(f"     ğŸ” Strategie 4: AnalizÄƒ densitate pentru zone de conÈ›inut maxim (fÄƒrÄƒ gap-uri)...")
    
    # AnalizÄƒm densitatea pe axe
    col_density = np.sum(binary > 0, axis=0)
    row_density = np.sum(binary > 0, axis=1)
    
    boxes = []
    padding = max(10, min(w, h) // 50)
    
    # GÄƒsim zonele cu densitate maximÄƒ pe fiecare jumÄƒtate a imaginii
    if w > h:
        # Vertical: analizÄƒm stÃ¢nga È™i dreapta
        print(f"        ğŸ“ Imagine latÄƒ ({w}x{h}), analizez stÃ¢nga È™i dreapta")
        left_half = col_density[:w//2]
        right_half = col_density[w//2:]
        
        # GÄƒsim zonele cu conÈ›inut Ã®n fiecare jumÄƒtate (nu folosim gap-uri, ci doar conÈ›inutul)
        left_max = np.max(left_half) if len(left_half) > 0 else 0
        right_max = np.max(right_half) if len(right_half) > 0 else 0
        threshold = 0.3  # 30% din maxim pentru a identifica zonele cu conÈ›inut
        
        left_content = np.where(left_half > left_max * threshold)[0] if left_max > 0 else []
        right_content = np.where(right_half > right_max * threshold)[0] if right_max > 0 else []
        
        print(f"        ğŸ“Š StÃ¢nga: max_density={left_max}, content_zones={len(left_content)}")
        print(f"        ğŸ“Š Dreapta: max_density={right_max}, content_zones={len(right_content)}")
        
        if len(left_content) > 0 and len(right_content) > 0:
            # Zona stÃ¢nga - luÄƒm Ã®ntreaga zonÄƒ cu conÈ›inut
            left_band = binary[:, :w//2]
            left_coords = np.where(left_band > 0)
            if left_coords[0].size > 0:
                y_min, y_max = left_coords[0].min(), left_coords[0].max()
                x_min, x_max = left_coords[1].min(), left_coords[1].max()
                box = [
                    max(0, x_min - padding),
                    max(0, y_min - padding),
                    min(w, x_max + padding),
                    min(h, y_max + padding)
                ]
                boxes.append((left_coords[0].size, box))
                print(f"        ğŸ“¦ Zone stÃ¢nga: bbox={box}, dimensiuni={box[2]-box[0]}x{box[3]-box[1]}px, area={left_coords[0].size}px")
            
            # Zona dreapta - luÄƒm Ã®ntreaga zonÄƒ cu conÈ›inut
            right_band = binary[:, w//2:]
            right_coords = np.where(right_band > 0)
            if right_coords[0].size > 0:
                y_min, y_max = right_coords[0].min(), right_coords[0].max()
                x_min, x_max = right_coords[1].min(), right_coords[1].max()
                box = [
                    max(0, w//2 + x_min - padding),
                    max(0, y_min - padding),
                    min(w, w//2 + x_max + padding),
                    min(h, y_max + padding)
                ]
                boxes.append((right_coords[0].size, box))
                print(f"        ğŸ“¦ Zone dreapta: bbox={box}, dimensiuni={box[2]-box[0]}x{box[3]-box[1]}px, area={right_coords[0].size}px")
    else:
        # Orizontal: analizÄƒm sus È™i jos
        print(f"        ğŸ“ Imagine Ã®naltÄƒ ({w}x{h}), analizez sus È™i jos")
        top_half = row_density[:h//2]
        bottom_half = row_density[h//2:]
        
        top_max = np.max(top_half) if len(top_half) > 0 else 0
        bottom_max = np.max(bottom_half) if len(bottom_half) > 0 else 0
        threshold = 0.3
        
        top_content = np.where(top_half > top_max * threshold)[0] if top_max > 0 else []
        bottom_content = np.where(bottom_half > bottom_max * threshold)[0] if bottom_max > 0 else []
        
        print(f"        ğŸ“Š Sus: max_density={top_max}, content_zones={len(top_content)}")
        print(f"        ğŸ“Š Jos: max_density={bottom_max}, content_zones={len(bottom_content)}")
        
        if len(top_content) > 0 and len(bottom_content) > 0:
            # Zona de sus - luÄƒm Ã®ntreaga zonÄƒ cu conÈ›inut
            top_band = binary[:h//2, :]
            top_coords = np.where(top_band > 0)
            if top_coords[0].size > 0:
                y_min, y_max = top_coords[0].min(), top_coords[0].max()
                x_min, x_max = top_coords[1].min(), top_coords[1].max()
                box = [
                    max(0, x_min - padding),
                    max(0, y_min - padding),
                    min(w, x_max + padding),
                    min(h, y_max + padding)
                ]
                boxes.append((top_coords[0].size, box))
                print(f"        ğŸ“¦ Zone sus: bbox={box}, dimensiuni={box[2]-box[0]}x{box[3]-box[1]}px, area={top_coords[0].size}px")
            
            # Zona de jos - luÄƒm Ã®ntreaga zonÄƒ cu conÈ›inut
            bottom_band = binary[h//2:, :]
            bottom_coords = np.where(bottom_band > 0)
            if bottom_coords[0].size > 0:
                y_min, y_max = bottom_coords[0].min(), bottom_coords[0].max()
                x_min, x_max = bottom_coords[1].min(), bottom_coords[1].max()
                box = [
                    max(0, x_min - padding),
                    max(0, h//2 + y_min - padding),
                    min(w, x_max + padding),
                    min(h, h//2 + y_max + padding)
                ]
                boxes.append((bottom_coords[0].size, box))
                print(f"        ğŸ“¦ Zone jos: bbox={box}, dimensiuni={box[2]-box[0]}x{box[3]-box[1]}px, area={bottom_coords[0].size}px")
    
    # SortÄƒm dupÄƒ arie
    boxes.sort(key=lambda x: x[0], reverse=True)
    boxes = [box for _, box in boxes]
    
    if len(boxes) >= 2:
        print(f"     âœ… Strategie 4 SUCCES: Detectat {len(boxes)} zone de interes folosind analiza densitÄƒÈ›ii pe jumÄƒtÄƒÈ›i.")
        return boxes
    else:
        print(f"     âš ï¸  Strategie 4 FAILED: GÄƒsite doar {len(boxes)} zone (necesare minim 2)")
    
    # DacÄƒ tot nu gÄƒsim, returnÄƒm Ã®ntreaga imagine ca o singurÄƒ zonÄƒ
    print(f"     âš ï¸ Nu am putut detecta zone separate. Returnez Ã®ntreaga imagine.")
    return [[0, 0, w, h]]


def _check_blueprint_and_sideview_in_cluster(crop_img: np.ndarray) -> bool:
    """
    VerificÄƒ dacÄƒ cluster-ul conÈ›ine atÃ¢t un blueprint cÃ¢t È™i un sideview.
    FoloseÈ™te acelaÈ™i sistem robust de clasificare ca pentru clustere, dar cu prompt explicit
    care verificÄƒ dacÄƒ existÄƒ ambele tipuri Ã®n imagine.
    """
    try:
        from .classifier import prep_for_vlm, pil_to_base64, parse_label, setup_gemini_client
        import os
        from openai import OpenAI
        from PIL import Image
        import tempfile
        
        # Setup clients pentru clasificare
        gpt_client = None
        gemini_client = None
        
        openai_key = os.getenv("OPENAI_API_KEY")
        if openai_key:
            try:
                gpt_client = OpenAI(api_key=openai_key)
            except:
                pass
        
        gemini_client = setup_gemini_client()
        
        if not gpt_client and not gemini_client:
            print("     âš ï¸ Nu pot clasifica - lipsesc API keys. Presupun cÄƒ nu are blueprint + sideview.")
            return False
        
        # SalvÄƒm temporar crop-ul pentru clasificare
        with tempfile.NamedTemporaryFile(suffix='.jpg', delete=False) as tmp_file:
            tmp_path = Path(tmp_file.name)
            cv2.imwrite(str(tmp_path), crop_img)
        
        try:
            # Prompt explicit care Ã®ntreabÄƒ dacÄƒ existÄƒ ambele tipuri
            PROMPT_MIXED = """You are an EXPERT architectural drawing classifier.

This image may contain MULTIPLE types of architectural drawings side by side.

Analyze the image carefully and determine if it contains BOTH:
1. A FLOOR PLAN (house_blueprint or site_blueprint) - showing rooms, walls, or property layout from above
2. A SIDE VIEW/ELEVATION (side_view) - showing the building facade from the side

If the image contains BOTH types, respond with: "mixed_blueprint_sideview"
If it contains ONLY a floor plan, respond with: "blueprint_only"
If it contains ONLY a side view, respond with: "sideview_only"
If it contains NEITHER (or is text_area), respond with: "neither"

Return ONLY one of these labels: mixed_blueprint_sideview | blueprint_only | sideview_only | neither"""
            
            # ÃncercÄƒm mai Ã®ntÃ¢i cu GPT
            has_both = False
            if gpt_client:
                try:
                    pil_img = prep_for_vlm(tmp_path)
                    b64 = pil_to_base64(pil_img)
                    
                    response = gpt_client.chat.completions.create(
                        model="gpt-4o",
                        messages=[
                            {
                                "role": "user",
                                "content": [
                                    {"type": "text", "text": PROMPT_MIXED},
                                    {
                                        "type": "image_url",
                                        "image_url": {"url": f"data:image/jpeg;base64,{b64}"}
                                    }
                                ]
                            }
                        ],
                        temperature=0.0,
                        max_tokens=50
                    )
                    
                    result = response.choices[0].message.content.strip().lower()
                    print(f"     ğŸ“‹ GPT mixed classification: {result}")
                    
                    if "mixed_blueprint_sideview" in result:
                        has_both = True
                        print(f"     âœ… GPT detectat blueprint + sideview Ã®n acelaÈ™i cluster!")
                        return True
                except Exception as e:
                    print(f"     âš ï¸ GPT mixed classification failed: {e}")
            
            # DacÄƒ GPT nu a detectat, Ã®ncercÄƒm cu Gemini
            if not has_both and gemini_client:
                try:
                    pil_img = prep_for_vlm(tmp_path)
                    
                    response = gemini_client.generate_content(
                        [PROMPT_MIXED, pil_img],
                        generation_config={
                            "temperature": 0.0,
                            "max_output_tokens": 50,
                        }
                    )
                    
                    if response.parts:
                        result = response.text.strip().lower()
                        print(f"     ğŸ“‹ Gemini mixed classification: {result}")
                        
                        if "mixed_blueprint_sideview" in result:
                            has_both = True
                            print(f"     âœ… Gemini detectat blueprint + sideview Ã®n acelaÈ™i cluster!")
                            return True
                except Exception as e:
                    print(f"     âš ï¸ Gemini mixed classification failed: {e}")
            
            # DacÄƒ niciunul nu a detectat mixed, folosim metoda de detectare cluster:
            # analizÄƒm densitatea pentru a gÄƒsi gap-uri È™i sÄƒ Ã®mpÄƒrÈ›im inteligent
            if not has_both:
                from .classifier import classify_image_robust
                
                # DetectÄƒm clustere Ã®n imagine folosind analiza densitÄƒÈ›ii
                sub_clusters = _detect_sub_clusters_in_image(crop_img)
                
                if len(sub_clusters) >= 2:
                    print(f"     ğŸ” Detectat {len(sub_clusters)} sub-clustere Ã®n imagine")
                    
                    # ClasificÄƒm fiecare sub-cluster
                    tmp_paths = []
                    try:
                        for idx, (x1, y1, x2, y2) in enumerate(sub_clusters):
                            # Crop la sub-cluster
                            sub_crop = crop_img[y1:y2, x1:x2]
                            
                            # SalvÄƒm temporar
                            with tempfile.NamedTemporaryFile(suffix=f'_sub{idx}.jpg', delete=False) as tmp_sub:
                                tmp_sub_path = Path(tmp_sub.name)
                                cv2.imwrite(str(tmp_sub_path), sub_crop)
                                tmp_paths.append(tmp_sub_path)
                            
                            # ClasificÄƒm sub-cluster-ul
                            result_sub = classify_image_robust(tmp_sub_path, gpt_client, gemini_client)
                            label_sub = result_sub.label if result_sub else None
                            
                            print(f"     ğŸ“‹ Sub-cluster {idx+1} ({x1},{y1}-{x2},{y2}): {label_sub}")
                            
                            # VerificÄƒm dacÄƒ avem blueprint È™i sideview Ã®n sub-clustere diferite
                            if label_sub in ("house_blueprint", "site_blueprint"):
                                # CÄƒutÄƒm sideview Ã®n celelalte sub-clustere
                                for idx2, (x1_2, y1_2, x2_2, y2_2) in enumerate(sub_clusters):
                                    if idx2 == idx:
                                        continue
                                    
                                    with tempfile.NamedTemporaryFile(suffix=f'_sub{idx2}.jpg', delete=False) as tmp_sub2:
                                        tmp_sub2_path = Path(tmp_sub2.name)
                                        sub_crop2 = crop_img[y1_2:y2_2, x1_2:x2_2]
                                        cv2.imwrite(str(tmp_sub2_path), sub_crop2)
                                    
                                    result_sub2 = classify_image_robust(tmp_sub2_path, gpt_client, gemini_client)
                                    label_sub2 = result_sub2.label if result_sub2 else None
                                    
                                    if label_sub2 == "side_view":
                                        print(f"     âœ… Detectat blueprint + sideview Ã®n sub-clustere separate!")
                                        # È˜tergem fiÈ™ierele temporare
                                        for tp in tmp_paths:
                                            try:
                                                tp.unlink()
                                            except:
                                                pass
                                        try:
                                            tmp_sub2_path.unlink()
                                        except:
                                            pass
                                        return True
                                    
                                    try:
                                        tmp_sub2_path.unlink()
                                    except:
                                        pass
                            
                            elif label_sub == "side_view":
                                # CÄƒutÄƒm blueprint Ã®n celelalte sub-clustere
                                for idx2, (x1_2, y1_2, x2_2, y2_2) in enumerate(sub_clusters):
                                    if idx2 == idx:
                                        continue
                                    
                                    with tempfile.NamedTemporaryFile(suffix=f'_sub{idx2}.jpg', delete=False) as tmp_sub2:
                                        tmp_sub2_path = Path(tmp_sub2.name)
                                        sub_crop2 = crop_img[y1_2:y2_2, x1_2:x2_2]
                                        cv2.imwrite(str(tmp_sub2_path), sub_crop2)
                                    
                                    result_sub2 = classify_image_robust(tmp_sub2_path, gpt_client, gemini_client)
                                    label_sub2 = result_sub2.label if result_sub2 else None
                                    
                                    if label_sub2 in ("house_blueprint", "site_blueprint"):
                                        print(f"     âœ… Detectat blueprint + sideview Ã®n sub-clustere separate!")
                                        # È˜tergem fiÈ™ierele temporare
                                        for tp in tmp_paths:
                                            try:
                                                tp.unlink()
                                            except:
                                                pass
                                        try:
                                            tmp_sub2_path.unlink()
                                        except:
                                            pass
                                        return True
                                    
                                    try:
                                        tmp_sub2_path.unlink()
                                    except:
                                        pass
                        
                        print(f"     â„¹ï¸ Cluster nu conÈ›ine blueprint + sideview Ã®n sub-clustere separate.")
                        
                    finally:
                        # È˜tergem fiÈ™ierele temporare
                        for tp in tmp_paths:
                            try:
                                tp.unlink()
                            except:
                                pass
                else:
                    print(f"     âš ï¸ Nu am gÄƒsit suficiente sub-clustere ({len(sub_clusters)} < 2). Folosesc metoda simplÄƒ de split.")
                    
                    # Fallback la metoda simplÄƒ dacÄƒ nu gÄƒsim clustere
                    h, w = crop_img.shape[:2]
                    
                    # ÃmpÄƒrÈ›im cluster-ul Ã®n 2 pÄƒrÈ›i (vertical sau orizontal)
                    if w > h:
                        # Cluster lat - verificÄƒm stÃ¢nga È™i dreapta
                        left_crop = crop_img[:, :w//2]
                        right_crop = crop_img[:, w//2:]
                    else:
                        # Cluster Ã®nalt - verificÄƒm sus È™i jos
                        left_crop = crop_img[:h//2, :]
                        right_crop = crop_img[h//2:, :]
                    
                    # SalvÄƒm temporar ambele pÄƒrÈ›i
                    with tempfile.NamedTemporaryFile(suffix='_left.jpg', delete=False) as tmp_left:
                        tmp_left_path = Path(tmp_left.name)
                        cv2.imwrite(str(tmp_left_path), left_crop)
                    
                    with tempfile.NamedTemporaryFile(suffix='_right.jpg', delete=False) as tmp_right:
                        tmp_right_path = Path(tmp_right.name)
                        cv2.imwrite(str(tmp_right_path), right_crop)
                    
                    try:
                        # ClasificÄƒm ambele pÄƒrÈ›i folosind sistemul robust
                        result_left = classify_image_robust(tmp_left_path, gpt_client, gemini_client)
                        result_right = classify_image_robust(tmp_right_path, gpt_client, gemini_client)
                        
                        label_left = result_left.label if result_left else None
                        label_right = result_right.label if result_right else None
                        
                        print(f"     ğŸ“‹ Partea stÃ¢nga/sus: {label_left}")
                        print(f"     ğŸ“‹ Partea dreapta/jos: {label_right}")
                        
                        # VerificÄƒm dacÄƒ una este blueprint È™i cealaltÄƒ este sideview
                        left_is_blueprint = label_left in ("house_blueprint", "site_blueprint")
                        left_is_sideview = label_left == "side_view"
                        right_is_blueprint = label_right in ("house_blueprint", "site_blueprint")
                        right_is_sideview = label_right == "side_view"
                        
                        has_both = (left_is_blueprint and right_is_sideview) or (left_is_sideview and right_is_blueprint)
                        
                        if has_both:
                            print(f"     âœ… Detectat blueprint + sideview Ã®n acelaÈ™i cluster (metodÄƒ simplÄƒ split)!")
                            return True
                        else:
                            print(f"     â„¹ï¸ Cluster nu conÈ›ine blueprint + sideview Ã®mpreunÄƒ.")
                            return False
                            
                    finally:
                        # È˜tergem fiÈ™ierele temporare
                        try:
                            tmp_left_path.unlink()
                            tmp_right_path.unlink()
                        except:
                            pass
            
            return False
                    
        finally:
            # È˜tergem fiÈ™ierul temporar
            try:
                tmp_path.unlink()
            except:
                pass
                
    except Exception as e:
        import traceback
        print(f"     âš ï¸ Eroare la verificare blueprint + sideview: {e}")
        print(f"     âš ï¸ Traceback: {traceback.format_exc()}")
        return False


def _ask_ai_how_many_buildings_in_cluster(crop_img: np.ndarray) -> int:
    """
    ÃntreabÄƒ AI-ul cÃ¢te clÄƒdiri separate existÄƒ Ã®ntr-un cluster (side-by-side).
    """
    openai_key = os.getenv("OPENAI_API_KEY")
    if not openai_key:
        print("     [AI] OPENAI_API_KEY lipsÄƒ - returnez 1 (default)")
        return 1
    
    try:
        from openai import OpenAI
        client = OpenAI(api_key=openai_key)
    except Exception as e:
        print(f"     [AI] Eroare iniÈ›ializare: {e}")
        return 1
    
    b64_img = _bgr_to_base64_png(crop_img)
    
    prompt = """You are an architectural analyst.
Look at this floor plan cluster image.
Count how many SEPARATE, DISTINCT BUILDINGS or HOUSE PLANS you can see (side by side).
IMPORTANT:
- If you see multiple buildings/houses side by side, count each one.
- If you see one building with multiple floors stacked vertically, that's still ONE building.
- If you see just one house/building, return 1.
Return ONLY a number (1, 2, 3...). No text."""

    try:
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": prompt},
                        {
                            "type": "image_url",
                            "image_url": {"url": f"data:image/jpeg;base64,{b64_img}"}
                        }
                    ]
                }
            ],
            max_tokens=10,
            temperature=0.0
        )
        
        answer = response.choices[0].message.content.strip()
        
        import re
        match = re.search(r'\d+', answer)
        if match:
            count = int(match.group())
            print(f"     [AI] Detectat {count} clÄƒdiri Ã®n cluster.")
            return count
        else:
            return 1
            
    except Exception as e:
        print(f"     [AI] Eroare: {e}")
        return 1


def _ask_ai_how_many_floors_in_cluster(crop_img: np.ndarray) -> int:
    """
    ÃntreabÄƒ AI-ul cÃ¢te PLANURI DE ETAJ (etaje) separate sunt Ã®n cluster.
    Ex.: parter + etaj 1 stacked vertical = 2. Folosit pentru a Ã®mpÄƒrÈ›i clustere cu mai multe etaje.
    """
    openai_key = os.getenv("OPENAI_API_KEY")
    if not openai_key:
        return 1
    
    try:
        from openai import OpenAI
        client = OpenAI(api_key=openai_key)
    except Exception:
        return 1
    
    b64_img = _bgr_to_base64_png(crop_img)
    
    prompt = """You are an architectural analyst.
Look at this image. It may contain one or more FLOOR PLAN drawings (planuri de etaj).
Count how many SEPARATE FLOOR PLAN drawings you see.
IMPORTANT:
- If you see ground floor plan and first floor plan stacked vertically (or one above the other), count 2.
- If you see 3 floor plans stacked (e.g. parter, etaj 1, etaj 2), return 3.
- If you see only ONE floor plan drawing, return 1.
- Count each distinct floor plan as one, even if they belong to the same building.
Return ONLY a number (1, 2, 3...). No text."""

    try:
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": prompt},
                        {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{b64_img}"}}
                    ]
                }
            ],
            max_tokens=10,
            temperature=0.0
        )
        answer = response.choices[0].message.content.strip()
        import re
        match = re.search(r'\d+', answer)
        if match:
            count = int(match.group())
            if count >= 1:
                print(f"     [AI] Detectat {count} etaje (planuri de etaj) Ã®n cluster.")
                return count
        return 1
    except Exception as e:
        print(f"     [AI] Eroare etaje: {e}")
        return 1


def _split_cluster_by_buildings(
    crop: np.ndarray, offset_x: int, offset_y: int, expected_count: int, prefer_horizontal: bool = False
) -> list[list[int]]:
    """
    Ãmparte un cluster Ã®n sub-zone bazat pe gap-uri verticale (clÄƒdiri side-by-side)
    sau orizontale (etaje stacked). CÃ¢nd prefer_horizontal=True (split dupÄƒ etaje),
    se Ã®ncearcÄƒ mai Ã®ntÃ¢i split-ul pe rÃ¢nduri.
    """
    print(f"     ğŸ”§ Ãncerc sÄƒ Ã®mpart cluster-ul Ã®n {expected_count} pÄƒrÈ›i (prefer_horizontal={prefer_horizontal})...")
    
    gray = cv2.cvtColor(crop, cv2.COLOR_BGR2GRAY) if len(crop.shape) == 3 else crop
    _, binary = cv2.threshold(gray, 127, 255, cv2.THRESH_BINARY_INV)
    
    h, w = binary.shape
    
    col_density = np.sum(binary > 0, axis=0)
    row_density = np.sum(binary > 0, axis=1)
    
    col_smooth = cv2.GaussianBlur(col_density.astype(np.float32).reshape(-1, 1), (31, 1), 0).flatten()
    row_smooth = cv2.GaussianBlur(row_density.astype(np.float32).reshape(1, -1), (1, 31), 0).flatten()
    
    col_norm = col_smooth / (np.max(col_smooth) + 1e-5)
    row_norm = row_smooth / (np.max(row_smooth) + 1e-5)
    
    # Gap-uri: pentru etaje stacked folosim prag puÈ›in mai relaxat (8%) ca sÄƒ prindem È™i titluri/linii subÈ›iri
    gap_threshold = 0.08 if prefer_horizontal else 0.05
    col_gaps = np.where(col_norm < gap_threshold)[0]
    row_gaps = np.where(row_norm < gap_threshold)[0]
    
    boxes = []
    
    def try_split_horizontal() -> list[list[int]]:
        nonlocal boxes
        if len(row_gaps) <= h * 0.05:
            return []
        gap_groups = []
        if len(row_gaps) > 0:
            current_group = [row_gaps[0]]
            for i in range(1, len(row_gaps)):
                if row_gaps[i] - row_gaps[i - 1] <= 5:
                    current_group.append(row_gaps[i])
                else:
                    if len(current_group) > 20:
                        gap_groups.append(current_group)
                    current_group = [row_gaps[i]]
            if len(current_group) > 20:
                gap_groups.append(current_group)
        if len(gap_groups) < expected_count - 1:
            return []
        print(f"     âœ… GÄƒsit {len(gap_groups)} gap-uri orizontale (etaje).")
        split_positions = [int(np.median(g)) for g in gap_groups[: expected_count - 1]]
        split_positions = [0] + sorted(split_positions) + [h]
        out = []
        for i in range(len(split_positions) - 1):
            y_start, y_end = split_positions[i], split_positions[i + 1]
            band = binary[y_start:y_end, :]
            coords = np.where(band > 0)
            if coords[1].size > 0:
                x_min, x_max = coords[1].min(), coords[1].max()
                out.append([offset_x + x_min, offset_y + y_start, offset_x + x_max, offset_y + y_end])
        return out if len(out) > 1 else []
    
    def try_split_vertical() -> list[list[int]]:
        nonlocal boxes
        if len(col_gaps) <= w * 0.05:
            return []
        gap_groups = []
        if len(col_gaps) > 0:
            current_group = [col_gaps[0]]
            for i in range(1, len(col_gaps)):
                if col_gaps[i] - col_gaps[i - 1] <= 5:
                    current_group.append(col_gaps[i])
                else:
                    if len(current_group) > 20:
                        gap_groups.append(current_group)
                    current_group = [col_gaps[i]]
            if len(current_group) > 20:
                gap_groups.append(current_group)
        if len(gap_groups) < expected_count - 1:
            return []
        print(f"     âœ… GÄƒsit {len(gap_groups)} gap-uri verticale.")
        split_positions = [int(np.median(g)) for g in gap_groups[: expected_count - 1]]
        split_positions = [0] + sorted(split_positions) + [w]
        out = []
        for i in range(len(split_positions) - 1):
            x_start, x_end = split_positions[i], split_positions[i + 1]
            band = binary[:, x_start:x_end]
            coords = np.where(band > 0)
            if coords[0].size > 0:
                y_min, y_max = coords[0].min(), coords[0].max()
                out.append([offset_x + x_start, offset_y + y_min, offset_x + x_end, offset_y + y_max])
        return out if len(out) > 1 else []
    
    # Pentru etaje: Ã®ncearcÄƒ mai Ã®ntÃ¢i split orizontal
    if prefer_horizontal:
        boxes = try_split_horizontal()
        if len(boxes) >= 2:
            return boxes
        boxes = try_split_vertical()
        if len(boxes) >= 2:
            return boxes
        # FÄƒrÄƒ tÄƒieri la jumÄƒtate: luÄƒm cele N cele mai mari componente
        print(f"     âš ï¸  Nu am gÄƒsit gap-uri pentru etaje. Ãncerc cele {expected_count} cele mai mari componente...")
        return _split_cluster_by_largest_components(crop, offset_x, offset_y, expected_count)
    
    # Comportament original: vertical (clÄƒdiri) apoi orizontal
    boxes = try_split_vertical()
    if len(boxes) >= 2:
        return boxes
    
    boxes = try_split_horizontal()
    if len(boxes) >= 2:
        return boxes
    
    # FÄƒrÄƒ tÄƒieri la jumÄƒtate: folosim cele N cele mai mari componente din imagine
    print(f"     âš ï¸  Nu am gÄƒsit gap-uri clare. Ãncerc split dupÄƒ cele {expected_count} cele mai mari componente...")
    return _split_cluster_by_largest_components(crop, offset_x, offset_y, expected_count)


def _split_cluster_by_largest_components(
    crop: np.ndarray, offset_x: int, offset_y: int, expected_count: int
) -> list[list[int]]:
    """
    Sparge clusterul luÃ¢nd cele N cele mai mari componente conectate (zone de conÈ›inut).
    NU face tÄƒieri la jumÄƒtate â€“ foloseÈ™te doar bounding box-urile componentelor reale.
    """
    gray = cv2.cvtColor(crop, cv2.COLOR_BGR2GRAY) if len(crop.shape) == 3 else crop
    _, binary = cv2.threshold(gray, 127, 255, cv2.THRESH_BINARY_INV)
    h, w = binary.shape
    area_total = h * w

    num, labels, stats, _ = cv2.connectedComponentsWithStats(binary, connectivity=8)
    # stats: x, y, w, h, area; index 0 = fundal
    if num < 2:
        print(f"     âš ï¸  Doar fundal (0 componente de conÈ›inut).")
        return []

    # SortÄƒm componentele (fÄƒrÄƒ fundal) dupÄƒ arie descrescÄƒtor
    comps = []
    for i in range(1, num):
        x, y, bw, bh, area = stats[i]
        if area < 0.02 * area_total:  # ignorÄƒm zgomot sub 2% din imagine
            continue
        comps.append((area, x, y, x + bw, y + bh))

    comps.sort(key=lambda t: t[0], reverse=True)
    n_take = min(expected_count, len(comps))
    if n_take < 2:
        print(f"     âš ï¸  Am gÄƒsit doar {len(comps)} componentÄƒ(e) suficient de mare.")
        return []

    boxes = []
    for _, x1, y1, x2, y2 in comps[:n_take]:
        boxes.append([offset_x + x1, offset_y + y1, offset_x + x2, offset_y + y2])

    print(f"     âœ… Luate cele {len(boxes)} cele mai mari componente (fÄƒrÄƒ tÄƒieri la jumÄƒtate).")
    return boxes


def detect_clusters(mask: np.ndarray, orig: np.ndarray, return_boxes: bool = False, crop_name_prefix: str = "") -> list[str] | list[list[int]]:
    print("\n========================================================")
    print("[STEP 7] START Detectare clustere")
    print("========================================================")
    
    # 1. Pre-procesare â€“ kernel mai mic (3x3) ca curÄƒÈ›area sÄƒ fie mai puÈ›in agresivÄƒ, pÄƒstrÄƒm mai mult detaliu
    gray = cv2.cvtColor(mask, cv2.COLOR_BGR2GRAY) if len(mask.shape) == 3 else mask.copy()
    inv = cv2.bitwise_not(gray)
    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))
    clean = cv2.morphologyEx(cv2.dilate(inv, kernel), cv2.MORPH_OPEN, kernel)
    save_debug(clean, STEP_DIRS["clusters"]["initial"], "mask_clean.jpg")

    # 2. Raw Boxes
    num, _, stats, _ = cv2.connectedComponentsWithStats(clean, 8)
    boxes = [[x, y, x + bw, y + bh] for x, y, bw, bh, a in stats[1:] if a > 200]

    print(f"ğŸ”¸ [GEO] 1. Cutii brute (Raw Boxes): {len(boxes)}")
    debug_raw = _draw_debug_boxes(orig, boxes, (0, 0, 255), "Raw_")
    save_debug(debug_raw, STEP_DIRS["clusters"]["initial"], "debug_1_raw_boxes.jpg")

    # 3. Refined (Split)
    refined: list[list[int]] = []
    for i, (x1, y1, x2, y2) in enumerate(boxes, 1):
        reg = clean[y1:y2, x1:x2]
        if reg.size == 0: continue
        sub_boxes = split_large_cluster(reg, x1, y1, i)
        for sb in sub_boxes:
            refined.append(expand_cluster(clean, *sb))
    
    print(f"ğŸ”¸ [GEO] 2. Cutii rafinate (Refined): {len(refined)}")
    debug_ref = _draw_debug_boxes(orig, refined, (255, 128, 0), "Ref_")
    save_debug(debug_ref, STEP_DIRS["clusters"]["initial"], "debug_2_refined_boxes.jpg")

    # 4. Merged
    merged = merge_overlapping_boxes(refined, clean.shape)
    print(f"ğŸ”¸ [GEO] 3. Cutii unite (Merged): {len(merged)}")
    debug_mrg = _draw_debug_boxes(orig, merged, (0, 255, 255), "Mrg_")
    save_debug(debug_mrg, STEP_DIRS["clusters"]["initial"], "debug_3_merged_boxes.jpg")

    # 4b. Mapare: fiecare cluster merged -> lista de cutii refined (constituenÈ›ii) din care s-a format
    merged_to_constituents: dict[tuple[int, int, int, int], list[list[int]]] = {}
    for M in merged:
        constituents = [R for R in refined if _box_inside(R, M)]
        merged_to_constituents[tuple(M)] = constituents

    # 5. LOGICÄ‚ DECIZIONALÄ‚ (AI) - Doar dacÄƒ avem 1 singur cluster
    filtered: list[list[int]] = []

    print(f"\n--- [DECISION] Analizez rezultatul Merge ({len(merged)} clustere) ---")

    if len(merged) == 1:
        print(" ğŸ” OpenCV a gÄƒsit 1 singur cluster. Ãntreb AI-ul pentru validare...")
        ai_plan_count = _ask_ai_how_many_plans(orig)

        if ai_plan_count > 1:
            print(f" âš ï¸  [CONFLICT] AI vede {ai_plan_count} planuri, dar geometric am gÄƒsit 1.")
            print(" -> [ACTION] Ignor clusterul PÄƒrinte È™i activez FALLBACK (recuperez copiii).")
            
            parent_box = merged[0]
            parent_area = (parent_box[2]-parent_box[0]) * (parent_box[3]-parent_box[1])
            print(f" -> [DEBUG] Aria PÄƒrintelui (de ignorat): {int(parent_area)} px")

            source_candidates = refined if len(refined) > 1 else boxes
            print(f" -> [DEBUG] SursÄƒ fallback: {'Refined' if source_candidates == refined else 'Boxes'} ({len(source_candidates)} elemente)")
            
            candidates_with_area = []
            for i, cand in enumerate(source_candidates):
                c_w = cand[2] - cand[0]
                c_h = cand[3] - cand[1]
                c_area = c_w * c_h
                candidates_with_area.append((i, c_area))
            
            candidates_with_area.sort(key=lambda x: x[1], reverse=True)
            
            if not candidates_with_area:
                print(" âŒ [FAIL] Nu am gÄƒsit niciun copil Ã®n fallback. Revin la merged.")
                filtered = merged
            else:
                ref_idx = -1
                max_child_area = 0
                
                for idx, area in candidates_with_area:
                    if area > 0.90 * parent_area:
                        print(f"    [SKIP Ref] Candidat {idx} e prea mare ({int(area)} px), probabil e un border.")
                        continue
                    
                    ref_idx = idx
                    max_child_area = area
                    break
                
                if ref_idx == -1:
                    print("    [WARN] ToÈ›i candidaÈ›ii erau imenÈ™i. Iau cel mai mare disponibil.")
                    ref_idx = candidates_with_area[0][0]
                    max_child_area = candidates_with_area[0][1]

                print(f" -> [LOGIC] ReferinÈ›a AleasÄƒ (Cel mai mare Copil): Index {ref_idx}, Aria: {int(max_child_area)} px")
                
                child_threshold = 0.10 * max_child_area
                print(f" -> [LOGIC] Prag minim (10% din ref): {int(child_threshold)} px")

                accepted_indices = set()
                fallback_accepted_count = 0
                
                for i, area in candidates_with_area:
                    cand = source_candidates[i]
                    
                    if area > 0.90 * parent_area:
                        continue

                    if area < child_threshold:
                        continue

                    is_duplicate = False
                    for existing_idx in accepted_indices:
                        existing_box = source_candidates[existing_idx]
                        if _is_overlapping(cand, existing_box, threshold=0.60):
                            print(f"    [SKIP Overlap] Candidat {i} se suprapune cu {existing_idx}. Ignor.")
                            is_duplicate = True
                            break
                    
                    if is_duplicate:
                        continue

                    filtered.append(cand)
                    accepted_indices.add(i)
                    fallback_accepted_count += 1
                
                if fallback_accepted_count > 0:
                    print(f" âœ… [SUCCESS] Am recuperat {fallback_accepted_count} clustere valide È™i unice.")
                    
                    debug_fb_detailed = _draw_detailed_fallback_debug(orig, source_candidates, accepted_indices, ref_idx, parent_box)
                    save_debug(debug_fb_detailed, STEP_DIRS["clusters"]["initial"], "debug_4_detailed_fallback_logic.jpg")
                    print(" ğŸ“¸ [DEBUG] Imagine detaliatÄƒ salvatÄƒ: debug_4_detailed_fallback_logic.jpg")

                else:
                    print(" âŒ [FAIL] ToÈ›i copiii au picat filtrul. Revin la 'Merged'.")
                    filtered = merged
        else:
            print(" âœ… [OK] AI confirmÄƒ: Este un singur plan. PÄƒstrez clusterul 'Merged'.")
            filtered = merged
    else:
        print(f" âœ… [OK] OpenCV a gÄƒsit deja {len(merged)} clustere. Nu este nevoie de intervenÈ›ia AI.")
        filtered = merged


    # 6. FILTRARE FINALÄ‚ DIMENSIUNI (referinÈ›Äƒ: al doilea cel mai mare cluster)
    img_area = orig.shape[0] * orig.shape[1]
    if filtered:
        print("\n--- [FINAL] Filtrare relativÄƒ dimensiuni ---")
        areas_map = []
        for i, box in enumerate(filtered):
            a = (box[2] - box[0]) * (box[3] - box[1])
            areas_map.append((i, float(a)))
        areas_map.sort(key=lambda x: x[1], reverse=True)
        reference_area = areas_map[1][1] if len(areas_map) > 1 else areas_map[0][1]
        img_area_f = float(img_area)
        MIN_REL = 0.10
        MIN_ABS = 0.0005
        min_allowed = max(MIN_REL * reference_area, MIN_ABS * img_area_f)
        print(f" -> ReferinÈ›Äƒ: al doilea cel mai mare = {int(reference_area)} px, min_allowed = {int(min_allowed)} px")
        final_list = []
        for idx, area in areas_map:
            if area >= min_allowed:
                final_list.append(filtered[idx])
            else:
                print(f"    [DROP Final] Cluster {idx} area={int(area)} < {int(min_allowed)}")
        final_list.sort(key=lambda b: (b[1], b[0]))
        filtered = final_list
        final_areas_map = [(i, (b[2]-b[0])*(b[3]-b[1])) for i, b in enumerate(filtered)]
        final_areas_map.sort(key=lambda x: x[1], reverse=True)

    # 6b. DacÄƒ avem >10 clustere, verificÄƒm dacÄƒ sunt grupate Ã®n â‰¥2 zone; dacÄƒ da, le unim pe zone
    if filtered and len(filtered) > 10:
        zones = _merge_clusters_into_zones(filtered, (orig.shape[0], orig.shape[1]))
        if zones and len(zones) >= 2:
            print(f"\n--- [ZONE] {len(filtered)} clustere grupate Ã®n {len(zones)} zone â€“ Ã®nlocuiesc cu zonele ---")
            filtered = zones
            final_areas_map = [(i, (b[2]-b[0])*(b[3]-b[1])) for i, b in enumerate(filtered)]
            final_areas_map.sort(key=lambda x: x[1], reverse=True)

    if filtered:
        final_list = filtered
        # âœ… VERIFICARE FRAME: DacÄƒ cel mai mare cluster ocupÄƒ >95% din imagine
        if len(final_list) > 0 and len(final_areas_map) > 0:
            largest_cluster_area = final_areas_map[0][1]  # Cel mai mare cluster
            largest_cluster_ratio = largest_cluster_area / img_area
            
            if largest_cluster_ratio > 0.95:
                print(f"\n--- [FRAME CHECK] Cel mai mare cluster ocupÄƒ {largest_cluster_ratio*100:.1f}% din imagine ---")
                print(f"   ğŸ” Verific dacÄƒ existÄƒ frame Ã®n jurul planului...")
                
                # GÄƒsim box-ul celui mai mare cluster folosind final_areas_map
                largest_idx = final_areas_map[0][0]  # Index Ã®n final_list
                largest_cluster_box = final_list[largest_idx]
                
                # Mai Ã®ntÃ¢i Ã®ncercÄƒm detectarea geometricÄƒ (mai rapidÄƒ È™i mai precisÄƒ)
                has_frame_geometric = _detect_frame_geometric(orig, largest_cluster_box)
                
                if has_frame_geometric:
                    print(f"   âœ… [GEOMETRIC] Detectat frame fix la marginile imaginii!")
                    print(f"   âœ‚ï¸  Fac crop la imagine (exclud frame-ul) È™i reapelez detectarea clusterelor...")
                    
                    # Facem crop la imagine la conÈ›inutul din interiorul cluster-ului (cu padding mic)
                    x1, y1, x2, y2 = largest_cluster_box
                    h, w = orig.shape[:2]
                    
                    # AdÄƒugÄƒm un padding mic (2% din dimensiune) pentru a nu tÄƒia prea mult
                    padding_x = int((x2 - x1) * 0.02)
                    padding_y = int((y2 - y1) * 0.02)
                    
                    # Crop la imagine (excludem marginile cu frame)
                    crop_x1 = max(0, x1 + padding_x)
                    crop_y1 = max(0, y1 + padding_y)
                    crop_x2 = min(w, x2 - padding_x)
                    crop_y2 = min(h, y2 - padding_y)
                    
                    cropped_img = orig[crop_y1:crop_y2, crop_x1:crop_x2]
                    print(f"   ğŸ“ Crop: [{crop_x1}, {crop_y1}, {crop_x2}, {crop_y2}] din [{w}x{h}]")
                    print(f"   ğŸ“ Dimensiuni dupÄƒ crop: {cropped_img.shape[1]}x{cropped_img.shape[0]}")
                    
                    # ReapeleazÄƒ detectarea clusterelor pe imaginea croppatÄƒ
                    print(f"   ğŸ”„ Reapelez detectarea clusterelor pe imaginea croppatÄƒ...")
                    cropped_clusters = detect_clusters(
                        cropped_img,  # mask
                        cropped_img,  # orig
                        return_boxes=True  # ReturnÄƒm box-uri Ã®n loc de path-uri
                    )
                    
                    # AjustÄƒm coordonatele clusterelor detectate la coordonatele originale
                    adjusted_clusters = []
                    for cluster_box in cropped_clusters:
                        cx1, cy1, cx2, cy2 = cluster_box
                        # AdÄƒugÄƒm offset-ul crop-ului
                        adjusted_box = [
                            cx1 + crop_x1,
                            cy1 + crop_y1,
                            cx2 + crop_x1,
                            cy2 + crop_y1
                        ]
                        adjusted_clusters.append(adjusted_box)
                    
                    print(f"   âœ… Detectat {len(adjusted_clusters)} cluster-uri dupÄƒ eliminare frame")
                    filtered = adjusted_clusters
                else:
                    # DacÄƒ detectarea geometricÄƒ nu confirmÄƒ, Ã®ncercÄƒm AI-ul
                    print(f"   ğŸ” [GEOMETRIC] Nu am detectat frame clar. Verific cu AI...")
                    has_frame_ai = _ask_ai_if_has_frame(orig)
                    
                    if has_frame_ai:
                        print(f"   âœ… AI confirmÄƒ: ExistÄƒ frame Ã®n jurul planului.")
                        print(f"   âœ‚ï¸  Fac crop la imagine (exclud frame-ul) È™i reapelez detectarea clusterelor...")
                        
                        # Facem crop la imagine la conÈ›inutul din interiorul cluster-ului (cu padding mic)
                        x1, y1, x2, y2 = largest_cluster_box
                        h, w = orig.shape[:2]
                        
                        # AdÄƒugÄƒm un padding mic (2% din dimensiune) pentru a nu tÄƒia prea mult
                        padding_x = int((x2 - x1) * 0.02)
                        padding_y = int((y2 - y1) * 0.02)
                        
                        # Crop la imagine (excludem marginile cu frame)
                        crop_x1 = max(0, x1 + padding_x)
                        crop_y1 = max(0, y1 + padding_y)
                        crop_x2 = min(w, x2 - padding_x)
                        crop_y2 = min(h, y2 - padding_y)
                        
                        cropped_img = orig[crop_y1:crop_y2, crop_x1:crop_x2]
                        print(f"   ğŸ“ Crop: [{crop_x1}, {crop_y1}, {crop_x2}, {crop_y2}] din [{w}x{h}]")
                        print(f"   ğŸ“ Dimensiuni dupÄƒ crop: {cropped_img.shape[1]}x{cropped_img.shape[0]}")
                        
                        # ReapeleazÄƒ detectarea clusterelor pe imaginea croppatÄƒ
                        print(f"   ğŸ”„ Reapelez detectarea clusterelor pe imaginea croppatÄƒ...")
                        cropped_clusters = detect_clusters(
                            cropped_img,  # mask
                            cropped_img,  # orig
                            return_boxes=True  # ReturnÄƒm box-uri Ã®n loc de path-uri
                        )
                        
                        # AjustÄƒm coordonatele clusterelor detectate la coordonatele originale
                        adjusted_clusters = []
                        for cluster_box in cropped_clusters:
                            cx1, cy1, cx2, cy2 = cluster_box
                            # AdÄƒugÄƒm offset-ul crop-ului
                            adjusted_box = [
                                cx1 + crop_x1,
                                cy1 + crop_y1,
                                cx2 + crop_x1,
                                cy2 + crop_y1
                            ]
                            adjusted_clusters.append(adjusted_box)
                        
                        print(f"   âœ… Detectat {len(adjusted_clusters)} cluster-uri dupÄƒ eliminare frame")
                        filtered = adjusted_clusters
                    else:
                        print(f"   âœ… AI confirmÄƒ: NU existÄƒ frame. PÄƒstrez toate cluster-urile.")
            else:
                print(f"   â„¹ï¸  Cel mai mare cluster ocupÄƒ {largest_cluster_ratio*100:.1f}% (<95%). Nu verific frame.")

    # âœ… 7. VALIDARE FINALÄ‚ AI - Desfacem clustere mari Ã®n funcÈ›ie de constituenÈ›i, apoi procesÄƒm fiecare la rÃ¢nd
    print("\n========================================================")
    print("[FINAL VALIDATION] Verificare AI â€“ desfacem dupÄƒ constituenÈ›i, apoi procesÄƒm fiecare cluster")
    print("========================================================")
    
    final_validated_boxes: list[list[int]] = []
    # CoadÄƒ: fiecare cluster este procesat; dacÄƒ AI zice 2+ etaje/clÄƒdiri, Ã®l desfacem Ã®n N È™i punem cele N Ã®n coadÄƒ
    to_process: list[list[int]] = [list(b) for b in filtered]
    max_rounds = 100  # limitÄƒ ca sÄƒ nu avem buclÄƒ infinitÄƒ
    round_num = 0

    while to_process and round_num < max_rounds:
        round_num += 1
        x1, y1, x2, y2 = to_process.pop(0)
        crop = orig[y1:y2, x1:x2]
        key = (x1, y1, x2, y2)
        constituents_raw = merged_to_constituents.get(key, [])
        parent_area = (x2 - x1) * (y2 - y1)
        min_area = max(0.02 * parent_area, 500)
        significant = [c for c in constituents_raw if (c[2] - c[0]) * (c[3] - c[1]) >= min_area]
        unique_const: list[list[int]] = []
        for c in sorted(significant, key=lambda b: (b[2] - b[0]) * (b[3] - b[1]), reverse=True):
            if any(_is_overlapping(c, u, 0.70) for u in unique_const):
                continue
            unique_const.append(c)

        buildings_count = _ask_ai_how_many_buildings_in_cluster(crop)
        if buildings_count > 1:
            print(f"   âš ï¸  AI: {buildings_count} clÄƒdiri! Desfac Ã®n {buildings_count} clustere dupÄƒ constituenÈ›i (orizontal).")
            if len(unique_const) >= 2:
                sub_boxes = _split_constituents_into_n_groups(unique_const, buildings_count, by_vertical=False)
                if sub_boxes:
                    for box in sub_boxes:
                        to_process.append(box)
                    print(f"   âœ… AdÄƒugat {len(sub_boxes)} clustere Ã®n coadÄƒ pentru procesare.")
                    continue
            fallback = _split_cluster_by_largest_components(crop, x1, y1, buildings_count)
            if len(fallback) >= 2:
                for box in fallback:
                    to_process.append(box)
                print(f"   âœ… Fallback: {len(fallback)} clustere (cele mai mari componente) Ã®n coadÄƒ.")
                continue
            final_validated_boxes.append([x1, y1, x2, y2])
            continue

        floors_count = _ask_ai_how_many_floors_in_cluster(crop)
        if floors_count >= 2:
            print(f"   âš ï¸  AI: {floors_count} etaje! Desfac Ã®n {floors_count} clustere dupÄƒ constituenÈ›i (vertical).")
            if len(unique_const) >= 2:
                sub_boxes = _split_constituents_into_n_groups(unique_const, floors_count, by_vertical=True)
                if sub_boxes:
                    for box in sub_boxes:
                        to_process.append(box)
                    print(f"   âœ… AdÄƒugat {len(sub_boxes)} clustere Ã®n coadÄƒ pentru procesare.")
                    continue
            fallback = _split_cluster_by_largest_components(crop, x1, y1, floors_count)
            if len(fallback) >= 2:
                for box in fallback:
                    to_process.append(box)
                print(f"   âœ… Fallback: {len(fallback)} clustere (cele mai mari componente) Ã®n coadÄƒ.")
                continue
            final_validated_boxes.append([x1, y1, x2, y2])
            continue

        # 1 clÄƒdire, 1 etaj (sau AI nu a zis 2+)
        has_blueprint_and_sideview = _check_blueprint_and_sideview_in_cluster(crop)
        if has_blueprint_and_sideview:
            print(f"   âš ï¸  Blueprint + sideview Ã®n acelaÈ™i cluster. Desfac Ã®n 2 dupÄƒ constituenÈ›i.")
            if len(unique_const) >= 2:
                sub_boxes = _split_constituents_into_n_groups(unique_const, 2, by_vertical=False)
                if sub_boxes:
                    for box in sub_boxes:
                        to_process.append(box)
                    print(f"   âœ… AdÄƒugat 2 clustere Ã®n coadÄƒ.")
                    continue
            sub_clusters = _detect_sub_clusters_in_image(crop)
            if len(sub_clusters) >= 2:
                for sub_x1, sub_y1, sub_x2, sub_y2 in sub_clusters:
                    to_process.append([x1 + sub_x1, y1 + sub_y1, x1 + sub_x2, y1 + sub_y2])
                print(f"   âœ… AdÄƒugat {len(sub_clusters)} sub-clustere Ã®n coadÄƒ.")
                continue
            fallback_boxes = _split_cluster_by_largest_components(crop, x1, y1, expected_count=2)
            if len(fallback_boxes) >= 2:
                for box in fallback_boxes:
                    to_process.append(box)
                continue
        final_validated_boxes.append([x1, y1, x2, y2])

    if to_process:
        print(f"   âš ï¸  LimitÄƒ {max_rounds} runde atinsÄƒ. {len(to_process)} clustere rÄƒmase le adÄƒugÄƒm direct.")
        final_validated_boxes.extend(to_process)

    final_validated_boxes.sort(key=lambda b: (b[1], b[0]))
    print(f"\nğŸ“Š Rezultat: {len(filtered)} â†’ {len(final_validated_boxes)} clustere finale")

    # 7.4. EliminÄƒ clustere â€prea puÈ›inâ€: dacÄƒ un box e conÈ›inut Ã®n altul, pÄƒstrÄƒm doar cel exterior
    # (evitÄƒ livrarea atÃ¢t a â€interior onlyâ€ cÃ¢t È™i a â€interior+terasÄƒâ€ â€“ livrÄƒm doar full)
    def _drop_nested_boxes(boxes: list[list[int]]) -> list[list[int]]:
        if len(boxes) < 2:
            return boxes
        to_remove = set()
        for i, inner in enumerate(boxes):
            for j, outer in enumerate(boxes):
                if i == j:
                    continue
                if _box_inside(inner, outer, tolerance=5):
                    area_inner = (inner[2] - inner[0]) * (inner[3] - inner[1])
                    area_outer = (outer[2] - outer[0]) * (outer[3] - outer[1])
                    if area_outer > area_inner:
                        to_remove.add(i)
                        print(f"   ğŸ§¹ [Nested] Scot clusterul interior (prea puÈ›in): {inner} â€“ pÄƒstrez exteriorul {outer}")
                        break
        if to_remove:
            boxes = [b for k, b in enumerate(boxes) if k not in to_remove]
            print(f"   âœ… DupÄƒ eliminare nested: {len(boxes)} clustere")
        return boxes

    final_validated_boxes = _drop_nested_boxes(final_validated_boxes)

    # 7.5. Fill background pentru imagini mici incluse Ã®n imagini mari
    def _fill_small_clusters_in_large(orig_img: np.ndarray, boxes: list[list[int]]) -> tuple[np.ndarray, list[list[int]]]:
        """
        DacÄƒ existÄƒ o imagine mare È™i una/mai multe mici incluse Ã®n ea,
        face fill cu background color Ã®n imaginea mare la coordonatele imaginilor mici.
        """
        if len(boxes) < 2:
            return orig_img, boxes
        
        h, w = orig_img.shape[:2]
        img_area = w * h
        
        # CalculÄƒm ariile È™i ratio-urile
        boxes_with_info = []
        for box in boxes:
            bx1, by1, bx2, by2 = box
            area = (bx2 - bx1) * (by2 - by1)
            ratio = area / float(img_area)
            boxes_with_info.append((box, area, ratio))
        
        # SortÄƒm dupÄƒ arie (descrescÄƒtor)
        boxes_with_info.sort(key=lambda x: x[1], reverse=True)
        
        # DetectÄƒm dacÄƒ primul box este mare (>80% din imagine)
        if boxes_with_info[0][2] < 0.80:
            return orig_img, boxes
        
        big_box, big_area, big_ratio = boxes_with_info[0]
        big_x1, big_y1, big_x2, big_y2 = big_box
        
        # DetectÄƒm box-uri mici incluse Ã®n box-ul mare
        small_boxes = []
        for box, area, ratio in boxes_with_info[1:]:
            sx1, sy1, sx2, sy2 = box
            
            # VerificÄƒm dacÄƒ box-ul mic este inclus Ã®n box-ul mare (cu margin mic)
            if (sx1 >= big_x1 and sy1 >= big_y1 and sx2 <= big_x2 and sy2 <= big_y2):
                # VerificÄƒm cÄƒ nu este un peer (sÄƒ fie semnificativ mai mic)
                peer_ratio = area / float(big_area)
                if peer_ratio < 0.55:  # Nu este peer
                    small_boxes.append(box)
        
        if not small_boxes:
            return orig_img, boxes
        
        print(f"\nğŸ¨ Detectat {len(small_boxes)} imagini mici incluse Ã®n imaginea mare")
        print(f"   ğŸ“¦ Imagine mare: {big_box} (ratio={big_ratio:.2f})")
        for idx, small_box in enumerate(small_boxes, 1):
            small_area = (small_box[2] - small_box[0]) * (small_box[3] - small_box[1])
            print(f"   ğŸ“¦ Imagine micÄƒ {idx}: {small_box} (aria={small_area}px)")
        
        # CalculÄƒm culoarea background-ului (media colÈ›urilor imaginii mari)
        big_crop = orig_img[big_y1:big_y2, big_x1:big_x2]
        h_crop, w_crop = big_crop.shape[:2]
        
        # Media colÈ›urilor pentru background
        corners = [
            big_crop[0, 0],  # top-left
            big_crop[0, w_crop-1],  # top-right
            big_crop[h_crop-1, 0],  # bottom-left
            big_crop[h_crop-1, w_crop-1]  # bottom-right
        ]
        background_color = np.mean(corners, axis=0).astype(np.uint8)
        print(f"   ğŸ¨ Culoare background detectatÄƒ: {background_color}")
        
        # Facem fill Ã®n imaginea mare la coordonatele imaginilor mici
        modified_img = orig_img.copy()
        for small_box in small_boxes:
            sx1, sy1, sx2, sy2 = small_box
            # Facem fill complet cu background color
            modified_img[sy1:sy2, sx1:sx2] = background_color
            print(f"   âœ… Fill aplicat la {small_box}")
        
        return modified_img, boxes
    
    # AplicÄƒm fill-ul dacÄƒ este necesar
    orig, final_validated_boxes = _fill_small_clusters_in_large(orig, final_validated_boxes)

    # 8. Output
    result = orig.copy()
    crop_paths: list[str] = []
    crops_dir = get_output_dir() / STEP_DIRS["clusters"]["crops"]
    crops_dir.mkdir(parents=True, exist_ok=True)

    print(f"\nğŸ’¾ Salvare {len(final_validated_boxes)} clustere validate...")

    prefix = crop_name_prefix or ""
    # Culori distincte BGR pentru fiecare cluster (ciclu dacÄƒ sunt multe)
    palette = [
        (255, 100, 100), (100, 255, 100), (100, 100, 255),
        (255, 255, 100), (255, 100, 255), (100, 255, 255),
        (180, 100, 255), (255, 180, 100), (100, 255, 180),
        (200, 50, 150), (50, 200, 150), (150, 50, 200),
    ]
    overlay = result.copy()

    for i, (x1, y1, x2, y2) in enumerate(final_validated_boxes, 1):
        color = palette[(i - 1) % len(palette)]
        # Interior cluster: culoare cu transparenÈ›Äƒ 50%
        cv2.rectangle(overlay, (x1, y1), (x2, y2), color, -1)

        crop = orig[y1:y2, x1:x2]
        crop = resize_bgr_max_side(crop)

        crop_path = crops_dir / f"{prefix}cluster_{i}.jpg"
        cv2.imwrite(str(crop_path), crop)
        crop_paths.append(str(crop_path))

    # Blend overlay 50% peste original
    result = cv2.addWeighted(overlay, 0.5, result, 0.5, 0)
    # ConstituenÈ›ii care nu au fost luaÈ›i Ã®n seamÄƒ: contur gri peste blend, per cluster
    for i, (x1, y1, x2, y2) in enumerate(final_validated_boxes, 1):
        key = (x1, y1, x2, y2)
        constituents_inside = merged_to_constituents.get(key, [])
        if not constituents_inside:
            constituents_inside = [r for r in refined if _box_inside(r, [x1, y1, x2, y2])]
        for cx1, cy1, cx2, cy2 in constituents_inside:
            if (cx1, cy1, cx2, cy2) == (x1, y1, x2, y2):
                continue
            cv2.rectangle(result, (cx1, cy1), (cx2, cy2), (100, 100, 100), 1)
            cv2.putText(result, "nu luat", (cx1 + 2, cy1 + 12), cv2.FONT_HERSHEY_SIMPLEX, 0.35, (100, 100, 100), 1)
    # Contururi È™i etichete clustere finale
    for i, (x1, y1, x2, y2) in enumerate(final_validated_boxes, 1):
        color = palette[(i - 1) % len(palette)]
        cv2.rectangle(result, (x1, y1), (x2, y2), color, 2)
        cv2.putText(result, str(i), (x1 + 5, y1 + 25), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 0), 2)
        cv2.putText(result, str(i), (x1 + 5, y1 + 25), cv2.FONT_HERSHEY_SIMPLEX, 0.8, color, 1)

    # Preview: per paginÄƒ cÃ¢nd e PDF multi-paginÄƒ (altfel se suprascrie È™i vezi doar ultima paginÄƒ)
    preview_name = f"final_clusters_validated_{prefix.rstrip('_')}.jpg" if prefix else "final_clusters_validated.jpg"
    save_debug(result, STEP_DIRS["clusters"]["final"], preview_name)
    print(f"âœ… [DONE] {len(final_validated_boxes)} clustere validate returnate")

    if return_boxes:
        return final_validated_boxes
    return crop_paths


def detect_wall_zones(orig: np.ndarray, thick_mask: np.ndarray, crop_name_prefix: str = "") -> list[str]:
    print("\n[STEP 6] Detectare zone pereÈ›i...")
    gray = (thick_mask / 255).astype(np.float32)
    dens = cv2.GaussianBlur(gray, (51, 51), 0)
    norm = cv2.normalize(dens, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)
    _, dense_mask = cv2.threshold(norm, 60, 255, cv2.THRESH_BINARY)

    filled = dense_mask.copy()
    flood = np.zeros((gray.shape[0] + 2, gray.shape[1] + 2), np.uint8)
    cv2.floodFill(filled, flood, (0, 0), 0)
    walls = cv2.bitwise_not(filled)

    save_debug(walls, STEP_DIRS["walls"], "filled_unified.jpg")
    crop_paths = detect_clusters(walls, orig, crop_name_prefix=crop_name_prefix)
    return crop_paths